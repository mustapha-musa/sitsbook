# Ensemble Prediction from Multiple Models{-}

Ensemble prediction is a powerful technique for combining predictions from multiple models to produce more accurate and robust predictions. In general, ensemble predictions produce better predictions than using a single model. This is because the errors of individual models can cancel out or be reduced when combined with the predictions of other models. As a result, ensemble predictions can lead to better overall accuracy and reduce the risk of overfitting. 

Ensemble predictions are  more robust to changes in the data or the model. If a single model is sensitive to certain types of data or errors, ensemble predictions can help reduce the impact of these issues by combining the predictions of multiple models. Ensemble predictions can help you choose the best model or models for a given task. By comparing the predictions of different models, you can identify which models perform best on different types of data or tasks. This can be especially useful when working with complex or uncertain data. By combining the predictions of multiple models, users can identify which features or factors are most important for making accurate predictions. When using ensemble methods, it's important to choose models that are diverse and have different sources of error. This can help ensure that the ensemble predictions are more accurate and robust.

Overall, ensemble predictions are a powerful tool for improving the accuracy and robustness of machine learning models. By combining the predictions of multiple models, users can reduce errors and uncertainty, and gain new insights into the underlying patterns in the data.

The `sits` package provides the function `sits_combine_predictions()` to estimate ensemble predictions using probability cubes produced by `sits_classify()` and optionally post-processed with `sits_smooth()`.  There are two ways to do ensemble predictions from multiple models in `sits`:

* Averaging: In this approach, the predictions of each model are averaged to produce the final prediction. This method works well when the models have similar accuracy and errors. 

* Uncertainty: Predictions from different models are compared in terms of their uncertainties on a pixel-by-pixel basis; predictions with lower uncertainty are chosen as the more likely ones to be valid. 

In what follows, we will use the same data used as in the "Image Classification in Data Cubes" chapter  section to illustrate how to produce an ensemble prediction. For simplicity, we repeat the steps taken for classify an image in that chapter: create a data cube, train a model using the lightweight temporal attention encoder algorithm (`sits_lighttae()`), then classify, post-process and label the data cube. As a starting point, we plot two instances of the data cube, at the start and at the end of the time series. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2020-07-06."}
# files are available in a local directory 
data_dir <- system.file("extdata/Rondonia-20LKP/", package = "sitsdata")
# read data cube
ro_cube_20LKP <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c('X1', "tile", "band", "date")
)
plot(ro_cube_20LKP, 
    date = "2020-07-06", 
    red = "B11", 
    green = "B8A", 
    blue = "B02"
)
```
The image from 2020-07-06 shows many areas under deforestation, especially a large one located in top center of the image. It is useful to compare to an image one year later, which shows a number of burned areas resulting from forest removal followed by fire. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2021-08-10."}
plot(ro_cube_20LKP, 
    date = "2021-08-10", 
    red = "B11", 
    green = "B8A", 
    blue = "B02"
)
```
The samples used in the classification are the same as those used in the "Image Classification in Data Cubes" chapter. Please refer to that chapter for a more detailed description of the temporal response of the samples. We first reproduce the result obtained in that chapter using the `sits_lighttae()` algorithm. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Image Classification using LightTAE model."}
# get the samples from library "sitsdata"
library(sitsdata)
data(samples_prodes_4classes)
# use only the bands available in the cube
samples_3bands <- sits_select(
    data = samples_prodes_4classes, 
    bands = sits_bands(ro_cube_20LKP)
)
# train model using LightTAE algorithm
ltae_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_lighttae(
        opt_hparams = list(lr = 0.001)
        )
)
# classify data cube
ro_cube_probs_ltae <- sits_classify(
    data     = ro_cube_20LKP,
    ml_model = ltae_model,
    output_dir = "./tempdir/chp12",
    version = "ltae",
    multicores = 4,
    memsize = 12
)
# smooth data cube
ro_cube_bayes_ltae <- sits_smooth(
    cube    = ro_cube_probs_ltae,
    output_dir = "./tempdir/chp12",
    version = "ltae",
    multicores = 4,
    memsize = 12
)
# generate thematic map
defor_map <- sits_label_classification(
    cube = ro_cube_bayes_ltae,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp12",
    version = "ltae"
)
plot(defor_map)
```
The deforestation map produced by the `sits_lighttae()` algorithm has spatial consistency; arguably, it underestimates the burned areas in the right-hand corner of the image. The method tries to modelling temporal behavior of the reflectances. For this reason, it sometimes fails to detect changes that occur in the last dates of the time series, as it occurs when areas are burned in August. 

To build a two-member ensemble, we now classify the same image using a random forests algorithm.  

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia using a random forests algorithm."}

# train model using random forests algorithm
rfor_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_rfor()
)

# classify the data cube using the tempCNN model
ro_cube_probs_rfor <- sits_classify(
    data = ro_cube_20LKP,
    ml_model = rfor_model,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4
)
# post-process the probability cube
ro_cube_bayes_rfor <- sits_smooth(
    cube = ro_cube_probs_rfor,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4
)
# label the post-processed  probability cube
ro_cube_label_rfor <- sits_label_classification(
    cube = ro_cube_bayes_rfor,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4  
)
# plot the random forests version of the classified cube
plot(ro_cube_label_rfor)
```

Comparing the two results, while most of the land areas have been classified equally, there are places of disagreement concerning the places classified as "Burned_Area" and "Highly Degraded". Since the random forests model is sensitive to the response of images in the end of the period, it tends to be better to distinguish burned areas. However, it tends to reduce the forest areas, classifying some of them as highly degraded. Such misclassification happens because the random forests algorithm disregards the temporal correlation of the input data. Values from a single date are used to distinguish between natural and degraded forest areas. 

Given the differences and complementarities between the two predicted outcomes, it is useful to combine them using `sits_combine_predictions()`. The first option for ensemble prediction is to take the average of the probability maps to reduce noise.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia near Samuel dam using the average of the probabilities produced by lightTAE and tempCNN algorithms."}
# combine the two predictions by taking the average of the probabilities for each class
s2_cube_average_probs <- sits_combine_predictions(
  cubes = list(ro_cube_bayes_ltae, ro_cube_bayes_rfor),
  type = "average",
  output_dir = "./tempdir/chp11/",
  version = "average",
  memsize = 16,
  multicores = 1
)
# label the average probability cube
s2_cube_average_class <- sits_label_classification(
    cube = s2_cube_average_probs,
    output_dir = "./tempdir/chp11/",
    version = "average",
    memsize = 16,
    multicores = 4  
)
# plot the second version of the classified cube
plot(s2_cube_average_class)
```
Compared with the initial map, the result has increased the number of pixels classified as burned areas and highly degraded. Not all areas classified as degraded forest by the random forests method have been included in the final map. Only those places where the random forests algorithm has high confidence have been included. In general, the average map results on a better classification than the individual results.
