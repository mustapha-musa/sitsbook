# Machine Learning for Data Cubes{-}

```{r, echo = FALSE}
source("common.R")
if (!file.exists("./tempdir/chp8"))
    dir.create("./tempdir/chp8")
```


## Machine learning classification{-}

<a href="https://www.kaggle.com/brazildatacube/sits-classification-r" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

Machine learning classification is a kind of supervised learning in which an algorithm is trained to predict which category or class an input data point belongs to. It involves teaching a computer program to recognize patterns in data and use those patterns to predict the class label of new data. The goal of classification is to build a model that can accurately assign a class label to new data based on patterns it has learned from previously labeled data. In `sits`, machine learning is used to classify individual time series, using the `time-first` approach. 

The goal of a machine learning models is to approximate a function $y = f(x)$ that maps an input $x$ to a category $y$. A model defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation [@Goodfellow2016]. The difference between the different algorithms is the approach they take to build the mapping that classifies the input data.  

The `sits` package includes two kinds of methods for time series classification: 

1. Machine learning algorithms that do not explicitly consider the temporal stucture of the time series. They treat time series as a vector in a high-dimensional feature space. Each instance of the time series is taken by the algorithm as independent from the others. They include random forest (`sits_rfor()`), support vector machines (`sits_svm()`), extreme gradient boosting (`sits_xgboost()`), and multilayer perceptrons (`sits_mlp()`). 

2. Deep learning methods designed to work with time series. Temporal relations between observed values in a time series are taken into account. From this kind of models, `sits` supports 1D convolution neural networks  (`sits_tempcnn()`), residual 1D networks (`sits_resnet()`), and temporal attention-based encoders (`sits_tae()` and `sits_lighttae()`). In these algorithms, the order of the samples in the time series is relevant for the classifier. 

Based on experience with `sits`, random forests, extreme gradient boosting, and temporal deep learning models outperform SVM and multilayer perceptron models. This is because the temporal behavior of land use and land cover classes varies, with certain dates providing more information than others. For instance, when monitoring deforestation, dates that correspond to forest removal actions are more informative than earlier or later dates. Similarly, in crop mapping, a few dates may capture a large portion of the variation. Classification methods that consider the temporal order of samples are therefore more likely to capture the seasonal behavior of image time series. Random forest and extreme gradient boosting methods that use individual measures as nodes in decision trees can also capture specific events such as deforestation.

The following examples show how to train machine learning methods and apply it to classify a single time series. We use the set `samples_matogrosso_mod13q1`, containing time series samples from Brazilian Mato Grosso state, obtained from the MODIS MOD13Q1 product. It has 1,892 samples and 9 classes (Cerrado, Fallow_Cotton, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet, Soy_Sunflower). Each time series covers 12 months (23 data points) with 6 bands ("NDVI", "EVI", "BLUE", "RED", "NIR", "MIR"). The samples are arranged along an agricultural year, starting in September and ending in August. The data set was used in the paper "Big Earth observation time series analysis for monitoring Brazilian agriculture" [@Picoli2018], and is available in the R package `sitsdata`. Please see the "Setup" section for instructions on how to obtain this package.

The results should not be taken as indication of which method performs better. The most important factor for achieving a good result is the quality of the training data [@Maxwell2018]. Experience shows that classification quality depends on the training samples and on how well the model matches these samples. For examples of ML for classifying large areas, please see the papers by the authors [@Picoli2018; @Picoli2020a; @Simoes2020; @Ferreira2020a].

## Visualizing Sample Patterns{-}

One useful way of describing and understanding the samples is by plotting them. A direct way of doing so is using the `plot` function, as discussed in the "Working with Time Series" chapter. A useful alternative is to estimate a statistical approximation to an idealized pattern based on a generalized additive models (GAM). A GAM is a linear model in which the linear predictor depends linearly on a smooth function of the predictor variables 

$$
y = \beta_{i} + f(x) + \epsilon, \epsilon \sim N(0, \sigma^2).
$$ 

The function `sits_patterns()` uses a GAM to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. This function is based on the R package `dtwSat`[@Maus2019], which implements the TWDTW time series matching method described in @Maus2016. The resulting patterns can be viewed using `plot`.

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Patterns for the samples for Mato Grosso."}
# Estimate the patterns for each class and plot them
samples_matogrosso_mod13q1 %>% 
    sits_patterns() %>% 
    plot()
```

The resulting patterns provide some insights over the time series behavior of each class. The response of the Forest class is quite distinctive.  They also shows that it should be possible to separate between the single cropping classes and the double cropping ones. There are similarities between the double-cropping classes (Soy_Corn, Soy_Millet, Soy_Sunflower and Soy_Sunflower) and  between the Cerrado and Pasture classes. The subtle differences between class signatures provide hints to possible ways by which machine leaning algorithms might distinguish between classes. One example is the difference between the middle-infrared response during the dry season (May to September) to distinguish between the Cerrado and Pasture classes. 

## Common interface to machine learning and deep learning models{-}

The `sits_train()` function provides a common interface to all machine learning models. This function takes two mandatory parameters: the training data (`samples`) and the ML algorithm (`ml_method`). After the model is estimated, it can be used to classify individual time series or data cubes with `sits_classify()`. In what follows, we show how to apply each method for the classification of a single time series. Then, in the "Classification of Images in Data Cubes" we discuss how to classify data cubes.

Since `sits` is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on  testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this chapter. 

When a set of time series organised as tibble is taken as input to the classifier, the result is the same tibble with one additional column ("predicted"), which contains the information on what labels are have been assigned for each interval. The results can be shown in text format using the function `sits_show_prediction()` or graphically using `plot()`.

## Random forest{-}

Random forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, where each decision tree is trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data, and the final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be useful in feature selection and data visualization. Pelletier et al[@Pelletier2016] discuss the robustness of random forest method for satellite image time series classification. 

```{r, echo = FALSE, out.width = "90%", fig.align="center", fig.cap="Random forests algorithm (source: Venkata Jagannath in Wikipedia - licenced as CC-BY-SA 4.0.)"}

knitr::include_graphics("images/random_forest.png") 
```

SITS provides `sits_rfor()`, which uses the R `randomForest` package [@Wright2017]; its main parameter is `num_trees`, which is the number of trees to grow with a default value of 100. The model can be visualized using `plot()`.

```{r, echo = FALSE}
set.seed(290356)
```

```{r, tidy="styler",out.width = "90%", fig.align="center", fig.cap="Most important variables in random forests model."}
# Train the Mato Grosso samples with Random Forests model.
rfor_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_rfor(num_trees = 100)
)
# plot the most important variables of the model
plot(rfor_model)
```

The most important explanatory variables are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band in date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes.  Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural classes and natural ones. This corresponds to summer time when the ground is drier and crops have been harvested.


```{r, tidy="styler",out.width = "90%", fig.align="center", fig.cap="Classification of time series using random forests."}
# Classify using Random Forest model and plot the result
point_class <- sits_classify(
    data = point_mt_mod13q1, 
    ml_model  = rfor_model
)
plot(point_class, bands = c("NDVI", "EVI"))
```

The result shows that the area started out as a forest in 2000, it was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. They are consistent with expert evaluation of the process of land use change in this region of Amazonia.

Random forests are robust to outliers and able to deal with irrelevant inputs [@Hastie2009]. The method tends to overemphasize some variables because its performance tends to stabilize after a part of the trees are grown [@Hastie2009]. In cases where abrupt change takes place, such deforestation mapping, random forests (if properly trained) will emphasize the temporal instances and bands that capture such quick change. 

## Support Vector Machines{-}

The support vector machine (SVM) classifier is a generalization of a linear classifier which finds an optimal separation hyperplane that minimizes misclassification [@Cortes1995]. Since a set of samples with $n$ features defines an n-dimensional feature space, hyperplanes are linear ${(n-1)}$-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations[@James2013]. The maximal margin is computed as the the smallest distance from the observations to the hyperplane. The solution for the hyperplane coefficients depends only on the samples that define the maximum margin criteria, the so-called support vectors.

```{r, echo = FALSE, out.width = "50%", fig.align="center", fig.cap="Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (source: Larhmam in Wikipedia - licensed as CC-BY-SA-4.0 )."}

knitr::include_graphics("images/svm_margin.png") 
```

For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. The new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundary in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been applied to classify remote sensing data [@Mountrakis2011].

In `sits`, SVM is implemented as a wrapper of `e1071` R package that uses the `LIBSVM` implementation [@Chang2011], the `sits` package adopts the *one-against-one* method for multiclass classification. For a $q$ class problem, this method creates ${q(q-1)/2}$ SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme.

The example below shows how to apply the SVM method for classification of time series using default values. The main parameters for the SVM are `kernel` which controls whether to use a non-linear transformation (default is `radial`), `cost` which measures the punishment for wrongly-classified samples (default is 10), and `cross` which sets the value of the k-fold cross validation (default is 10).

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using SVM."}
# Train a SVM model
svm_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_svm())
# Classify using SVM model and plot the result
point_class <- sits_classify(
    data = point_mt_mod13q1, 
    ml_model = svm_model
)
# plot the result
plot(point_class, bands = c("NDVI", "EVI"))

```
The SVM classifier is less stable and less robust to outliers than the random forests method. In this example, it tends to misclassify some of the data. In 2008, it is likely that the land cover was still "Pasture" rather than "Soy_Millet" as produced by the algorithm, while the "Soy_Cotton" class in 2012 is also inconsistent with the previous and latter classification of "Soy_Corn".


## Extreme Gradient Boosting{-}

The boosting method is based on the idea of starting from a weak predictor and then improving performance sequentially by fitting a better model at each iteration. It starts by fitting a simple classifier to the training data, and using the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although both random forests and boosting use trees for classification, there are important differences. The performance of random forests generally increases with the number of trees until it becomes stable. Boosting trees improve on previous result by applying finer divisions that improve the performance [@Hastie2009]. However, the number of trees grown by boosting techniques has to be limited at the risk of overfitting.

Gradient boosting is a variant of boosting methods where the cost function is minimized by gradient descent. Extreme gradient boosting [@Chen2016], called XGBoost, is an efficient approximation to the gradient loss function. Some recent papers show that it outperforms random forests for remote sensing image classification[@Jafarzadeh2021]. However, this result is not generalizable, since actual performance is controlled by the quality of the training data set.

In SITS, the XGBoost method is implemented by the `sits_xbgoost()` function, which is based on `XGBoost` R package and has five hyperparameters that require tuning. The `sits_xbgoost()` function takes the user choices as input to a cross validation to determine suitable values for the predictor.

The learning rate `eta` varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value `gamma` specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The `max_depth` value controls the maximum depth of the trees. Increasing this value will make the model more complex and more likely to overfit (default is 6). The `subsample` parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The `nrounds` parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. In order to follow the convergence of the algorithm, users can turn the `verbose` parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the random forests method.

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using XGBoost."}
# Train using  XGBoost
xgb_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_xgboost(verbose = 0)
)
# Classify using SVM model and plot the result
point_class <- sits_classify(
    data = point_mt_mod13q1, 
    ml_model = xgb_model
)
plot(point_class, bands = c("NDVI", "EVI"))
```


## Deep learning using multilayer perceptrons{-}

To support deep learning methods, `sits` uses the `torch` R package, which takes the Facebook `torch` C++ library as a back-end. Machine learning algorithms that use the R `torch` package are similar from those developed using `PyTorch`. Converting image time series algorithms developed in `PyTorch` to be using in `sits` is straightforward. Please see the chapter on "Extensibility" for guidance on how to include new deep learning algorithms in `sits`. 

The simplest deep learning method is multilayer perceptrons (MLPs), which are feedforward artificial neural networks. A MLP consists of three kinds of nodes: an input layer, a set of hidden layers and an output layer. The input layer has the same dimension as the number of the features in the data set. The hidden layers attempt to approximate the best classification function. The output layer makes a decision about which class should be assigned to the input.

In `sits`, users build MLP models using `sits_mlp()`. Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the `layers` parameters, which is a list of integer values. The size of the list is the number of layers and each element of the list indicates the number of nodes per layer.

The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.

MLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [@Goodfellow2016], we use the `relu` activation function.

Users can also define the optimization method (`optimizer`), which defines the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [@Ruder2016]. Based on experience with image time series, we recommend that users start by using the default method provided by `sits`, which is the `optimizer_adamw` method from package `torchopt`. Please refer to the `torchopt` package for additional information.

Another relevant parameter is the list of dropout rates (`dropout`). Dropout is a technique for randomly dropping units from the neural network during training [@Srivastava2014]. By randomly discarding some neurons, dropout reduces overfitting. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some neurons may seem a waste of resources. In practice, dropout prevents an early convergence to a local minimum [@Goodfellow2016]. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.

The following example shows how to use `sits_mlp()`. The default parameters for have been chosen based on a modified verion of @Wang2017, which proposes the use of multilayer perceptrons as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter `layers`; (b) Using the "relu" activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the "optimizer_adamw" as optimizer (default value); (e) a number of training steps (`epochs`) of 100; (f) a `batch_size` of 64, which indicates how many time series are used for input at a given steps; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set side for validation.

In our experience, if the training data is of good quality, using 3 to 5 layers is a reasonable compromise. Further increase on the number of layers will not improve the model. To simplify the output generation, the `verbose` option has been turned off. The default value is "on". After the model has been generated, we plot its training history. 

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Evolution of training accuracy of MLP model."}
# train using an MLP model
# this is an example of how to set parameters
# first-time users should test default options first
mlp_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_mlp(
        layers           = c(512, 512, 512),
        dropout_rates    = c(0.40, 0.30, 0.20),
        epochs           = 100,
        batch_size       = 64,
        verbose          = FALSE,
        validation_split = 0.2
    ) 
)
# show training evolution
plot(mlp_model)
```

Then, we classify a 16-year time series using the multilayer perceptron model.

```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using MLP."}
# Classify using DL model and plot the result
point_mt_mod13q1 %>% 
    sits_classify(mlp_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

In theory, multilayer perceptron model is able to capture more subtle changes than the random forests and XGBoost models. In this specific case, the result is similar to the them. Although the model mixes the "Soy_Corn" and "Soy_Millet" classes, the distinction between their temporal signatures is quite subtle. Also in this case, this suggests the need to improve the number of samples. In this examples, the MLP model shows an increase in the sensitivity compared to previous models. We recommend that the users compare different configurations, since the MLP model is sensitive to changes in its parameters.

## Temporal Convolutional Neural Network (TempCNN){-}

Convolutional neural networks (CNN) are a class of deep learning methods that apply convolution filters (sliding window) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture that is specifically designed for processing sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series and produces another time series as the result of the convolution. 

TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN is capable of capturing complex temporal patterns in the data and producing accurate predictions.

The TempCNN architecture for satellite image time series classification is proposed by Pelletier et al [@Pelletier2019].  It has three 1D convolutional layers, and a final softmax layer for classification (see figure). The authors use a combination of different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalisation. In the TempCNN reference paper [@Pelletier2019], the authors compare favorably their model with the Recurrent Neural Network proposed by Russwurm and Körner [@Russwurm2018] for land use classification. The figure below shows the architecture of the TempCNN model.

```{r, echo = FALSE, fig.align="center", out.width = "100%", fig.cap="Structure of tempCNN architecture (source: Pelletier et al.(2019))"}

knitr::include_graphics("images/tempcnn.png")
```

The function `sits_tempcnn()` implements the model. The parameter `cnn_layers` controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter `cnn_kernels` indicates the size of the convolution kernels; the default are kernels of size 7. Activation for all 1D-CNN layers uses the "relu" function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter `cnn_dropout_rates`. The `validation_split` controls the size of the test set, relative to the full data set. We recommend to set aside at least 20% of the samples for validation.

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of TempCNN model."}
library(torchopt)
# train using tempCNN
tempcnn_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_tempcnn(
                          optimizer            = torchopt::optim_adamw,
                          cnn_layers           = c(128, 128, 128),
                          cnn_kernels          = c(7, 7, 7),
                          cnn_dropout_rates    = c(0.2, 0.2, 0.2),
                          epochs               = 100,
                          batch_size           = 64,
                          validation_split     = 0.2,
                          verbose              = FALSE) )

# show training evolution
plot(tempcnn_model)
```

Then, we classify a 16-year time series using the TempCNN model.

```{r, out.width = "90%", tidy="styler", fig.align="center", fig.cap="Classification of time series using TempCNN."}
# Classify using TempCNN model and plot the result
class <- point_mt_mod13q1 %>% 
    sits_classify(tempcnn_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

The result has important differences from the previous ones. The TempCNN model indicates the "Soy_Cotton" class as the most likely one in 2004. While this result is likely to be wrong, it shows that the time series for year 2004 is different from those of "Forest" and "Pasture" classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, users could consider improving the training data by including samples that represent forest degradation. In our experience, TempCNN models are a reliable way for classifying image time series [@Simoes2021]. Recent work which compares different models also provides evidence of that TempCNN models have satisfactory behavior, especially in the case of crop classes [@Russwurm2020].

## Residual 1D CNN Networks (ResNet){-}

A residual 1D CNN network, also known as ResNet, is an extension of the standard 1D CNN architecture, with the addition of residual connections between the layers. Residual connections allow the network to learn residual mappings, which are the difference between the input and output of a layer. By adding these residual connections, the network can learn to bypass certain layers and still capture important features in the data.

The Residual Network (ResNet) for time series classification was proposed by Wang et al. [@Wang2017], based on the idea of deep residual networks for 2D image recognition [@He2016]. The ResNet architecture is composed of 11 layers, with three blocks of three 1D CNN layers each (see figure below). Each block corresponds to a 1D CNN architecture. The output of each block is combined with a shortcut that links its output to its input, called a "skip connection". The purpose of combining the input layer of each block with its output layer (after the convolutions) is to avoid the so-called "vanishing gradient problem". This issue occurs in deep networks as he neural network's weights are updated based on the partial derivative of the error function. If the gradient is too small, the weights will not be updated, stopping the training[@Hochreiter1998]. Skip connections aim to avoid vanishing gradients from occurring, allowing deep networks to be trained.

```{r, echo = FALSE, out.width = "100%", fig.align="center", fig.cap="Structure of ResNet architecture (source: Wang et al.(2017))."}
knitr::include_graphics("images/resnet.png")
```

In `sits`, the Residual Network is implemented using the `sits_resnet()` function. The default parameters are those proposed by Wang et al. [@Wang2017], as implemented by Fawaz et al.[@Fawaz2019]. The first parameter is `blocks`, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128 filters. Users can control the number of blocks and filter size by changing this parameter. The parameter `kernels` controls the size the of kernels of the three layers inside each block. We have found out that it is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is "relu", which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is `optim_adamw`, available in package `torchopt`.

```{r, tidy="styler", out.width = "100%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of ResNet model."}
# train using ResNet
resnet_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_resnet(
                          blocks               = c(64, 128, 128),
                          kernels              = c(7, 5, 3),
                          epochs               = 100,
                          batch_size           = 64,
                          validation_split     = 0.2,
                          verbose              = FALSE) )
# show training evolution
plot(resnet_model)
```

Then, we classify a 16-year time series using the ResNet model. The behaviour of the ResNet model is similar to that of TempCNN, with more variability.

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using ResNet."}
# Classify using DL model and plot the result
class <- point_mt_mod13q1 %>% 
    sits_classify(tempcnn_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```
 
## Attention-based models{-}

Attention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of an input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.

The basic idea behind attention-based models is to allow the model to selectively focus on different parts of the input at different times. This can be done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model can then use them to compute a weighted sum of the input. The results capture the model's attention on specific parts of the input.

Attention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that in applications such as language translation not all inputs are alike. Consider the English sentence "Look at all the lonely people". A good translation system needs to relate the words "look" and "people" as the key parts of this sentence and to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences [@Vaswani2017]. 

The basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms the input textual values into numerical vectors, and a decoder that processes these vectors and provides suitable answers. The difference is on how the values are handled internally. In multilayer perceptrons (MLP), all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most relevant combination of inputs that produces the best output. In convolutional nets (CNN), input values that are close in time (1D) or space (2D) are combined to produce higher-level information that helps to distinguish the different components of the input data. For text recognition, the initial choice of deep learning studies was to use recurrent neural networks (RNN) that handle input sequences sequentially. However, neither MLPs, nor CNNs or RNNs have been able to capture the structure of complex inputs such as natural language. The success of transformer-based solutions accounts for substantial improvements in natural language processing.

The two main differences between transformer models and other algorithms are the use of positional encoding and self-attention. Positional encoding assigns an index to each input value which ensures that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention works by comparing every word in the sentence to every other word in the sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT [@Devlin2019] and GPT-3 [@Brown2020].

The application of attention-based models for satellite image time series analysis is proposed by Garnot et [@Garnot2020a] and Russwurm and Körner [@Russwurm2020]. A self-attention network can learn to focus on specific time steps and image features that are most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use the observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In case of deforestation, the algorithm tries to identify the dates where the forest is being cut. Attention-based models are a means to indentify events that characterize each land use and land cover class.

The first model proposed by Garnot and co-authors is a full transformer-based model [@Garnot2020a]. Considering that image time series classification is a easier task that natural language processing, Garnot et al [@Garnot2020b] also propose a simplified version of the full transformer model. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of result. 

In `sits`, the full transformer-based model proposed by Garnot and co-authors [@Garnot2020a]   is implemented using the `sits_tae()` function. The default parameters are those proposed by the authors. The default optimizer is the same is `optim_adamw`, available in package `torchopt`.

```{r, tidy="styler", out.width = "100%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of Temporal Self-Attention model."}
# train a machine learning model using TAE
tae_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_tae(
                          epochs               = 150,
                          batch_size           = 64,
                          optimizer            = torchopt::optim_adamw,
                          validation_split     = 0.2,
                          verbose              = FALSE) )
# show training evolution
plot(tae_model)
```

Then, we classify a 16-year time series using the TAE model. 

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using TAE."}
# Classify using DL model and plot the result
class <- point_mt_mod13q1 %>% 
    sits_classify(tae_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

Garnot and co-authors [@Garnot2020a] also proposed the Lightweight Temporal Self-Attention Encoder.  the lightweight self-attention encoder model is that it can achieve high classification accuracy with fewer parameters compared to other neural network models. This makes it a good choice for applications where computational resources are limited. The `sits_lighttae()` function implements this algorithm. The default optimizer is `optim_adamw`, available in package `torchopt`. The most important parameter to be set is the learning rate `lr`. Values ranging from 0.001 to 0.005 should produce reasonable results. See also the section below on model tuning. 

```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of Lightweight Temporal Self-Attention model."}
# train a machine learning model using TAE
ltae_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_lighttae(
                          epochs               = 150,
                          batch_size           = 64,
                          optimizer            = torchopt::optim_adamw,
                          opt_hparams = list(lr = 0.001),
                          validation_split     = 0.2) )
# show training evolution
plot(ltae_model)
```

Then, we classify a 16-year time series using the LightTAE model. 

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using LightTAE."}
# Classify using DL model and plot the result
class <- point_mt_mod13q1 %>% 
    sits_classify(ltae_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```


The behaviour of both `sits_tae()` and `sits_lighttae()` is similar to that of `sits_tempcnn()`. It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated to the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty, since the classifiers do not agree on the resulting classes. In general, the deep learning temporal-aware models are more sensitive to class variability than random forests and extreme gradient boosters. 

## Model tuning{-}

Deep learning model tuning is the process of selecting the best set of hyperparameters for a specific application. Model tuning enables a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training, but instead are set prior to training and affect the behavior of the model during training. Examples of include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.

Deep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters and selecting the set that gives the best performance.

Deep learning algorithms try to find the optimal point that represents the best value of the prediction function that, given an input $X$ of data points, predicts the result $Y$. In our case, $X$ is a multidimensional time series and $Y$ is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search [@Bengio2012]. All gradient descent methods use an optimization algorithm that is adjusted with hyperparameters such as the learning rate and regularization rate [@Schmidt2021]. The learning rate controls the numerical step of the gradient descent function and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires the use of model tuning methods. 

To reduce the learning curve, `sits` provides default values for all machine learning and deep learning methods, which ensure a reasonable baseline performance. More experienced users may want to refine model hyperparameters, especially for more complex models such as `sits_lighttae()` or `sits_tempcnn()`. To that end, the package provides the `sits_tuning()` function. 

The simplest approach to model tuning is would be to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinational explosion and thus is not recommended. Instead, Bergstra and Bengio [@Bergstra2012] propose to use randomly chosen trials. In their paper, the authors show that random trials are more efficient than grid search trials, allowing the selection of adequate hyperparameters at a fraction of the computational cost. The `sits_tuning()` function follows Bergstra and Bengio [@Bergstra2012] and uses a random search on the chosen hyperparameters.

Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [@Bottou2018]. A large number of optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [@Schmidt2021]. For general deep learning applications, the Adam optimizer [@Kingma2017] provides a good baseline and reliable performance. For this reason, Adam  is the default optimizer in the R `torch` package. Experiments with image time series show that other optimizers may have better performance for the specific problem of land use classification. For this reason, the authors developed the  `torchopt` R package which includes a number of recently proposed optimizers, including Adamw  [@Loshchilov2019], Madgrad [@Defazio2021] and Yogi [@Zaheer2018]. Based on our experiments, we have selected Adamw as the default optimizer for deep learning methods. Using the `sits_tuning()` function allows testing these and other optimizers available in `torch` and `torch_opt` packages.

The `sits_tuning()` function takes the following parameters:

1. `samples` - Training data set to be used by the model.
2. `samples_validation` (optional) - If available, this data set contains time series to be used for validation. If missing, the next parameter will be used.
3. `validation_split` - If `samples_validation` is not used, this parameter defines the proportion of time series in the training data set to be used for validation (default is 20%).
4. `ml_method()`  - Deep learning method (either `sits_mlp()`, `sits_tempcnn()`, `sits_resnet()`, `sits_tae()` or `sits_lighttae()`)
5. `params` - defines the optimizer and its hyperparameters by calling the `sits_tuning_hparams()` function, as shown in the example below. 
6. `trials` - number of trials to run the random search.
7. `multicores` - number of cores to be used for the procedure.
8. `progress` - show progress bar?

The `sits_tuning_hparams()` function inside `sits_tuning()` allows users to define optimizers and their hyperparameters including `lr` (learning rate), `eps` (controls numerical stability) and `weight_decay` (controls overfitting). The default values for `eps` and `weight_decay` in all `sits` deep learning functions are 1.0e-08  and 1.0e-06, respectively. The default `lr` for `sits_lighttae()` and `sits_tempcnn()` is  0.005, and for `sits_tae()` and `sits_resnet()` is 0.001. Users have different ways to randomize the hyperparameters, including: `choice()` (a list of options), `uniform` (a uniform distribution), `randint` (random integers from a uniform distribution), `normal(mean, sd)` (normal distribution), `beta(shape1, shape2)`(beta distribution). These options allow extensive combination of hyperparameters.

In the example, the `sits_tuning()` function finds good hyperparameters to train the `sits_lighttae()` method for the Mato Grosso data set. It tests 100 combinations of learning rate and weight decay for the Adamw optimizer. To randomize the learning rate, it uses a beta distribution with parameters 0.35 and 10, which allows for variation between about 0.2 and 1.0e-00; for the weight decay, the beta distribution with parameters 0.1 and 2 generates values roughly between 1 and 1.0e-24. 

```{r, tidy="styler", eval = FALSE, echo = TRUE}
tuned <- sits_tuning(
     samples = samples_matogrosso_mod13q1,
     ml_method = sits_lighttae(),
     params = sits_tuning_hparams(
         optimizer = torchopt::optim_adamw,
         opt_hparams = list(
             lr = beta(0.35, 10),
             weight_decay = beta(0.1, 2)
         )
     ),
     trials = 100,
     multicores = 6,
     progress = FALSE
)
```

```{r, eval = TRUE, echo = FALSE}
tuned <- readRDS("./etc/tuned.rds")

```

The result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The 10 best results obtain accuracy values between 0.976 and 0.958, as shown below. The best result is obtained by a learning rate of 0.0011 and an weight decay of 2.14e-05, 

```{r} 
# obtain accuracy, kappa, lr and weight decay for the 10 best results
# hyperparameters are organized as a list
hparams_10 <- tuned[1:10,]$opt_hparams
# extract learning rate and weight decay from the list
lr_10 <- purrr::map_dbl(hparams_10, function(h) h$lr)
wd_10 <- purrr::map_dbl(hparams_10, function(h) h$weight_decay)

# create a tibble to display the results
best_10 <- tibble::tibble(
    accuracy = tuned[1:10,]$accuracy,
    kappa = tuned[1:10,]$kappa,
    lr    = lr_10,
    weight_decay = wd_10
)
# print the best combination of hyperparameters
best_10
```

For large data sets, the tuning process is time consuming. Despite this cost, it is recommended for achieving the best performance. In general, tuning hyperparameters for models such as `sits_tempcnn()` and `sits_lighttae()` will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer's and user's accuracies are possible. In cases where one wants to detect change in less frequent classes, tuning can make a difference in the results. 


## Considerations on model choice{-}

The development of machine learning methods for classification of satellite image time series is an ongoing task. There is a lot of recent work using methods such as convolutional neural networks [@Pelletier2019, @Wang2017, @Fawaz2020] and temporal self-attention [@Garnot2020a]. Given the rapid evolution of the field with new methods still being developed, there are few references that offer a comparison between different machine learning methods. Most works on the literature [@Wang2017, @Fawaz2019] compare methods for generic time series classification. Their insights are not directly applicable for satellite image time series data, which have different properties than the time series using in applications such as economics and health.

In the specific case of satellite image time series, Russwurm et al. [@Russwurm2020] present a comparative study between seven deep neural networks for classification of agricultural crops, using random forests (RF) as a baseline. The data is composed of Sentinel-2 images over Britanny, France. Their results indicate a slight difference between the best model (attention-based transformer model) over TempCNN, ResNet and RF. Attention-based models obtain accuracy ranging from 80-81%, TempCNN get 78-80%, and RF gets 78%. Based on this result and also on the authors' experience, we make the following recommendations:

1.  Random forests provide a good baseline for image time series classification and should be included in users' assessments. 

2.  XGBoost is an worthy alternative to Random forests. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.

3.  TempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.

4.  Attention-based models (TAE and LightTAE) can achieve the best overall performance, in case of well designed and balanced training sets and with hyperparameter tuning. 

5. The best means of improving classification performance is to provide an accurate and reliable training data set. Each class should have enough samples to account for spatial and temporal variability. 