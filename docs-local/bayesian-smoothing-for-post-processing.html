<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes</title>
<meta name="author" content="Gilberto Camara">
<meta name="author" content="Rolf Simoes">
<meta name="author" content="Felipe Souza">
<meta name="author" content="Charlotte Pelletier">
<meta name="author" content="Alber Sanchez">
<meta name="author" content="Pedro Ribeiro Andrade">
<meta name="author" content="Karine Ferreira">
<meta name="author" content="Gilberto Queiroz">
<meta name="description" content="Introduction Image classification post-processing has been defined as “a refinement of the labelling in a classified image in order to enhance its classification accuracy” [79]. In remote sensing...">
<meta name="generator" content="bookdown 0.33 with bs4_book()">
<meta property="og:title" content="Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover_sits_book.png">
<meta property="og:description" content="Introduction Image classification post-processing has been defined as “a refinement of the labelling in a classified image in order to enhance its classification accuracy” [79]. In remote sensing...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian smoothing for post-processing | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta name="twitter:description" content="Introduction Image classification post-processing has been defined as “a refinement of the labelling in a classified image in order to enhance its classification accuracy” [79]. In remote sensing...">
<meta name="twitter:image" content="/images/cover_sits_book.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/IBM_Plex_Serif-0.4.5/font.css" rel="stylesheet">
<link href="libs/IBM_Plex_Mono-0.4.5/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title=""><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="setup.html">Setup</a></li>
<li><a class="" href="acknowledgements.html">Acknowledgements</a></li>
<li><a class="" href="introduction-to-sits.html">Introduction to SITS</a></li>
<li><a class="" href="earth-observation-data-cubes.html">Earth observation data cubes</a></li>
<li><a class="" href="operations-on-data-cubes.html">Operations on Data Cubes</a></li>
<li><a class="" href="working-with-time-series.html">Working with time series</a></li>
<li><a class="" href="improving-the-quality-of-training-samples.html">Improving the Quality of Training Samples</a></li>
<li><a class="" href="machine-learning-for-data-cubes.html">Machine Learning for Data Cubes</a></li>
<li><a class="" href="image-classification-in-data-cubes.html">Image Classification in Data Cubes</a></li>
<li><a class="active" href="bayesian-smoothing-for-post-processing.html">Bayesian smoothing for post-processing</a></li>
<li><a class="" href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></li>
<li><a class="" href="uncertainty-and-active-learning.html">Uncertainty and active learning</a></li>
<li><a class="" href="ensemble-prediction-from-multiple-models.html">Ensemble Prediction from Multiple Models</a></li>
<li><a class="" href="visualising-and-exporting-data.html">Visualising and Exporting data</a></li>
<li><a class="" href="technical-annex.html">Technical Annex</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-smoothing-for-post-processing" class="section level1 unnumbered">
<h1>Bayesian smoothing for post-processing<a class="anchor" aria-label="anchor" href="#bayesian-smoothing-for-post-processing"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction" class="section level2 unnumbered">
<h2>Introduction<a class="anchor" aria-label="anchor" href="#introduction"><i class="fas fa-link"></i></a>
</h2>
<p>Image classification post-processing has been defined as “a refinement of the labelling in a classified image in order to enhance its classification accuracy” <span class="citation"><a href="references.html#ref-Huang2014" role="doc-biblioref">[79]</a></span>. In remote sensing image analysis, these procedures are used to combine pixel-based classification methods with a spatial post-processing method to remove outliers and misclassified pixels. For pixel-based classifiers, post-processing methods enable the inclusion of spatial information in the final results.</p>
<p>The <code>sits</code> package uses a <em>time-first, space-later</em> approach. Since machine learning classifiers in <code>sits</code> are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land-cover classification by incorporating spatial and contextual information into the classification process. Smoothing methods use the neighborhood data to remove outliers and enhance consistency in the resulting product.</p>
<p>Most statistical classifiers use training samples derived from “pure” pixels, that have been selected by users as representative of the desired output classes. However, images contain many mixed pixels irrespective of the resolution. Also, there is a considerable degree of data variability in each class. These effects lead to outliers whose chance of misclassification is significant. To offset these problems, most post-processing methods use the “smoothness assumption” <span class="citation"><a href="references.html#ref-Schindler2012" role="doc-biblioref">[80]</a></span>: nearby pixels tend to have the same label. To put this assumption in practice, smoothing methods in <code>sits</code> use the neighborhood information to remove outliers and enhance consistency in the resulting product.</p>
</div>
<div id="motivation" class="section level2 unnumbered">
<h2>Motivation<a class="anchor" aria-label="anchor" href="#motivation"><i class="fas fa-link"></i></a>
</h2>
<p>The smoothing method available in <code>sits</code> uses Bayesian inference. Bayesian inference can be thought of as way of allowing the inclusion of expert knowledge on the derivation of probabilities. As stated by <span class="citation"><a href="references.html#ref-Spiegelhalter2009" role="doc-biblioref">[81]</a></span>: “In the Bayesian paradigm, degrees of belief in states of nature are specified. Bayesian statistical methods start with existing ‘prior’ beliefs, and update these using data to give ‘posterior’ beliefs, which may be used as the basis for inferential decisions”. Bayesian inference has now been established as a major method for assessing probability.</p>
<p>Bayesian inference allows the inclusion of expert knowledge. The assumption is that, at local level, class probabilities should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels including spatial dependence.</p>
</div>
<div id="bayesian-estimation" class="section level2 unnumbered">
<h2>Bayesian estimation<a class="anchor" aria-label="anchor" href="#bayesian-estimation"><i class="fas fa-link"></i></a>
</h2>
<p>The Bayesian estimate is based on two random variables: (a) The observed class probabilities for each pixel denoted by a random variable <span class="math inline">\(p_{i,k}\)</span> where <span class="math inline">\(i\)</span> is the index of the pixel and <span class="math inline">\(k\)</span> indicates the class; (b) The underlying class probabilities for each pixel, denoted by a random variable <span class="math inline">\(\phi_{i,k}\)</span>. The probabilities <span class="math inline">\(p_{i,k}\)</span> are the output of the classifier and are subject to noise, outliers, and classification errors. Our estimation aims to remove these effects and obtain <span class="math inline">\(\phi_{i,k}\)</span> which would be a better approximation to the actual class probability.</p>
<p>To more our inference tractable, we first convert the class probability values <span class="math inline">\(p_{i,k}\)</span> to log-odds values using the logit function, as shown below. The logit function converts probability values from 0 to 1 to values from negative infinity to infinity. The conversion from probabilities ranging from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> to logit values is useful to support our assumption of normal distribution functions for our data.</p>
<p><span class="math display">\[
    x_{i,k} = \ln \left(\frac{p_{i,k}}{1 - p_{i,k}}\right)
\]</span>
In what follows, we will then consider two random variables for each pixel <span class="math inline">\(i\)</span>: (a) <span class="math inline">\(x_{i,k}\)</span>, the observed class logits; (b) <span class="math inline">\(\mu_{i,k}\)</span>, the inferred logit values. In other words, we measure <span class="math inline">\(x_{i,k}\)</span>, but want to obtain <span class="math inline">\(\mu_{i,k} | x_{i,k}\)</span>. The Bayesian inference procedure can be expressed as</p>
<p><span class="math display">\[
    \pi(\mu|x) \propto{} \pi(x|\mu)\pi(\mu).
\]</span>
To estimate the conditional posterior distribution <span class="math inline">\(\pi(\theta{}|x)\)</span>, we combine two distributions: (a) the distribution <span class="math inline">\(\pi(x|\mu)\)</span>, known as the likelihood function, which expresses the dependency of the measured values <span class="math inline">\(x_{i,k}\)</span> in the underlying values <span class="math inline">\(\mu_{i,k}\)</span>; and (b) <span class="math inline">\(\pi(\mu)\)</span>, which is our guess on the actual data distribution, known as the prior. For simplicity, we will also assume independence between the different classes <span class="math inline">\(k\)</span>, instead of considering a multivariate distribution. Therefore, the update will be performed for each class <span class="math inline">\(k\)</span> separately.</p>
<p>We assume that the likelihood <span class="math inline">\(x_{i,k} | \mu_{i,k}\)</span> follows a normal distribution, <span class="math inline">\(N(\mu_{i,k}, \sigma^2_{k})\)</span>, with parameters <span class="math inline">\(\mu_{i,k}\)</span> and <span class="math inline">\(\sigma^2_{k}\)</span>. The variance <span class="math inline">\(\sigma^2_{k}\)</span> will be estimated by users based on their expertise and will be used as a hyperparameter to control the level of smoothness of the resulting estimate. Therefore</p>
<p><span class="math display">\[
x_{i,k} | \mu_{i,k} \sim N(\mu_{i,k}, \sigma^2_{k})
\]</span>
is the likelihood function. We will assume a nomrmal local prior for the parameter <span class="math inline">\(\mu_{i,k}\)</span> with parameters <span class="math inline">\(m_{i,k}\)</span> and <span class="math inline">\(s^2_{i,k}\)</span>:</p>
<p><span class="math display">\[
\mu_{i,k} \sim N(m_{i,k}, s^2_{i,k}).
\]</span>
We estimate the local means and variances for the prior distribution by considering a spatial neighboring. Let <span class="math inline">\(\#(V_{i})\)</span> be the number of elements in the neighborhood <span class="math inline">\(V_{i}\)</span>. We then can calculate the mean value by</p>
<p><span class="math display">\[
m_{i,t,k} = \frac{\sum_{(j) \in V_{i}} x_{j,k}}{\#(V_{i})}
\]</span>
and the variance by
<span class="math display">\[
s^2_{i,k} = \frac{\sum_{(j) \in V_{i}} [x_{j,k} - m_{i,k}]^2}{\#(V_{i})-1}.    
\]</span>
Given these assumptions, the Bayesian update for the expected conditional mean <span class="math inline">\({E}[\mu_{i,k} | x_{i,k}]\)</span> is given by:
<span class="math display">\[
\begin{equation}
{E}[\mu_{i,k} | x_{i,k}] =
\frac{m_{i,t} \times \sigma^2_{k} +
x_{i,k} \times s^2_{i,k}}{ \sigma^2_{k} +s^2_{i,k}}
\end{equation}
\]</span></p>
<p>which can be expressed as a weighted mean</p>
<p><span class="math display">\[
{E}[\mu_{i,k} | x_{i,k}] =
\Biggl [ \frac{s^2_{i,k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times
x_{i,k} +
\Biggl [ \frac{\sigma^2_{k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times m_{i,k}
\]</span></p>
<p>where</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(x_{i,k}\)</span> is the logit value for pixel <span class="math inline">\(i\)</span> and class <span class="math inline">\(k\)</span>.</li>
<li>
<span class="math inline">\(m_{i,k}\)</span> is the average of logit values for pixels of class <span class="math inline">\(k\)</span> in the neighborhood of pixel <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(s^2_{i,k}\)</span> is the variance of logit values for pixels of class <span class="math inline">\(k\)</span> in the neighborhood of pixel <span class="math inline">\(i\)</span>.</li>
<li>
<span class="math inline">\(sigma^2_{k}\)</span> is the prior variance of the logit values for class <span class="math inline">\(k\)</span>.</li>
</ol>
<p>The above equation is weighted average between the value <span class="math inline">\(x_{i,k}\)</span> for the pixel and the mean <span class="math inline">\(m_{i,k}\)</span> for the neighboring pixels. When the variance <span class="math inline">\(s^2_{i,k}\)</span> for the neighbors is too high, the smoothing algorithm gives more weight to the pixel value <span class="math inline">\(x_{i,k}\)</span>. On the other hand, when the noise <span class="math inline">\(\sigma^2_k\)</span> increases, the method gives more weight to the neighborhood mean <span class="math inline">\(m_{i,k}\)</span>.</p>
<p>The parameter <span class="math inline">\(\sigma^2_k\)</span> controls the level of smoothness. If <span class="math inline">\(\sigma^2_k\)</span> is zero, the smoothed value <span class="math inline">\({E}[\mu_{i,k} | x_{i,k}]\)</span> will be equal to the pixel value <span class="math inline">\(x_{i,k}\)</span>. Making <span class="math inline">\(\sigma^2_k\)</span> high leads to a lot of smoothness. Values of the prior variance <span class="math inline">\(sigma^2_{k}\)</span> which are small relative to the local variance <span class="math inline">\(s^2_{i,k}\)</span> increase our confidence in the original probabilities. Conversely, values of the prior variance <span class="math inline">\(\sigma^2_{k}\)</span> which are big relative to the local variance <span class="math inline">\(s^2_{i,k}\)</span> increase our confidence in the average probability of the neighborhood.</p>
<p>Thus, the parameter <span class="math inline">\(\sigma^2_{k}\)</span> expresses our confidence on the inherent variability of the distribution of values of a class <span class="math inline">\(k\)</span>. The smaller the parameter <span class="math inline">\(\sigma^2_{k}\)</span>, the more we trust the estimated probability values produced by the classifier for class <span class="math inline">\(k\)</span>. Conversely, higher values of <span class="math inline">\(\sigma^2_{k}\)</span> indicate lower confidence on the outputs of the classifier and improved confidence on the values of the local average.</p>
<p>Consider the following two-class example. Take a pixel with probability 0.4 (logit <span class="math inline">\(x_{i,1}\)</span> = -0.4054) for class A, and probability 0.6 (logit <span class="math inline">\(x_{i,2}\)</span> = 0.4054) for class B. Without post-processing, the pixel will be labelled as class B. Consider that the local average is 0.6 (logit <span class="math inline">\(m_{i,1}\)</span> = 0.4054) for class A and 0.4 (logit <span class="math inline">\(m_{i,2}\)</span> = -0.4054) for class B. This is a case of an outlier classified originally as class B in the midst of a set of pixels of class A. Take the local variance of logits to be <span class="math inline">\(s^2_{i,1}\)</span> = 5 for class A and <span class="math inline">\(s^2_{i,2}\)</span> = 10 and for class B. This difference is to be expected if the local variability of class A is smaller than that of class B.</p>
<p>To complete the estimate, we need to set the parameter <span class="math inline">\(\sigma^2_{k}\)</span>, which represents our prior belief in the variability of the probability values for each class. If we take both <span class="math inline">\(\sigma^2_{A}\)</span> for class A and <span class="math inline">\(\sigma^2_{B}\)</span> for class B to be both 10, the Bayesian estimated probability for class A is 0.52 and for class B is 0.48. In this case, the pixel will be relabeled as being of class A. However, if our belief in the original values is higher, we will get a different result. If we set a value of <span class="math inline">\(\sigma^2\)</span> to be 5 for both classes A and B, the Bayesian probability estimate will be 0.48 for class A and will be 0.52 for class B. In this case, the original label will be kept.</p>
<p>We make the following recommendations for setting the <span class="math inline">\(\sigma^2_{k}\)</span> parameter:</p>
<ol style="list-style-type: decimal">
<li><p>Set the <span class="math inline">\(\sigma^2_{k}\)</span> parameter with high values (20 or above) to increase the neighborhood influence compared with the probability values for each pixel. Classes whose probabilities have strong spatial autocorrelation will tend to replace outliers of different classes.</p></li>
<li><p>Set the <span class="math inline">\(\sigma^2_{k}\)</span> parameter with low values (5 or below) to reduce the neighborhood influence compared with the probabilities for each pixel of class <span class="math inline">\(k\)</span>. In this way, classes which have low spatial autocorrelation are more likely not to be relabeled.</p></li>
</ol>
<p>Consider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests, without many outliers inside them, she would set the <span class="math inline">\(\sigma^2\)</span> parameter for the class “Forest” to be high. For comparison, one usually wants to avoid that small watersheds which have few similar neighbors be relabeled. In this case, it is advisable to avoid a strong influence of the neighbors and <span class="math inline">\(\sigma^2\)</span> should be set as low as possible.</p>
</div>
<div id="defining-the-neighborhood" class="section level2 unnumbered">
<h2>Defining the neighborhood<a class="anchor" aria-label="anchor" href="#defining-the-neighborhood"><i class="fas fa-link"></i></a>
</h2>
<p>The intuition for Bayesian smoothing is that homogeneous neighborhoods should have the same class. In homogeneous neighborhoods, the dominant class has both higher average probabilities and lower variance than the other classes. In these neighborhoods, a pixel of a different class is likely to be associated to lower average probabilities and higher local variance. Mixed pixels at the limits between areas with different classes pose a problem for classification. These pixels contain signatures of two classes. To account for these cases, Bayesian smoothing in <code>sits</code> uses a special definition of a neighborhood.</p>
<p>To be reliable, local class statistics should only include pixels that are likely to belong to such class. Windows centered on border pixels contain only some pixels belonging to same class as the central pixel; the others belongs to a different class. Consider a window of size 7 x 7 around a pixel in the probability map of class “Forest”. It will contain the central pixel and 80 neighbors. Instead of using all those neighbors to compute the local statistics, <code>sits</code> uses only some of them. The neighbors used to calculate the local statistics is set by the taking only those with the highest probability of belonging to class “Forest”. Such percentage of pixels per windows to calculate the local statistics is user-controllable.</p>
</div>
<div id="measuring-the-local-variance" class="section level2 unnumbered">
<h2>Measuring the local variance<a class="anchor" aria-label="anchor" href="#measuring-the-local-variance"><i class="fas fa-link"></i></a>
</h2>
<p>As discussed above, the effect of the Bayesian estimator depends on the values of the a priori variance <span class="math inline">\(sigma^2_{k}\)</span> set by the user and of the local variance <span class="math inline">\(s^2_{i,1}\)</span> measured for each pixel. To illustrate the impact of the choices of the <span class="math inline">\(sigma^2_{k}\)</span> parameter, we present a detailed example. The first step is to take a probability cube for a deforestation detection application in an area of the Brazilian Amazon. This cube has been produced by a random forest model with 6 classes. We first build the data cube and then plot the probabilities for classes “Water” and “Forest”.</p>
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># define the classes of the probability cube</span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"Water"</span>, <span class="st">"ClearCut_Burn"</span>, <span class="st">"ClearCut_Soil"</span>,</span>
<span>  <span class="st">"ClearCut_Veg"</span>, <span class="st">"Forest"</span>, <span class="st">"Wetland"</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># directory where the data is stored</span></span>
<span><span class="va">data_dir</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/system.file.html">system.file</a></span><span class="op">(</span><span class="st">"extdata/Rondonia-20LLQ/"</span>, package <span class="op">=</span> <span class="st">"sitsdata"</span><span class="op">)</span></span>
<span><span class="co"># create a probability data cube from a file</span></span>
<span><span class="va">probs_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_cube.html">sits_cube</a></span><span class="op">(</span></span>
<span>  source <span class="op">=</span> <span class="st">"MPC"</span>,</span>
<span>  collection <span class="op">=</span> <span class="st">"SENTINEL-2-L2A"</span>,</span>
<span>  data_dir <span class="op">=</span> <span class="va">data_dir</span>,</span>
<span>  bands <span class="op">=</span> <span class="st">"probs"</span>,</span>
<span>  labels <span class="op">=</span> <span class="va">labels</span>,</span>
<span>  parse_info <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"X1"</span>, <span class="st">"X2"</span>, <span class="st">"tile"</span>, <span class="st">"start_date"</span>, <span class="st">"end_date"</span>, <span class="st">"band"</span>, <span class="st">"version"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># plot the probabilities for water and forest</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">probs_cube</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Water"</span>, <span class="st">"Forest"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-121"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-121-1.png" alt="Probability map produced for classes Forest and Water." width="100%"><p class="caption">
Figure 72: Probability map produced for classes Forest and Water.
</p>
</div>
<p>The probability map for class “Forest” shows high values associated to compact patches as well linear stretches in riparian areas. By contrast, the probability map for class “Water” has mostly low values, except in a few places with high chance of occurrence of this class. To further understand the behavior of the Bayesian estimator, it is useful to examine the local variance associated the logits of the probabilities.</p>
<p>The <code>sits_variance</code> function estimates the local variances for the logits, which correspond to the <span class="math inline">\(s^2_{i,k}\)</span> parameter in the Bayesian estimator. It has the following parameters: (a) <code>cube</code>, a probability cube; (b) <code>window_size</code>, dimension of the local neighborhood; (c) <code>neigh_fraction</code>, percentage of pixels in the neighborhood which will be used to calculate the variance; (d) <code>multicores</code>, number of CPU cores that will be used for processing; (e) <code>memsize</code>, memory available; (f) <code>output_dir</code>, directory where results will be stored; (g) <code>version</code>, for version control. In the example below, we will use half of the pixels of a 7x7 window to estimate the variance. The chosen pixels will be those with highest probability pixels, to be more representative of the actual class distribution. The output values are the variances of the logits.</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">var_cube</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_variance.html">sits_variance</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  window_size <span class="op">=</span> <span class="fl">7</span>,</span>
<span>  neigh_fraction <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">24</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-n05"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">var_cube</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Water"</span>, <span class="st">"Forest"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-122"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-122-1.png" alt="Variance map for class Forest." width="100%"><p class="caption">
Figure 73: Variance map for class Forest.
</p>
</div>
<p>The plot for the “Forest” class shows that the areas of low variance are associated both to dense forest patches as well as areas where trees have been completely remove. Areas of high variance are mostly associated with the borders between forest areas and the other classes. These high values By contrast, the plot for the “Water” class is not informative, with small areas of high variance located near the areas of high water probability. Both plots show that most variance values are low, and high values reach 30. This information is relevant for setting the values of the prior variance <span class="math inline">\(\sigmaˆ2\)</span> as discussed below.</p>
</div>
<div id="running-bayesian-smoothing" class="section level2 unnumbered">
<h2>Running Bayesian smoothing<a class="anchor" aria-label="anchor" href="#running-bayesian-smoothing"><i class="fas fa-link"></i></a>
</h2>
<p>To run Bayesian smoothing we use the function <code><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth()</a></code> with parameters: (a) <code>cube</code>, a probability cube produced by <code><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify()</a></code>; (b) <code>window_size</code>, the local window to compute the neighborhood probabilities; (d) <code>neigh_fraction</code>, fraction of local neighbors used to calculate local statistics; (e) <code>smoothness</code>, a vector with estimates of the prior variance of each class; (f) <code>multicores</code>, number of CPU cores that will be used for processing; (g) <code>memsize</code>, memory available for classification; (h) <code>output_dir</code>, directory where results will be stored; (i) <code>version</code>, for version control. The resulting cube can be visualized with <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code>. In what follows, we compare the smoothing effect by varying the <code>window_size</code> and <code>smoothness</code> parameters.</p>
<p>Together, the parameters <code>window_size</code> and <code>neigh_fraction</code> control how many pixels in a neighborhood are used to calculate the local statistics used by the Bayesian estimator. For example, setting <code>window size</code> to 7 and <code>neigh_fraction</code> to 0.5 (the defaults) ensures that 25 samples are used to estimate the local statistics.</p>
<p>Our first reference is the classified map without smoothing which shows the presence of outliers and classification errors. To obtain it, we use the function <code>sits_label_classification</code> taking the probability map as an input, as follows.</p>
<div class="sourceCode" id="cb124"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># generate thematic map</span></span>
<span><span class="va">class_map</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"no_smooth"</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># plot the result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">class_map</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-123"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-123-1.png" alt="Classified map without smoothing." width="100%"><p class="caption">
Figure 74: Classified map without smoothing.
</p>
</div>
<p>To remove the outliers and classification errors, we will run a smoothing procedure where all prior variances are set the same value of 20 which is relatively high compared with the maximum local class variance shown above. In this case, for most situations, the new value of the</p>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># compute Bayesian smoothing</span></span>
<span><span class="va">cube_smooth_w7_f05_s20</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  window_size <span class="op">=</span> <span class="fl">7</span>,</span>
<span>  neigh_fraction <span class="op">=</span> <span class="fl">0.50</span>,</span>
<span>  smoothness <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-s20"</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># plot the result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cube_smooth_w7_f05_s20</span>, labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Water"</span>, <span class="st">"Forest"</span><span class="op">)</span>, palette <span class="op">=</span> <span class="st">"YlGn"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-124"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-124-1.png" alt="Probability maps after bayesian smoothing." width="100%"><p class="caption">
Figure 75: Probability maps after bayesian smoothing.
</p>
</div>
<p>Bayesian smoothing has removed some of local variability associated to misclassified pixels which are different from their neighbors. There is a side effect: the water areas surrounded by forests have not been preserved in the forest probability map. The impact of smoothing is best appreciated comparing the labelled map produced without smoothing to the one that follows the procedure, as shown below.</p>
<div class="sourceCode" id="cb126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># generate thematic map</span></span>
<span><span class="va">defor_map_w7_f05_20</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">cube_smooth_w7_f05_s20</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp8"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-s20"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">defor_map_w7_f05_20</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-125"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-125-1.png" alt="Final classification map after Bayesian smoothing with 7 x 7 window, neigh_fraction = 0.5 and smoothness = 20." width="100%"><p class="caption">
Figure 76: Final classification map after Bayesian smoothing with 7 x 7 window, neigh_fraction = 0.5 and smoothness = 20.
</p>
</div>
<p>In the smoothed map, the outliers have been removed by the expansion of forest areas. Small corridors of water and soil encircled by trees have been replaced by forests. This effect is due to high probability of forest detection in the training data. To keep the water areas and reducing the expansion of the forest area, a viable alternative is to reduce the smoothness (<span class="math inline">\(\sigma^2\)</span>) for the “Forest” and “Water” classes. In this way, the local influence of the forest in the other classes is reduced. As for the water areas, since they are narrow, their neighborhoods will have many low probability values, which would reduce the expected value of the bayesian estimator.</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># reduce smoothing for classes Water and Forest</span></span>
<span><span class="co"># labels:  "Water", "ClearCut_Burn", "ClearCut_Soil",</span></span>
<span><span class="co">#          "ClearCut_Veg", "Forest", "Wetland"</span></span>
<span><span class="va">smooth_water_forest</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">20</span>, <span class="fl">20</span>, <span class="fl">20</span>, <span class="fl">5</span>, <span class="fl">20</span><span class="op">)</span></span>
<span><span class="co"># compute Bayesian smoothing</span></span>
<span><span class="va">cube_smooth_w7_f05_swf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_smooth.html">sits_smooth</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">probs_cube</span>,</span>
<span>  window_size <span class="op">=</span> <span class="fl">7</span>,</span>
<span>  neigh_fraction <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span>  smoothness <span class="op">=</span> <span class="va">smooth_water_forest</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-swf"</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># computed labelled map</span></span>
<span><span class="va">defor_map_w7_f05_swf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_label_classification.html">sits_label_classification</a></span><span class="op">(</span></span>
<span>  cube <span class="op">=</span> <span class="va">cube_smooth_w7_f05_swf</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  memsize <span class="op">=</span> <span class="fl">12</span>,</span>
<span>  output_dir <span class="op">=</span> <span class="st">"./tempdir/chp10"</span>,</span>
<span>  version <span class="op">=</span> <span class="st">"w7-f05-swf"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">defor_map_w7_f05_swf</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-126"></span>
<img src="sitsbook_files/figure-html/unnamed-chunk-126-1.png" alt="Probability maps after bayesian smoothing with 7 x 7 window with low smoothness for classes Water and Forest" width="100%"><p class="caption">
Figure 77: Probability maps after bayesian smoothing with 7 x 7 window with low smoothness for classes Water and Forest
</p>
</div>
<p>Comparing the two maps, the narrow water streams inside the forest area have been better preserved. Small corridors between forest areas have also been maintained. A better comparison between the two maps requires importing them into QGIS. Exporting data from <code>sits</code> to QGIS is discussed in the chapter “Visualising and Exporting data”.</p>
<p>In conclusion, post-processing is a desirable step in any classification process. Bayesian smoothing improves the borders between the objects created by the classification and remove outliers that result from pixel-based classification. It is a reliable methods that should be used in most situations.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="image-classification-in-data-cubes.html">Image Classification in Data Cubes</a></div>
<div class="next"><a href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-smoothing-for-post-processing">Bayesian smoothing for post-processing</a></li>
<li><a class="nav-link" href="#introduction">Introduction</a></li>
<li><a class="nav-link" href="#motivation">Motivation</a></li>
<li><a class="nav-link" href="#bayesian-estimation">Bayesian estimation</a></li>
<li><a class="nav-link" href="#defining-the-neighborhood">Defining the neighborhood</a></li>
<li><a class="nav-link" href="#measuring-the-local-variance">Measuring the local variance</a></li>
<li><a class="nav-link" href="#running-bayesian-smoothing">Running Bayesian smoothing</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</strong>" was written by Gilberto Camara, Rolf Simoes, Felipe Souza, Charlotte Pelletier, Alber Sanchez, Pedro Ribeiro Andrade, Karine Ferreira, Gilberto Queiroz. It was last built on 2023-04-10.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
