<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Technical Annex | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes</title>
<meta name="author" content="Gilberto Camara">
<meta name="author" content="Rolf Simoes">
<meta name="author" content="Felipe Souza">
<meta name="author" content="Charlotte Pelletier">
<meta name="author" content="Alber Sanchez">
<meta name="author" content="Pedro Ribeiro Andrade">
<meta name="author" content="Karine Ferreira">
<meta name="author" content="Gilberto Queiroz">
<meta name="description" content="This chapter contains technical details on the algorithms available in sits. It is intended to support those that want to understand how the package works and also want to contribute to its...">
<meta name="generator" content="bookdown 0.32 with bs4_book()">
<meta property="og:title" content="Technical Annex | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover_sits_book.png">
<meta property="og:description" content="This chapter contains technical details on the algorithms available in sits. It is intended to support those that want to understand how the package works and also want to contribute to its...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Technical Annex | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta name="twitter:description" content="This chapter contains technical details on the algorithms available in sits. It is intended to support those that want to understand how the package works and also want to contribute to its...">
<meta name="twitter:image" content="/images/cover_sits_book.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/IBM_Plex_Serif-0.4.4/font.css" rel="stylesheet">
<link href="libs/IBM_Plex_Mono-0.4.4/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title=""><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="setup.html">Setup</a></li>
<li><a class="" href="acknowledgements.html">Acknowledgements</a></li>
<li><a class="" href="introduction-to-sits.html">Introduction to SITS</a></li>
<li><a class="" href="earth-observation-data-cubes.html">Earth observation data cubes</a></li>
<li><a class="" href="operations-on-data-cubes.html">Operations on Data Cubes</a></li>
<li><a class="" href="working-with-time-series.html">Working with time series</a></li>
<li><a class="" href="improving-the-quality-of-training-samples.html">Improving the Quality of Training Samples</a></li>
<li><a class="" href="machine-learning-for-data-cubes.html">Machine Learning for Data Cubes</a></li>
<li><a class="" href="image-classification-in-data-cubes.html">Image Classification in Data Cubes</a></li>
<li><a class="" href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></li>
<li><a class="" href="uncertainty-and-active-learning.html">Uncertainty and active learning</a></li>
<li><a class="" href="ensemble-prediction-from-multiple-models.html">Ensemble Prediction from Multiple Models</a></li>
<li><a class="" href="visualising-and-exporting-data.html">Visualising and Exporting data</a></li>
<li><a class="active" href="technical-annex.html">Technical Annex</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="technical-annex" class="section level1 unnumbered">
<h1>Technical Annex<a class="anchor" aria-label="anchor" href="#technical-annex"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter contains technical details on the algorithms available in <code>sits</code>. It is intended to support those that want to understand how the package works and also want to contribute to its development.</p>
<div id="bayesian-smoothing-1" class="section level2 unnumbered">
<h2>Bayesian smoothing<a class="anchor" aria-label="anchor" href="#bayesian-smoothing-1"><i class="fas fa-link"></i></a>
</h2>
<p>Doing post-processing using Bayesian smoothing in SITS is straightforward. The result of the <code>sits_classify</code> function applied to a data cube is set of probability images, one per class. The next step is to apply the <code>sits_smooth</code> function. By default, this function selects the most likely class for each pixel considering only the probabilities of each class for each pixel. For continuous probability distributions, Bayesian inference is expressed by the rule:</p>
<p><span class="math display">\[
\pi(\theta|x) \propto \pi(x|\theta)\pi(\theta)
\]</span></p>
<p>Bayesian inference involves the estimation of an unknown parameter <span class="math inline">\(\theta\)</span>, which is the random variable that describe what we are trying to measure. In the case of smoothing of image classification, <span class="math inline">\(\theta\)</span> is the class probability for a given pixel, conditioned by the probability values of that pixel. We model our initial belief about this value by a probability distribution, <span class="math inline">\(\pi(\theta)\)</span>, called the distribution. It represents what we know about <span class="math inline">\(\theta\)</span> observing the data. The distribution <span class="math inline">\(\pi(x|\theta)\)</span>, called the , is estimated based on the observed data. It represents the added information provided by our observations. The distribution <span class="math inline">\(\pi(\theta|x)\)</span> is our improved belief of <span class="math inline">\(\theta\)</span> seeing the data. Bayesâ€™s rule states that the probability is proportional to the product of the and the probability.</p>
<div id="derivation-of-bayesian-parameters-for-spatiotemporal-smoothing" class="section level3 unnumbered">
<h3>Derivation of bayesian parameters for spatiotemporal smoothing<a class="anchor" aria-label="anchor" href="#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing"><i class="fas fa-link"></i></a>
</h3>
<p>In our post-classification smoothing model, we consider the output of a machine learning algorithm that provides the probabilities of each pixel in the image to belong to target classes. More formally, consider a set of <span class="math inline">\(K\)</span> classes that are candidates for labelling each pixel. Let <span class="math inline">\(p_{i,t,k}\)</span> be the probability of pixel <span class="math inline">\(i\)</span> belonging to class <span class="math inline">\(k\)</span>, <span class="math inline">\(k = 1, \dots, K\)</span>. We have
<span class="math display">\[
\sum_{k=1}^K p_{i,k} = 1, p_{i,k} &gt; 0
\]</span>
We label a pixel <span class="math inline">\(p_i\)</span> as being of class <span class="math inline">\(k\)</span> if
<span class="math display">\[
    p_{i, k} &gt; p_{i,m}, \forall m = 1, \dots, K, m \neq k
\]</span></p>
<p>For each pixel <span class="math inline">\(i\)</span>, we take the odds of the classification for class <span class="math inline">\(k\)</span>, expressed as
<span class="math display">\[
    O_{i,k} = p_{i,k} / (1-p_{i,k})
\]</span>
where <span class="math inline">\(p_{i, k}\)</span> is the probability of class <span class="math inline">\(k\)</span>. We have more confidence in pixels with higher odds since their class assignment is stronger. There are situations, such as border pixels or mixed ones, where the odds of different classes are similar in magnitude. We take them as cases of low confidence in the classification result. To assess and correct these cases, Bayesian smoothing methods borrow strength from the neighbors and reduces the variance of the estimated class for each pixel.</p>
<p>We further make the transformation
<span class="math display">\[
    x_{i,k} = \log [O_{i,k}]
\]</span>
which measures the <em>logit</em> (log of the odds) associated to classifying the pixel <span class="math inline">\(i\)</span> as being of class <span class="math inline">\(k\)</span>. The support of <span class="math inline">\(x_{i, k}\)</span> is <span class="math inline">\(\mathbb{R}\)</span>. We can express the pixel data as a <span class="math inline">\(K\)</span>-dimensional multivariate logit vector</p>
<p><span class="math display">\[
\mathbf{x}_{i}=(x_{i,k_{0}},x_{i,k_{1}},\dots{},x_{i,k_{K}})
\]</span></p>
<p>For each pixel, the random variable that describes the class probability <span class="math inline">\(k\)</span> is denoted by <span class="math inline">\(\theta_{i,k}\)</span>. This formulation allows uses to use the class covariance matrix in our formulations. We can express Bayesâ€™ rule for all combinations of pixel and classes for a time interval as</p>
<p><span class="math display">\[
\pi(\boldsymbol\theta_{i}|\mathbf{x}_{i}) \propto \pi(\mathbf{x}_{i}|\boldsymbol\theta_{i})\pi(\boldsymbol\theta_{i}).  
\]</span></p>
<p>We assume the conditional distribution <span class="math inline">\(\mathbf{x}_{i}|\boldsymbol\theta_{i}\)</span> follows a multivariate normal distribution</p>
<p><span class="math display">\[
    [\mathbf{x}_{i}|\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\boldsymbol\theta_{i},\boldsymbol\Sigma_{i}),
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol\theta_{i}\)</span> is the mean parameter vector for the pixel <span class="math inline">\(i\)</span>, and <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span> is a known <span class="math inline">\(k\times{}k\)</span> covariance matrix that we will use as a parameter to control the level of smoothness effect. We will discuss later on how to estimate <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span>. To model our uncertainty about the parameter <span class="math inline">\(\boldsymbol\theta_{i}\)</span>, we will assume it also follows a multivariate normal distribution with hyper-parameters <span class="math inline">\(\mathbf{m}_{i}\)</span> for the mean vector, and <span class="math inline">\(\mathbf{S}_{i}\)</span> for the covariance matrix.</p>
<p><span class="math display">\[
    [\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\mathbf{m}_{i}, \mathbf{S}_{i}).
\]</span></p>
<p>The above equation defines our prior distribution. The hyper-parameters <span class="math inline">\(\mathbf{m}_{i}\)</span> and <span class="math inline">\(\mathbf{S}_{i}\)</span> are obtained using the neighboring pixels of pixel <span class="math inline">\(i\)</span>. The neighborhood can be defined as any graph scheme (e.g.Â a given Chebyshev distance on the time-space lattice) and can include the referencing pixel <span class="math inline">\(i\)</span> as a neighbor. More formally, let</p>
<p><span class="math display">\[
    \mathbf{V}_{i}=\{\mathbf{x}_{i_{j}}\}_{j=1}^{N},
\]</span>
denote the <span class="math inline">\(N\)</span> logit vectors of a spatiotemporal neighborhood <span class="math inline">\(N\)</span> of pixel <span class="math inline">\(i\)</span>. Then the prior mean is calculated by</p>
<p><span class="math display">\[
    \mathbf{m}_{i}=\operatorname{E}[\mathbf{V}_{i}],
\]</span></p>
<p>and the prior covariance matrix by</p>
<p><span class="math display">\[
    \mathbf{S}_{i}=\operatorname{E}\left[
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)^\intercal
    \right].
\]</span>
The <span class="math inline">\(\boldsymbol\theta_{i}\)</span> parameter model is our initial belief about a pixel vector using the neighborhood information in the prior distribution. It represents what we know about the probable value of <span class="math inline">\(\mathbf{x}_{i}\)</span> (and hence, about the class probabilities as the logit function is a monotonically increasing function) observing it. The function <span class="math inline">\(P[\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}]\)</span> represents the added information provided by our observation of <span class="math inline">\(\mathbf{x}_{i,t}\)</span>. The probability density function <span class="math inline">\(P[\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]\)</span> is our improved belief of the pixel vector seeing <span class="math inline">\(\mathbf{x}_{i,t}\)</span>.</p>
<p>Since the likelihood and prior are multivariate normal distributions, the posterior will also be a multivariate normal distribution, whose updated parameters can be derived by applying the density functions associated to the above equations. The posterior distribution is given by</p>
<p><span class="math display">\[
    [\boldsymbol\theta_{i}|\mathbf{x}_{i}]\sim\mathcal{N}_{K}\left(
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}( \mathbf{S}_{i}^{-1}\mathbf{m}_{i} + \boldsymbol\Sigma^{-1} \mathbf{x}_{i}),
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}
    \right).
\]</span>
At this point, we are able to infer the estimator <span class="math inline">\(\hat{\boldsymbol\theta}_{i}\)</span> for the <span class="math inline">\(\boldsymbol\theta_{i}|\mathbf{x}_{i}\)</span> parameter. For the multivariate normal distribution, the posterior mean minimizes the quadratic loss but the absolute and zero-one loss functions. It can be taken from the updated mean parameter of the posterior distribution which, after some algebra, can be expressed as</p>
<p><span class="math display">\[
    \hat{\boldsymbol{\theta}}_{i}=\operatorname{E}[\boldsymbol\theta_{i}|\mathbf{x}_{i}]=\boldsymbol\Sigma_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{m}_{i} +
    \mathbf{S}_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{x}_{i}.
\]</span></p>
<p>The estimator value for the logit vector <span class="math inline">\(\hat{\boldsymbol\theta}_{i}\)</span> is a weighted average of the original logit vector <span class="math inline">\(\mathbf{x}_{i}\)</span> and the neighborhood mean vector <span class="math inline">\(\mathbf{m}_{i}\)</span>. The weights are given by the covariance matrix <span class="math inline">\(\mathbf{S}_{i}\)</span> of the prior distribution and the covariance matrix of the conditional distribution. The matrix <span class="math inline">\(\mathbf{S}_{i}\)</span> is calculated considering the neighbors and the matrix <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span> is the smoothing factor provided as prior belief by the user.</p>
<p>When the values of local class covariance <span class="math inline">\(\mathbf{S}_{i}\)</span> are higher than those the conditional covariance <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span>, our confidence on the influence of the neighbors is low, and the smoothing algorithm gives more weight to the original pixel value <span class="math inline">\(x_{i,k}\)</span>. When the local class covariance <span class="math inline">\(\mathbf{S}_{i}\)</span> decreases relative to the smoothness factor <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span>, our confidence on the influence of the neighborhood increases. The smoothing procedure will be most relevant in situations where the original classification odds ratio is low, showing a low level of separability between classes. In these cases, the updated values of the classes will be influenced by the local class variances.</p>
<p>In practice, <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span> is a user-controlled covariance matrix parameter that will be set by users based on their knowledge of the region to be classified. In the simplest case, users can associate the conditional covariance <span class="math inline">\(\boldsymbol\Sigma_{i}\)</span> to a diagonal matrix, using only one hyperparameter <span class="math inline">\(\sigma^2_k\)</span> to set the level of smoothness. Higher values of <span class="math inline">\(\sigma^2_k\)</span> will cause the assignment of the local mean to the pixel updated probability. In our case, after some classification tests, we decided to use <span class="math inline">\(\sigma^2_k=20\)</span> by default for all <span class="math inline">\(k\)</span>.</p>
<p>The version implemented in <code>sits</code> includes the following parameters</p>
<ul>
<li>
<code>smoothness</code>: user-controlled parameter that controls the influence of the neighborhood values on the probabibility value of each class of a pixel. Can be defined as a unique value for all classes, or a matrix whose size of size <span class="math inline">\(num_classes * num_classes\)</span>. The default value is 20, but users are encouraged define <code>smoothness</code> as a diagonal matrix whose values reflect the relative importance of each class in the output product.</li>
<li>
<code>window_size</code>: size of <span class="math inline">\(n*n\)</span> window used to define the neighborhood.</li>
<li>
<code>neigh_fraction</code>: percentage of pixels per window used to calculate local class statistics. The aim is to use only those pixels in the window that are likely to be part of the same class as the central pixel. For example, for a <code>window_size</code> of size 9 and <code>neigh_fraction</code> of 0.5, half of the pixels of the 9 x 9 window (those with higher class probabibility) will be used to estimate the local class statistics <span class="math inline">\(\mathbf{m}_{i,t}\)</span> and <span class="math inline">\(\mathbf{S}_{i,t}\)</span> which represent the mean and standard deviation of the classes associated to prox</li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="visualising-and-exporting-data.html">Visualising and Exporting data</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#technical-annex">Technical Annex</a></li>
<li>
<a class="nav-link" href="#bayesian-smoothing-1">Bayesian smoothing</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#derivation-of-bayesian-parameters-for-spatiotemporal-smoothing">Derivation of bayesian parameters for spatiotemporal smoothing</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</strong>" was written by Gilberto Camara, Rolf Simoes, Felipe Souza, Charlotte Pelletier, Alber Sanchez, Pedro Ribeiro Andrade, Karine Ferreira, Gilberto Queiroz. It was last built on 2023-03-06.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
