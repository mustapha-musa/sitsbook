--- 
title: '**sits**: Satellite Image Time Series Analysis 
    on Earth Observation Data Cubes'
author:
- Gilberto Camara
- Rolf Simoes
- Felipe Souza
- Charlotte Pelletier
- Alber Sanchez
- Pedro Ribeiro Andrade
- Karine Ferreira
- Gilberto Queiroz
date: "`r Sys.Date()`"
output:
  html_document: 
    df_print: tibble
    theme:
        base_font:
          google: "IBM Plex Serif"
        code_font:
          google: "IBM Plex Mono"
documentclass: report
link-citations: yes
colorlinks: yes
lot: yes
lof: yes
always_allow_html: true
fontsize: 10,5pt
site: bookdown::bookdown_site
cover-image: images/cover_sits_book.png
bibliography: e-sensing.bib
biblio-style: apalike
csl: ieee.csl
indent: true
description: |
  This book presents  **sits**, an open-source R package for satellite image time series analysis. The package supports the application of machine learning techniques for classifying image time series obtained from Earth observation data cubes.
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(
    class.output = 'sourceCode'
)
```

```{r, echo = FALSE}
source("common.R")
```

```{r, echo = FALSE}
library(sits)
library(sitsdata)
library(tibble)
```


# Preface {-}

<a href="https://github.com/e-sensing/sitsbook"><img class="cover" src="images/cover_sits_book.png" width="326" align="right" alt="Cover image" /></a>

Satellite images provide key information on the Earth’s environment and the impacts caused by human actions. Petabytes of Earth observation data are now open and free, making the full extent of image archives available for researchers and experts.  Remote sensing experts can now track environmental change using satellite image time series.  Using image time series, analysts make best use of the full extent of big Earth observation data collections, capturing subtle changes in ecosystem health and condition and improving the distinction between different land classes.

This book introduces `sits`, an open-source **R** package for land use and land cover classification of big Earth observation data using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Brazil Data Cube, and Digital Earth Africa. The `sits` API includes an assessment of training sample quality, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, `sits` supports best practice accuracy assessments.

## Who this book is for {-}

The target audience for `sits` is the community of remote sensing experts with Earth Sciences background who want to use state-of-the-art data analysis methods with minimal investment in programming skills. The package provides a clear and direct set of functions, which are easy to learn and master. Users with a minimal background on **R** programming can start using `sits` right away. Those not yet familiar with **R** need only to learn introductory concepts.  

If you are not an **R** user and would like to quickly master what is needed to run `sits`, please read Parts 1 and 2 of Garrett Golemund's book, "Hands-On Programming with R"(https://rstudio-education.github.io/hopr/>). If you already are an **R** user and would like to update your skills with the latest trends,  please read the book by Hadley Wickham and Gareth Golemund, "R for Data Science". (<https://r4ds.had.co.nz/>). Important concepts of spatial analysis are presented by Edzer Pebesma and Roger Bivand in their book "Spatial Data Science" (<https://r-spatial.org/book/>).

## Software version described in this book{-}

The version of the `sits` package described in this book is version 1.3.0.

## Main reference for sits {-}

If you use sits in your work, please cite the following paper: 

Rolf Simoes, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R. Andrade,  Lorena Santos, Alexandre Carvalho, and Karine Ferreira. “Satellite Image Time Series Analysis for Big Earth Observation Data”. Remote Sensing, 13, p. 2428, 2021. <https://doi.org/10.3390/rs13132428>. 

## Intellectual property rights {-}

This book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons, described in the terms available at <https://creativecommons.org/licenses/by-nc-sa/4.0/>. The `sits` package is licensed under the GNU General Public License, version 3.0. 

<!--chapter:end:index.Rmd-->

# Setup {.unnumbered}

The `sits` package relies on the `sf` and `terra` **R** packages, which in turn require the GDAL and PROJ libraries. Please follow the instructions below for installing `sf` and `terra` together with GDAL, provided by Edzer Pebesma.

## Support for GDAL and PROJ {.unnumbered}

### Windows and MacOS {.unnumbered}

Windows and MacOS users are strongly encouraged to install the `sf` and `terra` binary packages from CRAN. To install `sits` from source, please install package `Rtools` to have access to the compiling environment.

### Ubuntu {.unnumbered}

We recommend using the latest version of the GDAL, GEOS, and PROJ4 libraries and binaries. To do so, use the repository `ubuntugis-unstable`, which should be done as follows:

``` sh
sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
sudo apt-get update
sudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev 
sudo apt-get install gdal-bin
sudo apt-get install proj-bin
```

If you get an error while adding this PPA repository, it could be because you miss the package `software-properties-common`. When GDAL is running in `docker` containers, please add the security flag `--security-opt seccomp=unconfined` on start. 

After installing GDAL, GEOS, and PROJ4, please install packages "sf" and "terra", in this order.

```{r, eval = FALSE}
install.packages("sf")
install.packages("terra")
```


### Debian{-}

To install on Debian, use the [rocker geospatial](https://github.com/rocker-org/geospatial) dockerfiles. 

### Fedora {.unnumbered}

The following command installs all required dependencies:

``` sh
sudo dnf install gdal-devel proj-devel geos-devel sqlite-devel udunits2-devel
```

## Support for deep learning with torch {.unnumbered}

The deep learning models of `sits` use the `torch` package, which is an **R** version of `pyTorch`. Before installing "sits", please also install packages "torch" and "luz", and initialize torch. 

```{r, eval = FALSE}
install.packages("torch")
install.packages("luz")
torch::install_torch()
```


## Installing the `sits` package {.unnumbered}

After installing `sf`, `terra`, `torch` and `luz`, please proceed to install `sits`,  which is available on CRAN and should be installed as a normal **R** package.

```{r, eval = FALSE}
install.packages("sits", dependencies = TRUE)
```

The source code repository is on GitHub <https://github.com/e-sensing/sits>. To install the development version of `sits`, which contains the latest updates but might be unstable, users should install `devtools` if not already available, as then install sits as follows:

```{r, eval = FALSE}
install.packages("devtools")
devtools::install_github("e-sensing/sits@dev", dependencies = TRUE)
```

To run the examples in the book, please also install the "sitsdata" package.

```{r, eval = FALSE}
options(download.file.method = "wget")
devtools::install_github("e-sensing/sitsdata")
```


## Using GPUs with `sits` 

The `torch` package automatically recognizes if a GPU is available on the machine and uses the GPU for training and classification. There is a significant performance gain when GPUs are used instead of CPUs for deep learning models. There is no need for specific adjustments to `torch` scripts.  To use GPUs with `torch` requires version 11.6 of the CUDA library, which is available for Ubuntu 20.04 and Ubuntu 18.04. 
 

<!--chapter:end:01-setup.Rmd-->

# Acknowledgements {-}

## Funding Sources {-}

The authors acknowledge the funders that supported the development of `sits`:

1.  Amazon Fund, established by the Brazil with financial contribution from Norway, through contract 17.2.0536.1. between the Brazilian Development Bank (BNDES) and the Foundation for Science, Technology and Space Applications (FUNCATE), for the establishment of the Brazil Data Cube,   

2. Coordenação de Aperfeiçoamento de Pessoal de Nível Superior-Brasil (CAPES) and from the Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for grants 312151/2014-4 and 140684/2016-6. 

3. Sao Paulo Research Foundation (FAPESP) under eScience Program grant 2014/08398-6, for for providing MSc, PhD and post-doc scholarships, equipment, and travel support.  

4. International Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (IKI) under grant 17-III-084- Global-A-RESTORE+ (“RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil”). 

5. Microsoft Planetary Computer initiative under the GEO-Microsoft Cloud Computer Grants Programme. 

6. Instituto Clima e Sociedade, under the project grant "Modernization of PRODES and DETER Amazon monitoring systems". 

7.  Open-Earth-Monitor Cyberinfrastructure project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No. 101059548.


## Community Contributions {-}

The authors thank the R-spatial community for their foundational work, including Marius Appel, Tim Appelhans, Robert Hijmans, Edzer Pebesma, 	Martijn Tennekes for their R packages `gdalcubes`,  `leafem`, `terra`, `sf`/`stars`, and `tmap`. We are greateful for the work of Dirk Eddelbuettel on `Rcpp` and `RcppArmadillo` and Ron Wehrens in package `kohonen`. We are much indebted to Hadley Wickham for the `tidyverse`, to Daniel Falbel for the `torch` and `luz` packages, and the RStudio team for package `leaflet`. The multiple authors of machine learning packages `randomForest`, `e1071` and `xgboost` provided robust algorithms. We would like to thank Python developers that shared their deep learning algorithms for image time series classification: Vivien Sainte Fare Garnot, Zhiguang Wang, Maja Schneider, and Marc Rußwurm. The first author also thanks Roger Bivand for his benign influence in all things related to **R**.  

## Reproducible papers used in building sits {-}

We thank the authors of these papers for making their code available. 

- Edzer Pebesma, "Simple Features for R: Standardized Support for Spatial Vector Data". R Journal, 10(1):2018.

- Ron Wehrens and Johannes Kruisselbrink, "Flexible Self-Organising Maps in kohonen 3.0". Journal of Statistical Software, 87, 7 (2018). <https://doi.org/10.18637/jss.v087.i07>.

- Hassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,  and Pierre-Alain Muller, "Deep learning for time series classification: a review". Data Mining and Knowledge Discovery, 33(4): 917--963, 2019. <https://doi.org/10.1007/s10618-019-00619-1>.

- Charlotte Pelletier, Geoffrey Webb, and Francois Petitjean. “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series”. Remote Sensing 11 (5), 2019. <https://doi.org/10.3390/rs11050523>.

- Marc Rußwurm, Charlotte Pelletier, Maximilian Zollner, Sèbastien Lefèvre, and Marco Körner, "Breizhcrops: a Time Series Dataset for Crop Type Mapping". International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS, 2020. 

- Marius Appel and Edzer Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library.” Data 4 (3): 1–16, 2020. <https://doi.org/10.3390/data4030092>

- Vivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata,  "Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention", Conference on Computer Vision and Pattern Recognition, 2020. <https://doi.org/10.1109/CVPR42600.2020.01234>.

- Vivien Garnot, Loic Landrieu, "Lightweight Temporal Self-Attention  for Classifying Satellite Images Time Series", 2020. <arXiv:2007.00586>.

- Maja Schneider, Marco Körner, "[Re] Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention." ReScience C 7 (2), 2021. <doi:10.5281/zenodo.4835356>.


## Publications using sits {-}

This section gathers the publications that have used **sits** to generate their results.

**2023**

- Bruno Adorno, Thales Körting, Silvana Amaral, "Contribution of time-series data cubes to classify urban vegetation types by remote sensing", Urban Forest & Urban Greening, vol. 79, 127817, January 2023. https://doi.org/10.1016/j.ufug.2022.127817

**2021**

- Lorena Santos, Karine R. Ferreira, Gilberto Camara, Michelle Picoli, Rolf Simoes, “Quality control and class noise reduction of satellite image time series”. ISPRS Journal of Photogrammetry and Remote Sensing, vol. 177, pp 75-88, 2021. https://doi.org/10.1016/j.isprsjprs.2021.04.014.

- Lorena Santos, Karine Ferreira, Michelle Picoli, Gilberto Camara, Raul Zurita-Milla and Ellen-Wien Augustijn, “Identifying Spatiotemporal Patterns in Land Use and Cover Samples from Satellite Image Time Series”. Remote Sensing, 2021, 13(5), 974; https://doi.org/10.3390/rs13050974. 


**2020**

- Rolf Simoes, Michelle Picoli, Gilberto Camara, Adeline Maciel, Lorena Santos, Pedro Andrade, Alber Sánchez, Karine Ferreira & Alexandre Carvalho. “Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017”. Nature Scientific Data 7, article 34 (2020). DOI: 10.1038/s41597-020-0371-4.

- Michelle Picoli, Ana Rorato, Pedro Leitão, Gilberto Camara, Adeline Maciel, Patrick Hostert, Ieda Sanches, “Impacts of Public and Private Sector Policies on Soybean and Pasture Expansion in Mato Grosso—Brazil from 2001 to 2017”. Land, 9(1), 2020. DOI: 10.3390/land9010020. 

- Karine Ferreira, Gilberto Queiroz et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products”. Remote Sensing, 12, 4033, 2020.

- Adeline Maciel, Lubia Vinhas, Michelle Picoli and Gilberto Camara, “Identifying Land Use Change Trajectories in Brazil’s Agricultural Frontier”. Land, 9, 506, 2020. DOI: 10.3390/land9120506. DOI: 10.3390/rs12244033.

**2018**

- Michelle Picoli, Gilberto Camara, et al.,  “Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture”. ISPRS Journal of Photogrammetry and Remote Sensing, 2018.

<!--chapter:end:02-acknowledgements.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

# Introduction to SITS{-}

```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/chp3")
```

<a href="https://www.kaggle.com/esensing/introduction-to-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

## Why work with satellite image time series?{-}

Satellite images are the most comprehensive source of data about our environment.  Covering a large area of the Earth's surface, images allow researchers to study regional and global changes. Sensors capture data in multiple spectral bands to measure the physical, chemical, and biological properties of the Earth's surface. Covering the same location multiple times, satellites provide data on changes in the environment and survey areas that are difficult to observe from the ground. Given its unique features, images provide essential information for many applications, including deforestation, crop production, food security, urban footprints, water scarcity, and land degradation.

A time series is a set of data points collected at regular intervals over time. Time series data is used to analyze trends, patterns, and changes. Satellite image time series refer to time series obtained from a collection of images captured by a satellite over a period of time, typically months or years. Using time series, experts improve their understanding of ecological patterns and processes. Instead of selecting individual images from specific dates and comparing them, researchers track change continuously [@Woodcock2020]. 

## Time-first, space-later{-}

"Time-first, space-later" is a concept in satellite image classification that takes time series analysis as the first step for analyzing remote sensing data, with spatial information being considered after all time series are classified. The *time-first* approach brings a better understanding of changes in landscapes. Detecting and tracking trends in the data, including seasonal and long-term trends becomes feasible, as well as identifying anomalous events or patterns in the data, such as wildfires, floods, or droughts.

The *space-later* part requires spatial data analysis that use the neighborhood of each pixel. The classification assigned to each pixel by time series classification is taken as a prior probability information. Using additional data from the pixel's neighborhood, users obtain the posterior probability distribution per pixel. This Bayesian inference method allows combining the spatial with the temporal information for each pixel.  

## How `sits` works {.unnumbered}

The `sits` package uses satellite image time series to for land classification, using  a *time-first, space-later* approach. In the data preparation part, collections of big Earth observation images are organized as data cubes. Each spatial location of a data cube is associated to a time series. Locations with known labels train a machine learning classifier, which classifies all time series of a data cube, as shown in Figure 1.

```{r, echo = FALSE, out.width = "70%", out.height = "70%", fig.align="center", fig.cap="Using time series for land classification (source: authors)"}
knitr::include_graphics("images/sits_general_view.png")
```

The package provides tools for analysis, visualization and classification of satellite image time series. Users follow a typical workflow:

1.  Select an analysis-ready data image collection on cloud providers such as AWS, Microsoft Planetary Computer, Digital Earth Africa and Brazil Data Cube.
2.  Build a regular data cube using the chosen image collection.
3.  Obtain new bands and indices with operations on data cubes.
4.  Extract time series samples from the data cube to be used as training data.
5.  Perform quality control and filtering on the time series samples.
6.  Train a machine learning model using the extracted samples.
7.  Use the model to classify the data cube and get class probabilities for each pixel.
8.  Post-process the probability cube to remove outliers.
9.  Produce a labeled map from the post-processed probability cube.
10. Evaluate the accuracy of the classification using best practices.

Each step of workflow corresponds to a function of the `sits` API, as shown in the table and figure below. These functions have convenient default parameters and behavior. A single function builds machine learning (ML) models. The classification function processes big data cubes with efficient parallel processing. Since the `sits` API is simple to learn, users can achieve good results without in-depth knowledge about machine learning and parallel processing.

```{r, echo = FALSE}
library(kableExtra)

sits_api <- data.frame(
    API_function = c("sits_cube()", 
                     "sits_regularize()",
                     "sits_apply()",
                     "sits_get_data()",
                     "sits_train()", 
                     "sits_classify()", 
                     "sits_smooth()",
                     "sits_label_classification()", 
                     "sits_accuracy()"),
    Inputs = c("ARD image collection", 
               "Irregular data cube", 
               "Regular data cube", 
               "Data cube and sample locations",
               "Time series and ML method", 
               "ML classification model and regular data cube",
               "Probability cube", 
               "Post-processed probability cube",
               "Classified map and validation samples"),
    Output = c("Irregular data cube", 
               "Regular data cube",
               "Regular data cube with new bands and indices",
               "Time series",
               "ML classification model", 
               "Probability cube", 
               "Post-processed probability cube",
               "Classified map",
               "Accuracy assessment")
)
kableExtra::kbl(sits_api, 
                caption = "The sits API workflow for land classification",
                booktabs = TRUE) %>%
    kableExtra::kable_styling(position = "center", 
                              font_size = 14,
                              latex_options = c("scale_down", "HOLD_position")) %>% 
    kableExtra::column_spec(column = 1, monospace = TRUE, color =  "RawSienna")
```


```{r, echo = FALSE, out.width = "100%", out.height = "100%", fig.align="center", fig.cap="Main functions of the SITS API (source: authors)."}

knitr::include_graphics("images/sits_api.png") 
```

## Creating a Data Cube {.unnumbered}

There are two kinds of data cubes in `sits`: (a) irregular data cubes generated by selecting image collections image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. For further details, please refer to Chapter "Earth Observation Data Cubes".

The first steps in using `sits` are: (a) select an analysis-ready data image collection available in a cloud provider or stored locally using `sits_cube()`; (b) if the collection is not regular, use `sits_regularize()` to build a regular data cube.

This section shows how to build a data cube from local images already organized as a regular data cube. The data cube is composed of of MODIS MOD13Q1 images for the Sinop region in Mato Grosso, Brazil. All images have indexes "NDVI" and "EVI" covering a one-year period from 2013-09-14 to 2014-08-29 (we use "year-month-day" for dates). There are 23 time instances, each covering a 16-day period. The data is available in the R package `sitsdata`.

To build a data cube from local files, users need to provide information about the original source from with the data was obtained. In this case, `sits_cube()` needs the parameters:

(a) `source`, the cloud provider from where the data has been obtained (in this case the Brazil Data Cube "BDC");
(b) `collection`, the collection from where the images have been extracted. In this case, data comes from the MOD13Q1 collection 6; 
(c) `data_dir`, the local directory where the image files are stored; 
(d) `parse_info`, stating how file names store information on tile, band and date. In this case, local images stored as `TERRA_MODIS_012010_EVI_2014-07-28.tif`. 

```{r, out.width = "100%", tidy="styler", fig.align = 'center', fig.cap="Color composite image MODIS cube for 2013-09-14 (red = EVI, green = NDVI, blue = EVI)."}
# create a data cube object based on the information about the files
sinop <- sits_cube(
  source = "BDC", 
  collection  = "MOD13Q1-6",
  data_dir = system.file("extdata/sinop", package = "sitsdata"),  
  parse_info = c("X1", "X2", "tile", "band", "date")
)
# plot a the NDVI  for the first date (2013-09-14)
plot(sinop, 
     band = "NDVI", 
     dates = "2013-09-14",
     palette = "RdYlGn"
)
```


The R object returned by `sits_cube()` contains the metadata that describes the the contents of the data cube. The metadata includes data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube.

```{r}
# show the R object that describes the data cube
sinop
```

## The time series table {-}

To handle time series information, `sits` uses a tabular data structure. The example below shows a table with 1,218 time series obtained from MODIS MOD13Q1 images. Each series has four attributes: two bands ("NIR" and "MIR") and two indexes ("NDVI" and "EVI"). This data set is available in package `sitsdata`.

```{r}
# load the MODIS samples for Mato Grosso from the "sitsdata" package
library(tibble)
library(sitsdata)
data("samples_matogrosso_mod13q1", package = "sitsdata")
samples_matogrosso_mod13q1[1:2,]
```

The data structure associated to the time series is a table that contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The `time_series` column contains the time series data for each spatiotemporal location. This data is also organized as a table, with a column with the dates and the other columns with the values for each spectral band. For more details on how to handle time series data, please see the "Working with Time Series" chapter.

It is useful to plot the dispersion of the time series. In what follows, for brevity we will select only one label ("Forest") and one index ("NDVI"). The resulting plot shows all of the time series associated to the label and attribute, highlighting the median and the first and third quartiles.

```{r, out.width = "80%", tidy="styler", fig.align = 'center', fig.cap="Joint plot of all samples in band NDVI for class Forest.", strip.white = FALSE}
samples_forest <- dplyr::filter(
    samples_matogrosso_mod13q1, 
    label == "Forest"
)
samples_forest_ndvi <- sits_select(
    samples_forest, 
    band = "NDVI"
)
plot(samples_forest_ndvi)
```

## Training a machine learning model {.unnumbered}

The next step is to train a machine learning (ML) model using `sits_train()`. It takes two inputs, `samples` (a time series table) and `ml_method` (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, `sits` provides default parameters which produce a good result. For more details, please see the "Machine Learning for Data Cubes" chapter. 

Since the time series data has four attributes ("EVI", "NDVI", "NIR", "MIR") and the data cube images only two, we select the "NDVI" and "EVI" values and use the resulting data for training. To build the classification model, we use a random forest model called by the `sits_rfor()` function.

```{r, out.width = "80%", tidy="styler", fig.align="center", fig.cap="Most relevant variables of trained random forests model."}
# select the bands "ndvi", "evi"
samples_2bands <- sits_select(
    data = samples_matogrosso_mod13q1, 
    bands = c("NDVI", "EVI"))

# train a random forest model
rf_model <- sits_train(
    samples = samples_2bands, 
    ml_method = sits_rfor()
)
# plot the most important variables of the model
plot(rf_model)
```

## Data cube classification {.unnumbered}

After training the machine learning model, the next step is to classify the data cube using `sits_classify()`. This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the the probability that it belongs to the class. This function has two mandatory parameters: `data`, the data cube or time series tibble to be classified; and `ml_model`, the trained ML model. Optional parameters include: (a) `multicores`, number of cores to be used; (b) `memsize`, RAM used in the classification; (c) `output_dir`, the directory where the classified files will be written. Details of the classification process are available in "Classification of Images in Data Cubes".

```{r, out.width = "90%", tidy="styler", fig.align = 'center', fig.cap = "Probability map for class Forest."}

# classify the raster image
sinop_probs <- sits_classify(
    data = sinop, 
    ml_model = rf_model,
    multicores = 2,
    memsize = 8,
    output_dir = "./tempdir/chp3"
)
# plot the probability cube for class Forest
plot(sinop_probs, labels = c("Forest"), palette = "BuGn")
```

After classification has been completed, we plot the probability maps for class "Forest". Probability maps are useful to visualize the degree of confidence that the classifier assigns to the labels for each pixel and can be used to produce uncertainty information and support active learning, as described in Chapter "Data Cube Classification".

## Spatial smoothing {.unnumbered}

When working with big EO data, there is much variability in each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, `sits_smooth()` takes a probability cube as input and  uses the class probabilities of each pixel's neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for class "Forest" shows that most outliers have been removed.

```{r, out.width = "90%",  tidy="styler", fig.align = 'center', fig.cap = "Smoothed probability map for class Forest."}
# perform spatial smoothing
sinop_bayes <- sits_smooth(
    cube = sinop_probs,
    multicores = 2,
    memsize = 8,
    output_dir = "./tempdir/chp3"
)
plot(sinop_bayes, labels = c("Forest"), palette = "BuGn")
```

## Labelling a probability data cube {.unnumbered}

After removing outliers using local smoothing, one can obtain the labeled classification map using the function `sits_label_classification()`. This function assigns each pixel to the class with highest probability.
\newpage

```{r, out.width = "100%", out.height = "75%", tidy="styler", fig.cap = "Classification map for Sinop", fig.align="center"}

# label the probability file 
sinop_map <- sits_label_classification(
    cube = sinop_bayes, 
    output_dir = "./tempdir/chp3"
)
plot(sinop_map, title = "Sinop Classification Map")
```

The resulting classification files can be read by QGIS. Links to the associated files are available in the `sinop_map` object in the nested table `file_info`.

```{r}
# show the location of the classification file
sinop_map$file_info[[1]]
```

As shown in this "Introduction", `sits` provides an end-to-end API to land use and land cover classification. In what follows, each chapter provides a detailed description of the training, modelling and classification workflow. 



<!--chapter:end:03-intro.Rmd-->

# Earth observation data cubes{-}

```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/chp4")
library(sits)
```

## Analysis-ready data image collections{-}

Analysis-ready data (ARD) are images that are ready for analysis without the need for further preprocessing or transformation. They are designed to simplify and accelerate the analysis of Earth observation (EO) data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD data is typically provided as a collection of single files, with each pixel of the file containing a single value for each spectral band for a given date.

ARD collections are available in cloud services such as Amazon Web Service, Brazil Data Cube, Digital Earth Africa, Swiss Data Cube, and Microsoft's Planetary Computer. These collections have been processed to improve multidate comparability.  Radiance measures at the top of the atmosphere are converted to ground reflectance measures.  In general, timelines of the images of an ARD image collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure 9 shows an example of the Landsat ARD image collection. 

```{r ardtile, echo = FALSE, out.width="80%", fig.align="center", fig.cap="ARD image collection (source: USGS). Reproduction based on fair use doctrine."}
knitr::include_graphics("images/usgs_ard_tile.png")
```

ARD image collections are organized in spatial partitions. Sentinel-2/2A images follow the MGRS tiling system, which divides the world in 60 UTM zones of 8 degrees of longitude each. Each zone has blocks of 6 degrees of latitude. Blocks are split into tiles of 110 x 110 km2 with a 10 km overlap. Figure 10 shows the MGRS tiling system for a part of the Northeastern coast of Brazil, contained in UTM zone 24, block M. 

```{r wrs, echo = FALSE, out.width="80%", fig.align="center", fig.cap="MGRS tiling system used by Sentinel-2 images (source: GISSurfer 2.0). Reproduction based on fair use doctrine."}
knitr::include_graphics("images/s2_mgrs_grid.png")
```

The Landsat 4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which breaks the coverage of Landsat satellites into images identified by path and row.  The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit and each path is divided in 119 rows,where each row refers to ta latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection. 

```{r mgrs, echo = FALSE, out.width="80%", fig.align="center", fig.cap="WRS-2 tiling system used by Landsat-5/7/8/9 images (source: INPE and ESRI). Reproduction based on fair use doctrine."}
knitr::include_graphics("images/landsat_wrs_grid.png")
```

## ARD image collections handled by sits{-}

As of version 1.2.0, `sits` supports access to the following ARD image collections:

1. Amazon Web Services (AWS): Open data Sentinel-2/2A level 2A collections for the Earth's land surface. 
2. Brazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MODIS images for Brazil. These collections organized as regular data cubes. 
3. Digital Earth Africa (DEA): Open data collections of Sentinel-2/2A and Landsat-8 for Africa.
4. Microsoft Planetary Computer (MPC): Open data collections of Sentinel-2/2A and Landsat-4/5/7/8/9 for the Earth's land areas. 
5. USGS: Landsat-4/5/7/8/9 collections available in AWS, which require payment to access. 
6. Swiss Data Cube (SDC): Open data collection of Sentinel-2/2A and Landsat-8 images for Switzerland. 

## Regular image data cubes{-}

Machine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use of ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes. Following Appel and Pebesma [@Appel2019], a *regular data cube*  has the following definition and properties:

1. A regular data cube is a four-dimensional structure with dimensions x (longitude or easting), y (latitude or northing), time, and bands.
2. Its spatial dimensions refer to a single spatial reference system (SRS). Cells of a data cube have a constant spatial size with respect to the cube’s SRS.
3. The temporal dimension is composed of a set of continuous and equally-spaced intervals. 
4. For every combination of dimensions, a cell has a single value.

All cells of a data cube have the same spatiotemporal extent. The spatial resolution of each cell is the same in X and Y dimensions. All temporal intervals are the same. Each cell contains a valid set of measures. For each position in space, the data cube should provide a set of valid time series. For each time interval, the regular data cube should provide a valid 2D image (see Figure 11). 

```{r dcconcept, echo = FALSE, out.width="100%", fig.align="center", fig.cap="Conceptual view of data cubes (source: authors)"}
knitr::include_graphics("images/datacube_conception.png")
```

Currently, the only cloud service that provides regular data cubes by default is the Brazil Data Cube (BDC). Analysis-ready data (ARD) collections available in AWS, MSPC, USGS and DE Africa are not regular in space and time. Bands may have different resolutions, images may not cover the entire time, and time intervals are not regular. For this reason, subsets of these collections need to be converted to regular data cubes before further processing. To produce data cubes for machine-learning data analysis, users should first create an irregular data cube from an ARD collection and then use `sits_regularize()`, as described below.

## Creating data cubes{-}

<a href="https://www.kaggle.com/esensing/creating-data-cubes-in-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

To obtain information on ARD image collection from cloud providers, `sits` uses the [STAC (SpatioTemporal Asset Catalogue) protocol](https://stacspec.org/en), a specification of geospatial information which has been adopted by many large image collection providers. A 'spatiotemporal asset' is any file that represents information about the Earth captured in a certain space and time. To access STAC endpoints, `sits` uses the [rstac](http://github.com/brazil-data-cube/rstac) R package.

The function `sits_cube()` supports access to image collections in cloud services; it has the following parameters:

1. `source`: name of the provider. 
2. `collection`: a collection available in the provider and supported by `sits`. To find out which collections are supported by `sits`, see `sits_list_collections()`. 
3. `platform`: optional parameter specifying the platform in case of collections that include more than one satellite. 
4. `tiles`: Set of tiles of image collection reference system. Either `tiles` or `roi` should be specified. 
5. `roi`: a region of interest. Either: (a) a named vector (`lon_min`, `lon_max`, `lat_min`, `lat_max`) in WGS 84 coordinates; or (b) an `sf` object.  All images that intersect the convex hull of the `roi` are selected. 
6. `bands`: (optional) bands to be used. If missing, all bands from the collection are used.
7. `start_date`: the initial date for the temporal interval containing the time series of images.
8. `end_date`: the final date for the temporal interval containing the time series of images.

The result of `sits_cube()` is a tibble with a description the selected images required for further processing. It does not contain the actual data, but only pointers to the actual images.  The attributes of individual image files can be assessed by listing the `file_info` column of the tibble. 


## Assessing Amazon Web Services{-}

Amazon Web Services (AWS) holds two kinds of collections: *open-data* and *requester-pays*. Open data collections can be accessed without cost. Requester-pays collections require payment to an AWS account. Currently, `sits` supports collections `SENTINEL-S2-L2A` (requester-pays) and `SENTINEL-S2-L2A-COGS` (open-data). Both collections include all Sentinel-2/2A bands.  The bands in 10m resolution are `B02`, `B03`, `B04`, and `B08`. The  20m bands are `B05`, `B06`, `B07`, `B8A`, `B11`, and `B12`. Bands `B01` and `B09` are available at 60m resolution. A `CLOUD` band is also available. The example below shows how to access one tile of the open data `SENTINEL-S2-L2A-COGS` collection.  The `tiles` parameter allows selection of desired area according to the MGRS reference system. 

```{r, tidy="styler"}
# create a data cube covering an area in the Rondonia state in the Brazilian Amazon
# Sentinel-2 images over 
s2_20LKP_cube <- sits_cube(
    source = "AWS",
    collection = "SENTINEL-S2-L2A-COGS",
    tiles = "20LKP",
    bands = c("B02", "B8A", "B11", "CLOUD"),
    start_date = "2018-07-12",
    end_date = "2019-07-28"
)
```


## Assessing Microsoft's Planetary Computer{-}

Microsoft's Planetary Computer (MPC) hosts two open data collections: `SENTINEL-2-L2A` and `LANDSAT-C2-L2`. The first collection contains SENTINEL-2/2A ARD images, with the same bands and resolutions as those available in AWS (see above). The example below shows how to access the `SENTINEL-2-L2A` collection. 

```{r, tidy="styler",  out.width="100%", fig.align="center", fig.cap= "Sentinel-2 image in an area of the state of Rondonia, Brazil"}
# create a data cube covering an area in the Brazilian Amazon
s2_20LKP_cube_MPC <- sits_cube(
      source = "MPC",
      collection = "SENTINEL-2-L2A",
      tiles = "20LKP",
      bands = c("B02", "B8A", "B11", "CLOUD"),
      start_date = "2019-07-01",
      end_date = "2019-07-28"
      
)
# plot a color composite of one date of the cube
plot(s2_20LKP_cube_MPC, red = "B11", blue = "B02", green = "B8A", 
     date = "2019-07-18")
```

The `LANDSAT-C2-L2` collection provides access to data from Landsat-4, 5, 7, 8, and 9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are `BLUE`, `GREEN`, `RED`,  `NIR08`,  `SWIR16`, and `SWIR22`. All images have 30m resolution. For this collection,  tile search is not supported; the `roi` parameter should be used. The example shows how to retrieve data from a region of interest covering the city of Brasilia in Brazil. 


```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap= "Landsat-8 image in an area of the city of Brasilia, Brazil"}
# Read a shapefile thar covers the city of Brasilia
shp_file <- system.file("extdata/shapefiles/df_bsb/df_bsb.shp", 
                        package = "sitsdata")
sf_bsb <- sf::read_sf(shp_file)
# select the cube
s2_L8_cube_MPC <- sits_cube(
        source = "MPC",
        collection = "LANDSAT-C2-L2",
        bands = c("BLUE", "NIR08", "SWIR16", "CLOUD"),
        roi = sf_bsb,
        start_date = "2019-06-01",
        end_date = "2019-10-01"
)
# Plot the second tile that covers Brasilia
plot(s2_L8_cube_MPC[2,], red = "SWIR16", green = "NIR08", blue = "BLUE", 
     date = "2019-07-30")
```


## Assessing Digital Earth Africa{-}

Digital Earth Africa (DEAFRICA) is a cloud service that provides open access Earth Observation data for the African continent. The ARD image collections available in `sits` are `S2_L2A` (Sentinel-2 level 2A) and `LS8_SR` (Landsat-8). Since the STAC interface for DEAFRICA does not implement the concept of tiles, users users need to specify their area of interest using the `roi` parameter. The requested `roi` produces a cube that contains three MGRS tiles ("35HLD", "35HKD", and "35HLC") covering part of South Africa. 
```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Sentinel-2 image in an area over South Africa"}
dea_cube <- sits_cube(
    source = "DEAFRICA",
    collection = "S2_L2A",
    roi = c(lon_min = 24.97, lat_min = -34.30,
            lon_max = 25.87, lat_max = -32.63),
    bands = c("B05", "B8A", "B11"),
    start_date = "2019-09-01",
    end_date = "2019-10-01"
)
```

## Assessing the Brazil Data Cube{-}

The [Brazil Data Cube](http://brazildatacube.org/) (BDC) is built by Brazil’s National Institute for Space Research (INPE). The BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The three grids are generated taking -54$^\circ$ longitude as the central reference and defining tiles of $6\times4$, $3\times2$ and $1.5\times1$ degrees. The large grid is composed by tiles of $672\times440$ km^2^ and is used for CBERS-4 AWFI collections at 64 meter resolution; each CBERS-4 AWFI tile contains images of $10,504\times6,865$ pixels. The medium grid is used for Landsat-8 OLI collections at 30 meter resolution; tiles have an extension of $336\times220$ km^2^ and each image has $11,204\times7,324$ pixels. The small grid covers $168\times110$ km^2^ and is used for Sentinel-2 MSI collections at 10m resolutions; each image has $16,806\times10,986$ pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [@Ferreira2020a]. 

```{r, echo = FALSE, out.width="80%", fig.align="center", fig.cap="Hierarchical BDC tiling system showing overlayed on Brazilian Biomes (a), illustrating that one large tile (b) contains four medium tiles (c) and that medium tile contains four small tiles. Source: Ferreira et al.(2020). Reproduction under fair use doctrine."}

knitr::include_graphics("images/bdc_grid.png")
```

The collections available in the BDC are: `LC8_30_16D_STK-1` (Landsat-8 OLI, 30m resolution, 16-day intervals),  `S2-SEN2COR_10_16D_STK-1` (Sentinel-2 MSI images at 10 meter resolution, 16-day intervals), `CB4_64_16D_STK-1` (CBERS 4/4A AWFI, 64m resolution, 16 days intervals), `CB4_20_1M_STK-1` (CBERS 4/4A MUX, 20m resolution, one month intervals) and `MOD13Q1-6` (MODIS MOD13SQ1 product, collection 6, 250m resolution, 16-day intervals). For more details, use `sits_list_collections(source = "BDC")`.

To access the Brazil Data Cube, users need to provide their credentials using an environment variables, as shown below. Obtaining a BDC access key is free. Users need to register at the [BDC site](https://brazildatacube.dpi.inpe.br/portal/explore) to obtain the key.
```{r,eval = FALSE}
Sys.setenv(
    "BDC_ACCESS_KEY" = <your_bdc_access_key>
)
```

In the example below, the data cube is defined as one tile ("022024") of `CB4_64_16D_STK-1` collection which holds CBERS AWFI images at 16 days resolution.

```{r, tidy="styler", eval = FALSE}
# define a tile from the CBERS-4/4A AWFI collection
cbers_tile <- sits_cube(
    source = "BDC",
    collection = "CB4_64_16D_STK-1",
    tiles = "022024",
    bands = c("B13", "B14", "B15", "B16", "CLOUD"),
    start_date = "2018-09-01",
    end_date = "2019-08-28"
)
# plot one time instance
plot(cbers_tile, red = "B15", green = "B16", blue = "B13", date = "2018-09-30")
```

```{r, echo = FALSE, out.width="100%", fig.align="center", fig.cap="Plot of CBERS-4 image obtained from the BDC with a single tile covering an area in the Brazilian Cerrado."}

knitr::include_graphics("images/cbers_4_image_bdc.png")
```

## Defining a data cube using ARD local files{-}

ARD images downloaded from cloud collections to a local computer are not associated to a STAC endpoint that describes them. They need to be be organized and named to allow `sits` to create a data cube from them. All local files should be in the same directory and have the same spatial resolution and projection. Each file should contain a single image band for a single date. Each file name needs to include tile, date and band information. Users need to provide information about the original data source to allow `sits` to retrieve information about image attributes such as band names, missing values, etc. When working with local cubes,  `sits_cube()` needs the following parameters: 

1. `source`: name of the original data provider; either `BDC`, `AWS`, `USGS`, `MSPC` or `DEAFRICA`. 
2. `collection`:  collection from where the data was extracted. 
3. `data_dir`: local directory for images.
4. `bands`: optional parameter to describe the bands to be retrieved.
5. `parse_info`: information to parse the file names. File names need to contain information on tile, date and band, separated by a delimiter (usually "_").
6. `delim`: separator character between descriptors in the file name (default is "_").

The example shows how to define a data cube using files from the `sitsdata` package.  This is a data set containing part of tile "20LKP" of Sentinel-2 images for the period 2020-06-04 to 2021-08-26, with bands "B02", "B8A" and "B11". Data is extracted from collection "SENTINEL-2-L2A" on Microsoft Planetary Computer ("MPC")  Given the file name `cube_20LKP_B02_2020-06-04.tif`, to retrieve information about the images, one needs to set the `parse_info` parameter to `c("X1", "tile", "band", "date")`. 

```{r}
library(sits)
# Create a cube based on a stack of CBERS data
data_dir <- system.file("extdata/Rondonia-20LKP", package = "sitsdata")
# list the first file
list.files(data_dir)[1]
```

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="CBERS-4 NDVI in an area over Brazil"}
# create a data cube from local files
s2_cube_20LKP <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c("X1", "tile", "band", "date")
)
# plot the band B8A in the first time instance
plot(s2_cube_20LKP, red = "B11", green = "B8A", blue = "B02", dates = "2021-07-25")
```

## Defining a data cube using classified images{-}

It is also possible to create local cubes based on results that have been produced by classification or post-classification algorithms. In this case, more parameters are required and the parameter `parse_info` is specified differently, as follows:

1. `source`:  name of the original data provider. 
2. `collection`:  name of the collection from where the data was extracted. 
3. `data_dir`: local directory for the classified images.
4. `band`: Band name is associated to the type of result. Use: (a) `probs` for probability cubes produced by `sits_classify()`; (b) `bayes`, for cubes produced by `sits_smooth()`; (c) `entropy`, `least`, `ratio` or `margin`, according to the method selected when using `sits_uncertainty()`; and (d) `class` for labelled cubes.
5. `labels`: Labels associated to the classification results (not required for cubes produced by `sits_uncertainty()`).
6. `version`: Version of the result (default = `v1`).
7. `parse_info`: File name parsing information to allow `sits` to deduce the values of `tile`, `start_date`, `end_date`, `band` and `version` from the file name. Unlike non-classified image files, cubes produced by classification and post-classification have both `start_date` and `end_date`. 

The following code creates a results cube based on the classification of deforestation in Brazil.  This classified cube was obtained by a large data cube of Sentinel-2 images, covering the state of Rondonia, Brazil comprising 40 tiles, 10 spectral bands, and covering the period from 2020-06-01 to 2021-09-11. Samples of four classes were trained by a random forest classifier. 

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Classified data cube for year 2020/2021 in Rondonia, Brazil"}
# Create a cube based on a classified image 
data_dir <- system.file("extdata/Rondonia-20LLP", package = "sitsdata")
# file is named "SENTINEL-2_MSI_20LLP_2020-06-04_2021-08-26_class_v1.tif" 
Rondonia_class_cube <- sits_cube(
    source = "AWS",
    collection = "SENTINEL-S2-L2A-COGS",
    bands = "class",
    labels = c("Burned_Area", "Cleared_Area", 
               "Highly_Degraded", "Forest"),
    data_dir = data_dir,
    parse_info = c("X1", "X2", "tile", "start_date", "end_date", 
                   "band", "version")
)
# plot the classified cube
plot(Rondonia_class_cube)
```

## Regularizing data cubes{-}

Analysis-ready data (ARD) collections available in AWS, MSPC, USGS and DEAFRICA are not regular in space and time. Bands may have different resolutions, images may not cover the entire tile, and time intervals are not regular. For this reason, data from these collection need to be converted to regular data cubes to run machine learning methods. This is done in by the function `sits_regularize()`, which uses the  *gdalcubes* package [@Appel2019]. 

In the following example, the user has created an irregular data cube from the Sentinel-2 collection available in Microsoft's Planetary Computer (MSPC) for tiles `20LKP` and `20LLP` in the state of Rondonia, Brazil. We first build an irregular data cube using `sits_cube()`.

```{r, tidy="styler"}
# creating an irregular data cube from MSPC
s2_cube <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    tiles = c("20LKP", "20LLP"),
    bands = c("B05", "B8A", "B12", "CLOUD"),
    start_date = as.Date("2018-07-01"),
    end_date = as.Date("2018-08-31")
)
# show the different timelines of the cube tiles
sits_timeline(s2_cube)
```


```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Sentinel-2 tile 20LLP for date 2018-07-03"}
# plot the first image of the irregular cube
s2_cube %>% 
    dplyr::filter(tile == "20LLP") %>% 
    plot(red = "B12", green = "B8A", blue = "B05", date = "2018-07-03")
```

Because of different acquisition orbits of the Sentinel-2 and Sentinel-2A satellites, the two tiles also have different timelines. Tile `20LKP` has 12 instances and tile `20LLP` has 24 instances for the chosen period. The function  `sits_regularize()` builds a data cube with a regular timeline and a best estimate of a valid pixel for each interval. The `period` parameter sets the time interval between two images. Values of `period` use the ISO8601 time period specification, which defines time intervals as `P[n]Y[n]M[n]D`, where Y stands for years, "M" for months and "D" for days. Thus, `P1M` stands for a one-month period, `P15D` for a fifteen-day period. When joining different images to get the best image for a period, `sits_regularize()` uses an aggregation method which organizes the images for the chosen interval in order of increasing cloud cover, and selects the first cloud-free pixel in the sequence. 

```{r, tidy="styler", message=FALSE, results='hide', out.width="100%", fig.align="center", fig.cap="Regularized image for tile Sentinel-2 tile 20LLP"}
# regularize the cube to 15 day intervals
reg_cube <- sits_regularize(
          cube       = s2_cube,
          output_dir = "./tempdir/chp4",
          res        = 120,
          period     = "P15D",
          multicores = 4
)
# plot the first image of the tile 20LLP of the regularized cube
# The pixels of the regular data cube cover the full MGRS tile
reg_cube %>% 
    dplyr::filter(tile == "20LLP") %>% 
    plot(red = "B12", green = "B8A", blue = "B05")
```

After obtaining a regular data cube, users can perform data analysis and classification operations, as shown in the next chapters.

<!--chapter:end:04-datacubes.Rmd-->

# Operations on Data Cubes{-}

```{r, echo = FALSE}
source("common.R")
library(sits)
```

## Pixel-based and neighborhood-based operations{-}

Pixel-based operations in remote sensing images refer to image processing techniques that operate on individual pixels or cells in an image, without taking into account their spatial relationships with neighboring pixels. These operations are typically applied to each pixel in the image independently and can be used to extract information on spectral, radiometric, or spatial properties of the image. Pixel-based operations produce spectral indexes which combine information from multiple bands.

Neighborhood-based operations are operations that are applied to a group of pixels in an image. The neighborhood is typically defined as a rectangular or circular region centered on a given pixel. These operations can be used for removing noise from images, detecting edges, and sharpening, among other uses.

Using the  `sits_apply()` function, users specify the desired mathematical operation as function of the bands available on the cube using any valid R expression to compute the new indices. Then, `sits_apply()` computes the operation for all tiles and all temporal intervals. There are two types of operations in `sits_apply()`: 

1. Pixel-based operations that produce an index based on individual pixels of existing bands. The input bands and indexes should be part of the input data cube and should have the same names as used in the cube. The new index will be computed for every pixel of all images in the time series. Besides arithmetic operators, the function also accepts vectorized R functions that can be applied to matrices (e.g., `sqrt()`, `log()`, and `sin()`.

2. Neighborhood-based operations that produce a derived value based on a window centered around each individual pixel. The available functions  are `w_median()`, `w_sum()`, `w_mean()`, `w_min()`, `w_max()`, `w_sd()` (standard deviation) and `w_var()` (variance). Users set the size of the window (only odd values are allowed).

The following examples show how to use the `sits_apply()` function.

## Computing NDVI and its variations{-}

Using vegetation indexes is an established practice on remote sensing. These indexes aim to improve discrimination of vegetation structure by combining two wavebands, one where leaf pigments reflect incoming light with another where leaves absorb incoming radiation. Green leaves from natural vegetation such as forests have a strong emissivity rate in the near infrared bands and low emissivity rates on the red bands of the electromagnetic spectrum. These spectral properties are used to calculate the NDVI (Normalized Difference Vegetation Index), a widely used index which is  computed as the normalized difference between the values of infra-red and red bands. The inclusion of red-edge bands in Sentinel-2 images has broadened the scope of the bands used to calculate these indices[@Xie2019][@Sun2020a]. In what follows, we show examples of vegetation index calculation, using a Sentinel-2 data cube. 

First, we define a data cube for a tile in the Amazon region in the state of Rondonia, Brazil, including bands used to compute different vegetation indexes. We regularize the a cube using a target resolution of 60 meters to reduce processing time. 

```{r, tidy="styler"}
# create a directory to store files
if (!file.exists("./tempdir/chp5"))
    dir.create("./tempdir/chp5")

# creating an irregular data cube from MSPC
s2_cube <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    tiles = c("20LKP"),
    bands = c("B02", "B03", "B04", 
              "B05", "B06", "B07", 
              "B08", "B8A", "B11", 
              "B12","CLOUD"),
    start_date = as.Date("2018-07-01"),
    end_date = as.Date("2018-08-31")
)
# regularize the cube to 15 day intervals
reg_cube <- sits_regularize(
          cube       = s2_cube,
          output_dir = "./tempdir/chp5",
          res        = 60,
          period     = "P15D",
          multicores = 4
)
```

There are many options for calculating NDVI-related indexes for Sentinel-2 bands. The most widely used method combines band "B08" (785-899 nm) and band "B04" (650-680 nm). Recent works in the literature propose the use of the red-edge bands "B05" (698-713 nm), "B06" (733-748 nm) and "B07" (773-793 nm) for capturing subtle variations in chlorophyll absorption producing indexes which are called Normalized Difference Vegetation Red-edge indexes (NDRE) [@Xie2019]. In recent paper, Sun et al.[@Sun2020a] argue that a vegetation index built using bands "B06" and "B07" provide a better approximation to leaf area index estimates. In a recent review, Chaves et al.[@Chaves2020] argues that red-edge bands are important for distinguishing leaf structure and chlorophyll content of different vegetation species. In the example below, we show how to include additional indexes in the regular data cube which has the Sentinel-2 spectral bands. 

We first calculate the NDVI in the usual way, using bands B08 and B04.

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="NDVI using bands B08 and B04 of Sentinel-2"}

# calculate new indexes
reg_cube <- sits_apply(reg_cube,
    NDVI = (B08 - B04)/(B08 + B04),
    output_dir = "./tempdir/chp5",
    multicores = 4,
    memsize = 12
)
plot(reg_cube, band = "NDVI", palette = "RdYlGn")
```
We now compare the traditional NDVI with other vegetation indexes computed using red-edge bands. We first compute the NDRE1 using bands "B06" and "B05". 

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="NDRE1 using bands B06 and B05 of Sentinel-2"}

# calculate new indexes
# NDRE1 using bands B06 and B05
reg_cube <- sits_apply(reg_cube,
    NDRE1 = (B06 - B05)/(B06 + B05),
    output_dir = "./tempdir/chp5",
    multicores = 4,
    memsize = 12
)
# plot NDRE1 index
plot(reg_cube, band = "NDRE1",  palette = "RdYlGn")
```
You can notice that the contrast between forests and deforested areas is stronger in the NDRE1 index than with NDVI. We can also compare this index with other red-edge based indexes as shown below. We first calculate NDRE2 using bands "B07" and "B05". 

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="NDRE2 using bands B07 and B05 of Sentinel-2"}

# calculate new index
# NDRE2 using bands B07 and B05
reg_cube <- sits_apply(reg_cube,
    NDRE2 = (B07 - B05)/(B07 + B05),
    output_dir = "./tempdir/chp5",
    multicores = 4,
    memsize = 12
)
# plot NDRE2 index
plot(reg_cube, band = "NDRE2", palette = "RdYlGn")
```
Finally, we can calculate the third red-edge based vegetation index using bands "B06" and "B07". 

```{r, tidy="styler", out.width="90%", fig.align="center", fig.cap="NDVI using bands B08 and B04 of Sentinel-2"}

# calculate new indexes
# NDRE3 using bands B07 and B06 
reg_cube <- sits_apply(reg_cube,
    NDRE3 = (B07 - B06)/(B07 + B06),
    output_dir = "./tempdir/chp5",
    multicores = 4,
    memsize = 12
)
# plot NDRE3 index
plot(reg_cube, band = "NDRE3", palette = "RdYlGn")
```

## Spectral indexes for identification of burned areas{-}

Another relevant use of band combination is the creation of spectral indexes for detection of degradation by fires, which are an important element in environmental degradation. Forest fires have a significant impact on emissions and impoverish natural ecosystems[@Nepstad1999]. Fires open the canopy, make the microclimate drier and increase the amount of dry fuel[@Gao2020]. One well-established technique for detecting burned areas with remote sensing images is the normalized burn ratio (NBR) as the difference between the near infrared and the short wave infrared band, here calculated using the `B8A` and `B12` bands.

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="NBR ratio for a regular data cube built using Sentinel-2 tiles and 20LKP and 20LLP"}
# calculate the normalized burn ratio 
reg_cube <- sits_apply(reg_cube,
    NBR = (B12 - B8A)/(B12 + B8A),
    output_dir = "./tempdir/chp5",
    multicores = 4,
    memsize = 12
)
# plot the NBR for the first date"
plot(reg_cube, band = "NBR", palette = "Reds")
```

## Spectral mixture analysis{-}

One of most useful tools for analysis of land remote sensing images is the use of spectral mixture models[@Roberts1993, @Shimabukuro1991, @Souza2005]. The rationale for mixture models arises due to the spatial resolution associated to pixels of medium-resolution images. In general, the values for these pixels  contain a mixture of spectral responses of different types of land cover contained inside a resolution element [@Shimabukuro2019]. Assuming that the set of land cover classes (called endmembers) is known, the goal of spectral mixture analysis is to derive a set of new bands, each containing the proportion of each endmember. The most used method for spectral mixture analysis is the linear model [@Shimabukuro2019], expressed as 

$$
R_i = \sum_{j=1}^N a_{i,j}*x_j + \epsilon_i, i \in {1,...M}, M > N
$$
where $i=1,..M$ is the set of spectral bands and $j=1,..N$ is the set of land cover types. For each pixel, $R_i$ is the reflectance in the i-th spectral band, $x_j$ is the value of the reflectance due to the j-th endmember, $a_{i,j}$ is the proportion between the j-th endmember and the i-th spectral band. The model includes an error term $e_i$. The linear model can be interpreted as system of equations where the spectral response of the pixel is a linear combination of the spectral response of the endmembers[@Shimabukuro2019]. To solve this system of equations and obtain the proportion of each endmember, `sits` uses a non-negative least squares (NNLS) regression algorithm which is available in the R package `RStoolbox` and was developed by Jakob Schwalb-Willmann, based on the sequential coordinate-wise algorithm (SCA) proposed on Franc et al. [@Franc2005]. 

To run the mixture model in `sits`, it is necessary to inform the values of pixels which are covered entirely by a single class. These are the so-called "pure" pixels. These pixels should be chosen carefully and based on expert knowledge of the area. The quality of the resulting endmember images depends on the quality of the pure pixels. Since `sits` supports multiple endmember spectral mixture analysis[@Roberts1998], users can specify more than one pure pixel per endmember to account for natural variability. 

In `sits`, spectral mixture analysis is done by the `sits_mixture_model()` function, which has two mandatory parameters: `cube` (a data cube) and `endmembers`, a named table (or equivalent) defines the defines the pure pixels. The `endmembers` table should name the following named columns: (a) `type`, that defines the class associated to an endmember; (b) names, the names of the bands. Each line of the table should contain the value of each endmember for all bands (see example). To improve readability, we suggest that the `endmembers` parameters be defined as a `tribble`. A `tribble` is a `tibble` with an easier to read row-by-row layout. In the example below, we define three endmmembers for classes "forest", "soil", "water".  Note that the values for each band are expressed as integers ranging from 0 to 10,000. 

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Percentage of forest per pixel estimated by mixture model"}
# define the endmembers for three classes and six bands
em <- tibble::tribble(
    ~type,    ~B02, ~B03, ~B04, ~B8A, ~B11, ~B12,
    "forest",  200,  352,  189, 2800, 1340,  546,
    "soil",    400,  650,  700, 3600, 3500, 1800,
    "water",   700, 1100, 1400,  850,   40,   26,
)
# Generate the mixture model
reg_cube <- sits_mixture_model(
    data = reg_cube,
    endmembers = em,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp5"
)
# plot the FOREST for the first date using the "forest" palette
plot(reg_cube, band = "FOREST", palette = "Greens")
```

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Percentage of water per pixel estimated by mixture model"}

# plot the water endmember for the first date using the water palette
plot(reg_cube, band = "WATER", palette = "Blues")
```

```{r, tidy="styler", out.width="100%", fig.align="center", fig.cap="Percentage of soil per pixel estimated by mixture model"}
# plot the SOIL endmember for the first date using the "soil" palette 
plot(reg_cube, band = "SOIL", palette = "OrRd")
```

Spectral mixture analysis methods have many application on remote sensing, including  forest degradation[@Cochrane1998, @Shimabukuro2019, @Bullock2020, @Chen2021], wetland surface dynamics [@Halabisky2016] and urban area characterization [@Wu2003]. Their use in `sits` allows a unique combination of mixture models and time series analysis.

<!--chapter:end:05-cubeoperations.Rmd-->

# Working with time series{-}

```{r, echo = FALSE}
source("common.R")
```

## Data structures for satellite time series{-}

<a href="https://www.kaggle.com/esensing/working-with-time-series-in-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

The `sits` package uses sets of time series data describing properties in spatiotemporal locations of interest. For land use classification, these sets consist of samples labelled by experts. The package can also be used for any type of classification, provided that the timeline and bands of the time series used for training match that of the data cubes. 

The package uses a `tibble` data structure to organize time series data. A `tibble` is a generalization of a `data.frame`, the usual way in R to organize data in tables. Tibbles are part of the `tidyverse`, a collection of R packages designed to work together in data manipulation [@Wickham2017].  The following code shows the first three lines of a tibble containing 1,882 labeled samples of land cover in Mato Grosso state of Brazil. The samples contain time series extracted from the MODIS MOD13Q1 product from 2000 to 2016, provided every 16 days at 250-meter resolution in the Sinusoidal projection. Based on ground surveys and high-resolution imagery, it includes samples of nine classes: "Forest", "Cerrado", "Pasture", "Soy_Fallow", "Fallow_Cotton", "Soy_Cotton", "Soy_Corn", "Soy_Millet", and "Soy_Sunflower". 

```{r}
# samples
data("samples_matogrosso_mod13q1")
samples_matogrosso_mod13q1[1:4,]
```

A sits tibble contains data and metadata. The first six columns contain spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The first sample has been labeled "Pasture" at location (-58.5631, -13.8844) and is valid for the period (2006-09-14, 2007-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers involved in labeling the samples chose to use the agricultural calendar in Brazil. For other applications and other countries, the relevant dates will most likely be different from those used in the example. The `time_series` column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. 

## Utilities for handling time series{-}

The package provides functions for data manipulation and displaying information for sits tibble. For example, `sits_labels_summary()` shows the labels of the sample set and their frequencies.

```{r}
sits_labels_summary(samples_matogrosso_mod13q1)
```

In many cases, it is helpful to relabel the data set. For example, there may be situations when one wants to use a smaller set of labels, since samples in one label on the original set may not be distinguishable from samples with other labels. We then could use `sits_labels()<-` to assign new labels). The example below shows how to do relabelling on a the time series set shown above; all samples associated to crops are grouped in a single "Croplands" label.
```{r, tidy = "styler"}
# copy the sample set for Mato Grosso 
samples_new_labels <- samples_matogrosso_mod13q1
sits_labels(samples_new_labels) <- c("Cerrado",   "Croplands", 
                                     "Forest",    "Pasture",
                                     "Croplands", "Croplands",
                                     "Croplands", "Croplands",
                                     "Croplands"
)
sits_labels_summary(samples_new_labels)
```



Given that we have used the tibble data format for the metadata and the embedded time series, one can use the functions from `dplyr`, `tidyr`, and `purrr` packages of the `tidyverse` [@Wickham2017] to process the data. For example, the following code uses `sits_select()` to get a subset of the sample data set with two bands (NDVI and EVI) and then uses the `dplyr::filter()` to select the samples labelled as "Cerrado". 

```{r, tidy="styler", message = FALSE}
# select NDVI band
samples_ndvi <- sits_select(samples_matogrosso_mod13q1, 
                            bands = "NDVI")

# select only samples with Cerrado label
samples_cerrado <- dplyr::filter(samples_ndvi, 
                                 label == "Cerrado")
```

## Time series visualisation{-}

Given a small number of samples to display, `plot()` tries to group as many spatial locations together. In the following example, the first 12 samples of  "Cerrado" class refer to the same spatial location in consecutive time periods. For this reason, these samples are plotted together.

```{r cerrado-12, fig.align="center", out.width="70%", fig.cap="Plot of the first 'Cerrado' sample" }
# plot the first 12 samples
plot(samples_cerrado[1:12,])
```

For a large number of samples, the default visualization combines all samples together in a single temporal interval even if they belong to different years. This plot is useful to show the spread of values for the time series of each band. The strong red line in the plot shows the median of the values, while the two orange lines are the first and third interquartile ranges. See `?plot` for more details on data visualization in  `sits`.

```{r fig.align="center", out.width="70%", fig.cap="Plot of all Cerrado samples "}
# plot all cerrado samples together
plot(samples_cerrado)
```

## Obtaining time series data from data cubes{-}

To get a set of time series in sits, one has to first create a regular data cube and then request one or more time series from the cube  using `sits_get_data()`.  This function uses two mandatory parameters: `cube` and `samples`. The `cube` parameter indicates the data cube from which time series will be extracted.  The `samples` parameter accepts the following data types: 

1. A data.frame with information on latitude and longitude (mandatory), start_date, end_date and label for each sample point.
2. A csv file with columns latitude and longitude, start_date, end_date and label.
3. A shapefile containing either POINT or POLYGON geometries. See details below.
4. An `sf` object (from the `sf` package) with POINT or POLYGON geometry information. See details below.

In the example below, given a data cube the user provides the latitude and longitude of the desired location. Since the bands, start date and end date of the time series are not informed, `sits` obtains them from the data cube. The result is a tibble with one time series that can be visualized using `plot()`.
\newpage
```{r, tidy="styler", fig.align="center", out.width="90%", fig.cap="NDVI and EVI time series fetched from local raster cube.", message = FALSE}
# Obtain a raster cube with based on local files
data_dir <- system.file("extdata/sinop", package = "sitsdata")
raster_cube <- sits_cube(
    source     = "BDC",
    collection = "MOD13Q1-6",
    data_dir   = data_dir,
    parse_info = c("X1", "X2", "tile", "band", "date")
)
# obtain a time series from the raster cube from a point
sample_latlong <- tibble::tibble(
    longitude = -55.57320, 
    latitude  = -11.50566 
)
series <- sits_get_data(cube    = raster_cube,
                        samples = sample_latlong)
plot(series)
```

A useful case is when a set of labelled samples are available to be used as a training data set. In this case, one usually has trusted observations that are labelled and commonly stored in plain text files in comma-separated values (CSV) or using shapefiles (SHP). 

```{r, tidy="styler", message=FALSE}
# retrieve a list of samples described by a CSV file
samples_csv_file <- system.file("extdata/samples/samples_sinop_crop.csv",
                                package = "sits")
# for demonstration, read the CSV file into an R object
samples_csv <- read.csv(samples_csv_file)
# print the first three lines
samples_csv[1:3,]
```

To retrieve training samples for time series analysis, users need to provide the temporal information (`start_date` and `end_date`). In the simplest case, all samples share the same dates. That is not a strict requirement. Users can specify different dates, as long as they have a compatible duration. For example, the data set `samples_matogrosso_mod13q1` provided with the `sitsdata` package contains samples from different years covering the same duration. These samples were obtained from the MOD13Q1 product, which contains the same number of images per year. Thus, all time series in the data set `samples_matogrosso_mod13q1` have the same number of instances. 

Given a suitably built CSV sample file, `sits_get_data()` requires two parameters: (a) `cube`, the name of the R object that describes the data cube; (b) `samples`, the name of the CSV file. 
```{r, tidy="styler"}
# get the points from a data cube in raster brick format
points <- sits_get_data(
    cube = raster_cube, 
    samples = samples_csv_file)
# show the tibble with the first three points
points[1:3,]
```

Users can also specify samples by providing shapefiles or `sf` objects containing POINT or POLYGON geometries. The geographical location is inferred from the geometries associated with the shapefile or `sf` object. For files containing points, the geographical location is obtained directly. For polygon geometries, the parameter `n_sam_pol` (defaults to 20) determines the number of samples to be extracted from each polygon. The temporal information can be provided explicitly by the user; if absent, it is inferred from the data cube. If label information is available in the shapefile or `sf` object, users should include the parameter `label_attr` to indicate the column which contains the label to be associated with each time series.

```{r, tidy="styler", warning = FALSE, message = FALSE, error = FALSE}
# obtain a set of points inside the state of Mato Grosso, Brazil
shp_file <- system.file("extdata/shapefiles/mato_grosso/mt.shp", 
                        package = "sits")
# read the shapefile into an "sf" object
sf_shape <- sf::st_read(shp_file)
```

```{r, tidy="styler", eval = FALSE}
# create a data cube based on MOD13Q1 collection from BDC
modis_cube <- sits_cube(
    source      = "BDC",
    collection  = "MOD13Q1-6",
    bands       = c("NDVI", "EVI"),
    roi         = sf_shape,
    start_date  = "2020-06-01", 
    end_date    = "2021-08-29"
)
# read the points from the cube and produce a tibble with time series
samples_mt <- sits_get_data(
    cube         = modis_cube, 
    samples      = shp_file, 
    start_date   = "2020-06-01", end_date     = "2021-08-29", 
    n_sam_pol    = 20, multicores   = 4
)
```

## Filtering techniques for time series{-}

Satellite image time series generally is contaminated by atmospheric influence, geolocation error, and directional effects [@Lambin2006]. Atmospheric noise, sun angle, interferences on observations or different equipment specifications, as well as the very nature of the climate-land dynamics can be sources of variability [@Atkinson2012]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on a year-to-year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with  *noisy* and *non-homogeneous* data sets. In this vignette, we discuss filtering techniques to improve time series data that present missing values or noise.

The literature on satellite image time series has several applications of filtering to correct or smooth vegetation index data. The package supports the well-known Savitzky–Golay (`sits_sgolay()`) and Whittaker (`sits_whittaker()`) filters. In an evaluation of MERIS NDVI time series filtering for estimating phenological parameters in India, @Atkinson2012 found that the Whittaker filter provides good results. @Zhou2016 found that the Savitzky-Golay filter is good for reconstruction in tropical evergreen broadleaf forests.

### Savitzky–Golay filter{-}

The Savitzky-Golay filter works by fitting a successive array of $2n+1$ adjacent data points with a $d$-degree polynomial through linear least squares. The main parameters for the filter are  the polynomial degree ($d$) and the length of the window data points ($n$). In general, it produces smoother results for a larger value of $n$ and/or a smaller value of $d$ [@Chen2004]. The optimal value for these two parameters can vary from case to case. In SITS, the user can set the order of the polynomial using the parameter `order` (default = 3), the size of the temporal window with the parameter `length` (default = 5), and the temporal expansion with the parameter `scaling` (default = 1). The following example shows the effect of Savitsky-Golay filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01.


```{r, fig.align="center", out.width="90%", fig.cap="Savitzky-Golay filter applied on a multi-year NDVI time series."}

# Take NDVI band of the first sample data set
point_ndvi <- sits_select(point_mt_6bands, band = "NDVI")
# apply Savitzky Golay filter
point_sg <- sits_sgolay(point_ndvi, length = 11)
# merge the point and plot the series
sits_merge(point_sg, point_ndvi) %>% plot()
```

Notice that the resulting smoothed curve has both desirable and unwanted properties. For the period 2000 to 2008, the Savitsky-Golay filter removes noise resulting from clouds. However, after 2010, when the region has been converted to agriculture, the filter removes an important part of the natural variability from the crop cycle. Therefore, the `length` parameter is arguably too big and results in oversmoothing. Users can try to reduce this parameter and analyse the results.

### Whittaker filter{-}

The Whittaker smoother attempts to fit a curve that represents the raw data, but is penalized if subsequent points vary too much [@Atzberger2011]. The Whittaker filter is a balancing between the residual to the original data and the "smoothness" of the fitted curve. The filter has one parameter: $\lambda{}$ that works as a "smoothing weight" parameter. 

The following example shows the effect of Whitakker filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01. The `lambda` parameter controls the smoothing of the filter. By default, it is set to 0.5, a small value. For illustrative purposes, we show the effect of a larger smoothing parameter.
\newpage

```{r, fig.align="center", out.width="90%", fig.cap="Whittaker filter applied on a one-year NDVI time series."}

# Take NDVI band of the first sample data set
point_ndvi <- sits_select(point_mt_6bands, band = "NDVI")
# apply Whitakker filter
point_whit <- sits_whittaker(point_ndvi, lambda = 8)
# merge the point and plot the series
sits_merge(point_whit, point_ndvi) %>% plot()
```

In the same way as what is observed in the Savitsky-Golay filter, high values of the smoothing parameter `lambda` produce an over-smoothed time series that reduces the capacity of the time series to represent natural variations on crop growth. For this reason, low smoothing values are recommended when using the `sits_whittaker` function.

<!--chapter:end:06-timeseries.Rmd-->

# Improving the Quality of Training Samples{-}

```{r, echo = FALSE}
source("common.R")
```

Selecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has demonstrated that the number and quality of training samples are critical factors in obtaining accurate results [@Maxwell2018]. Large and accurate datasets are preferable, regardless of the algorithm used, while noisy training samples can negatively impact classification performance [@Frenay2014]. Thus, it is beneficial to use pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labeled or possess low discriminatory power.

One needs to distinguish between wrongly labelled samples and differences that result from natural variability of class signatures. When training data is collected over a large geographic region, natural variability of vegetation phenology leads to different patterns being assigned to the same label. A related issue is the limitation of crisp boundaries to describe the natural world. Class definitions use idealized descriptions (e.g., "a savanna woodland has tree cover of 50% to 90% ranging from 8 to 15 meters in height"). In practice, the boundaries between classes are fuzzy and sometimes overlap, making it hard to distinguish between them. To help users improve sample quality, `sits` provides methods for evaluate training data

## Geographical variability of training samples{-}

When working with machine learning classification of Earth observation data, it is important to evaluate if the training samples are well distributed in the study area. In many cases, training data comes from ground surveys made at chosen location. When working in large areas, ideally one needs a representative sample which captures spatial variability. In practice, however, ground surveys or other means of data collection are limited to selected areas. In many case, the geographical distribution of the training data does not cover the study area equally. Such mismatch can be a problem for achieving a good quality classification. As stated by Meyer and Pebesma[@Meyer2022]: "large gaps in geographic space do not always imply large gaps in feature space". 

Meyer and Pebesma[@Meyer2022] propose the use of spatial distance distribution plot, which display two distributions of nearest-neighbor distances: sample-to-sample and prediction-location-to-sample. The difference between the two distributions reflects the degree of spatial clustering in the reference data. Ideally, the two distributions should be similar. Cases where the sample-to-sample distance distribution does not match prediction-location-to-sample distribution indicate possible problems of training data collection. 

`sits` implements spatial distance distribution plots with the `sits_geo_dist()` function. This function asks users to provide their training data in the `samples` parameter, and the study area in the `roi` parameter expressed as an `sf` object. Additional parameters are `n` (maximum number of samples for each distribution) and `crs` (coordinate reference system for the samples). By default, `n` is 1000, and `crs` is "EPSG:4326". The example below shows how to use `sits_geo_dist()`. 

```{r, tidy = "styler", out.width = "100%", message = FALSE, warning = FALSE, fig.cap = "SOM map for the Cerrado samples"}
# read a shapefile for the state of Mato Grosso, Brazil
mt_shp <- system.file("extdata/shapefiles/mato_grosso/mt.shp",
                      package = "sits")
# convert to an sf object
mt_sf <- sf::read_sf(mt_shp)

# calculate sample-to-sample and sample-to-prediction distances
distances <- sits_geo_dist(
    samples = samples_modis_ndvi,
    roi = mt_sf)
# plot sample-to-sample and sample-to-prediction distances
plot(distances)
```
The plot shows a mismatch between the sample-to-sample and the sample-to-prediction distribution. Most samples are closer to each other then they are close to the locatiom where values need to be predicted. In this case, there are many areas where few or no samples have been collected and where the prediction uncertainty will be higher. In this and similar cases, users should invest in improving the distribution of training samples. If that is not possible, they should be aware that areas with insufficient samples could have lower accuracy, and that fact should be reported to potential users of their results. 


## Hierachical clustering for sample quality control{-}

The package provides two clustering methods to assess sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b) Self-organizing Maps (SOM). These methods have different computational complexities. AHC has a computational complexity of $\mathcal{O}(n^2)$ given the number of time series $n$, whereas SOM complexity is linear with respect to n. For large data, AHC requires an substantial amount of memory and running time; in these cases, SOM is recommended. In this section, we describe how to run AHC in `sits`. The SOM-based technique is presented in the following section.

Agglomerative hierarchical clustering (AHC) computes the dissimilarity between any two elements from a data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. This approach is useful for exploring samples due to its visualization power and ease of use [@Keogh2003]. In `sits`, AHC is implemented using `sits_cluster_dendro()`.

```{r, tidy = "styler", cache=TRUE, fig.align="center", out.width="90%", fig.cap="Example of hierarchical clustering for a two class set of time series", message=FALSE}
# take a set of patterns for 2 classes
# create a dendrogram, plot, and get the optimal cluster based on ARI index
clusters <- sits_cluster_dendro(
    samples = cerrado_2classes, 
    bands = c("NDVI", "EVI"),
    dist_method = "dtw_basic",
    linkage =  "ward.D2"
)
```

The `sits_cluster_dendro()` function has one mandatory parameter (`samples`), where users should provide the samples to be evaluated. Optional parameters include `bands`, `dist_method` and `linkage`. The `dist_method` parameter specifies how to calculate the distance between two time series. We recommend a metric that uses dynamic time warping (DTW)[@Petitjean2012], as DTW is reliable method for measuring differences between satellite image time series [@Maus2016]. The options available in `sits` are based on those provided by package `dtwclust`, which include `dtw_basic`, `dtw_lb`, and `dtw2`. Please check `?dtwclust::tsclust` for more information on DTW distances.

The `linkage` parameter defines the distance metric between clusters. The recommended linkage criteria are: `complete` or `ward.D2`. Complete linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples, but results are sensitive to outliers. As an alternative, Ward proposes to use the sum-of-squares error to minimize data variance [@Ward1963]; his method is available as `ward.D2` option to the `linkage` parameter.  To cut the dendrogram, the `sits_cluster_dendro()` function computes the adjusted rand index (ARI) [@Rand1971] and returns the height where the cut of the dendrogram maximizes the index . In the example, the ARI index indicates that six (6) clusters are present. The result of `sits_cluster_dendro()` is a time series tibble with one additional column, called "cluster". The function `sits_cluster_frequency()` provides information on the composition of each cluster.

```{r}
# show clusters samples frequency
sits_cluster_frequency(clusters)
```

The cluster frequency table shows that each cluster has a predominance of either "Cerrado" or "Pasture" class with the exception of cluster 3 which has a mix of samples from both classes. Such confusion may have resulted from incorrect labeling, inadequacy of selected bands and spatial resolution, or even a natural confusion due to the variability of the land classes. To remove cluster 3, use `dplyr::filter()`. The resulting clusters still contained mixed labels, possibly resulting from outliers. In this case, users may want to remove the outliers and leave only the most frequent class using `sits_cluster_clean()`. After cleaning the samples, the result set of samples is likely to improve the classification results.

```{r}
# remove cluster 3 from the samples
clusters_new <- dplyr::filter(clusters, cluster != 3)
# clear clusters, leaving only the majority class
clean <- sits_cluster_clean(clusters_new)
# show clusters samples frequency
sits_cluster_frequency(clean)
```


## Using SOM for sample quality control{-}

<a href="https://www.kaggle.com/esensing/using-som-for-sample-quality-control-in-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"></a>

As an alternative for hierarchical clustering for quality control of training samples, SITS provides a clustering technique based on self-organizing maps (SOM). SOM is a dimensionality reduction technique [@Kohonen1990], where high-dimensional data is mapped into a two dimensional map, keeping the topological relations between data patterns. As the shown below, the SOM 2D map is composed by units called neurons. Each neuron has a weight vector, with the same dimension as the training samples. At the start, neurons are assigned a small random value and then trained by competitive learning. The algorithm computes the distances of each member of the training set to all neurons and finds the neuron closest to the input, called the best matching unit.

```{r, out.width = "90%", out.height = "90%", echo = FALSE, fig.align="center", fig.cap="SOM 2D map creation (source: Santos et al.(2021). Reproduction under fair use doctrine."}

knitr::include_graphics("images/som_structure.png")
```

The input data for quality assessment is a set of training samples, which are high-dimensional data; for example, a time series with 25 instances of 4 spectral bands has 100 dimensions. When projecting a high-dimensional data set into a 2D SOM map, the units of the map (called neurons) compete for each sample. Each time series will be mapped to one of the neurons. Since the number of neurons is smaller than the number of classes, each neuron will be associated to many time series. The resulting 2D map will be a set of clusters. Given that SOM preserves the topological structure of neighborhoods in multiple dimensions, clusters that contain training samples of a given class will usually be neighbors in 2D space. The neighbors of each neuron of a SOM map provide information on intraclass and interclass variability which is used to detect noisy samples. The methodology of using SOM for sample quality assessment is discussed in detail in the reference paper [@Santos2021a].

```{r, out.width = "90%", out.height = "90%", echo = FALSE, fig.align="center", fig.cap="Using SOM for class noise reduction (source: Santos et al.(2021). Reproduction under fair use doctrine."}

knitr::include_graphics("images/methodology_bayes_som.png")
```

As an example, we take a set of time series from the Cerrado region of Brazil, the second largest biome in South America with an area of more than 2 million km2. This data ranges from 2000 to 2017 and includes 50,160 land use and cover samples divided into 12 classes("Dense_Woodland", "Dunes", "Fallow_Cotton", "Millet_Cotton", "Pasture", "Rocky_Savanna", "Savanna", "Savanna_Parkland", "Silviculture", "Soy_Corn", "Soy_Cotton", "Soy_Fallow"). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 4 bands ("EVI", "NDVI", "MIR", and "NIR"). We use bands "NDVI" and "EVI" for faster processing.

```{r, message = FALSE, tidy = "styler", warning = FALSE}
# take only the NDVI and EVI bands
samples_cerrado_mod13q1_2bands <- sits_select(
    data = samples_cerrado_mod13q1, 
    bands = c("NDVI", "EVI")
)
# show the summary of the samples
sits_labels_summary(
  data = samples_cerrado_mod13q1_2bands
)
```

## Creating the SOM map{-}

To perform the SOM-based quality assessment, the first step is to run `sits_som_map()` which uses the `kohonen` R package [@Wehrens2018] to compute a SOM grid, controlled by five parameters. The grid size is given by `grid_xdim` and `grid_ydim`. The starting learning rate is `alpha`, which decreases during the interactions. To measure separation between samples, use `distance` (either "sumofsquares" or "euclidean"). The number of iterations is set by `rlen`. For more details, please consult `?kohonen::supersom`.


```{r, tidy = "styler", out.width = "100%", message = FALSE, warning = FALSE, fig.cap = "SOM map for the Cerrado samples"}
# clustering time series using SOM
som_cluster <- sits_som_map(samples_cerrado_mod13q1_2bands,
    grid_xdim = 15,
    grid_ydim = 15,
    alpha = 1.0,
    distance = "euclidean",
    rlen = 20
)
# plot the som map
plot(som_cluster)
```

The output of the `sits_som_map()` is a list with 3 elements: (a) the original set of time series with two additional columns for each time series: `id_sample` (the original id of each sample) and `id_neuron` (the id of the neuron to which it belongs); (b) a tibble with information on the neurons. For each neuron, it gives the prior and posterior probabilities of all labels which occur in the samples assigned to it; and (c) the SOM grid. To plot the SOM grid, use `plot()`. The neurons are labelled using majority voting.

The SOM grid shows that most classes are associated to neurons close to each other.  The are exceptions. Some "Pasture" neurons are far from the main cluster, because  the transition between areas of open savanna and pasture is not always well defined and depends on climate and latitude. Also, the neurons associated to "Soy_Fallow" are dispersed in the map; this indicates possible problems in distinguishing this class from the other agricultural classes. The SOM map can be used to remove outliers, as shown below.

## Measuring confusion between labels using SOM{-}

The second step in SOM-based quality assessment is understanding the confusion between labels. The function `sits_som_evaluate_cluster()` groups neurons by their majority label and produces a tibble. For each label, the tibble show the percentage of samples with a different label that have been mapped to a neuron whose majority is that label.

```{r}
# produce a tibble with a summary of the mixed labels
som_eval <- sits_som_evaluate_cluster(som_cluster)
# show the result
som_eval 
```

Many labels are associated to clusters where there are some samples with a different label. Such confusion between labels arises because visual labeling of samples is subjective and can be biased. In many cases, interpreters use high-resolution data to identify samples. However, the actual images to be classified are captured by satellites with lower resolution. In our case study, a MOD13Q1 image has pixels with 250 x 250 meter resolution and as such the correspondence between labelled locations in high-resolution images and mid to low-resolution images is not direct. The confusion by class can be visualized in a bar plot using `plot()`, as shown below. The bar plot shows some confusion between the classes associated to the natural vegetation typical of the Brazilian Cerrado ("Savanna", "Savanna_Parkland", "Rocky_Savanna"). This mixture is due to the large variability of the natural vegetation of the Cerrado biome, which makes it difficult to draw sharp boundaries between each label. Some confusion is also visible between the agricultural classes. The "Millet_Cotton" class is a particularly difficult one, since many of the samples assigned to this class are confused with "Soy_Cotton" and "Fallow_Cotton". 

```{r, out.width = "90%", fig.align="center", fig.cap="Confusion between classes as measured by SOM."}
# plot the confusion between clusters
plot(som_eval)
```

## Detecting noisy samples using SOM{-}

The third step in the quality assessment uses the discrete probability distribution associated to each neuron, which is included in the `labelled_neurons` tibble produced by `sits_som_map()`. More homogeneous neurons (those with a single class of high probability) are assumed to be composed of good quality samples. Heterogeneous neurons (those with two or more classes with significant probability) are likely to contain noisy samples. The algorithm computes two values for each sample:

- *prior probability*: the probability that the label assigned to the sample is correct, considering only the samples contained in the same neuron. For example, if a neuron has 20 samples, of which 15 are labeled as "Pasture" and 5 as "Forest", all samples labeled "Forest" are assigned a prior probability of 25%. This is an indication that the "Forest" samples in this neuron may not be of good quality.

- *posterior probability*: the probability that the label assigned to the sample is correct, considering the neighboring neurons. Take the case of the above-mentioned neuron whose samples labeled "Pasture" have a prior probability of 75%. What happens if all the neighboring samples have "Forest" as a majority label? To answer this question, we use Bayesian inference to estimate if these samples are noisy based on the neighboring neurons [@Santos2021]. The Bayesian inference method, described in the Technical Annex, recalculates the class probabilities in a neuron, based in its neighbors. 

To identify noisy samples, we take the result of the `sits_som_map()` function as the first argument to the function `sits_som_clean_samples()`. This function finds out which samples are noisy, those that are clean, and some that need to be further examined by the user. It requires the `prior_threshold` and `posterior_threshold` parameters according to the following rules:

-   If the prior probability of a sample is less than `prior_threshold`, the sample is assumed to be noisy and tagged as "remove";
-   If the prior probability is greater or equal to `prior_threshold` and the posterior probability calculated by Bayesian inference is greater or equal to `posterior_threshold`, the sample is assumed not to be noisy and thus is tagged as "clean";
-   If the prior probability is greater or equal to `prior_threshold` and the posterior probability is less than `posterior_threshold`, we have a situation the sample is part of the majority level of those assigned to its neuron, but its label is not consistent with most of its neighbors. This is an anomalous condition and is tagged as "analyze". Users are encouraged to inspect such samples to find out whether they are in fact noisy or not.

The default value for both `prior_threshold` and `posterior_threshold` is 60%. The `sits_som_clean_samples()` has an additional parameter (`keep`) which indicates which samples should be kept in the set based on their prior and posterior probabilities. The default for `keep` is `c("clean", "analyze")`. As a result of the cleaning, about 900 samples have been considered to be noisy and thus removed.

```{r, tidy = "styler", message = FALSE, warning = FALSE}
new_samples <- sits_som_clean_samples(
    som_map = som_cluster, 
    prior_threshold = 0.6,
    posterior_threshold = 0.6,
    keep = c("clean", "analyze")
)
# print the new sample distribution
sits_labels_summary(new_samples)
```

All samples of the class which had the highest confusion with others("Millet_Cotton") have been removed. Most samples of class "Silviculture" (planted forests) have also been removed, since in the SOM map they have been confused with natural forests and woodlands. Further analysis includes calculating the SOM map and confusion matrix for the new set, as shown in the following example. 

```{r, tidy = "styler", message = FALSE, warning = FALSE}
# evaluate the misture in the SOM clusters of new samples
new_cluster <- sits_som_map(
   data = new_samples,
   grid_xdim = 15,
   grid_ydim = 15,
   alpha = 1.0,
   distance = "euclidean",
)
```

```{r, out.width="90%", fig.align="center", fig.cap="Cluster confusion plot for samples cleaned by SOM"}
new_cluster_mixture <- sits_som_evaluate_cluster(new_cluster)
# plot the mixture information.
plot(new_cluster_mixture)
```

As expected, the new confusion map shows a significant improvement over the previous one. This result should be interpreted carefully, since it may be due to different effects. The most direct interpretation is that "Millet_Cotton" and "Silviculture" cannot be easily separated from the other classes, given the current attributes (a time series of "NDVI" and "EVI" indices from MODIS images). In such situations, users should consider improving the number of samples from the less represented classes, including more MODIS bands, or working with higher resolution satellites. Results of the SOM method should be interpreted based on the users' understanding of the ecosystems and agricultural practices of the study region. 

A further comparison between the original and clean samples is to run a 5-fold validation on the original and on the cleaned sample sets using `sits_kfold_validate()` and a random forests model. The SOM procedure improves the validation results from 95% on the original data set to 99% in the cleaned one. This improvement should not be interpreted as providing better fit for the final map accuracy. A 5-fold validation procedure only measures how well the machine learning model fits the samples; it is not an accuracy assessment of classification results. The result only indicates that the resulting training set after the SOM sample removal procedure is more internally consistent than the original one. For more details on accuracy measures, please see chapter on "Validation and Accuracy Measures".

```{r, tidy = "styler", message = FALSE, warning = FALSE}
# run a k-fold validation
assess_orig <- sits_kfold_validate(
    samples = samples_cerrado_mod13q1_2bands, 
    folds = 5,
    ml_method = sits_rfor()
)
# print summary 
sits_accuracy_summary(assess_orig)
```

```{r, tidy = "styler", message = FALSE, warning = FALSE}
assess_new <- sits_kfold_validate(
    samples = new_samples,
    folds = 5,
    ml_method = sits_rfor()
)
# print summary 
sits_accuracy_summary(assess_new)
```

The SOM-based analysis discards samples which can be confused with samples of other classes. After removing noisy samples or uncertain classes, the data set obtains a better validation score since there is less confusion between classes. Users should analyse the results with care. Not all discarded samples are low quality ones. Confusion between samples of different classes can result from inconsistent labeling or from the lack of capacity of satellite data to distinguish between chosen classes. When many samples are discarded, as in the current example, it is advisable to revise the whole classification schema. The aim of selecting training data should always be to match the reality in the ground to the power  of remote sensing data to identify differences. No analysis procedure can replace actual user experience and knowledge of the study region. 

## Reducing sample imbalance{-} 

Many training samples for Earth observation data analysis are imbalanced. This situation arises when the distribution of samples associated to each class is uneven. One example is the Cerrado data set used in this chapter. The three most frequent classes ("Dense Woodland", "Savanna" and "Pasture") include 53% of all samples, while the three least frequent classes ("Millet-Cotton", "Silviculture", and "Dunes") comprise only 2.5% of the data set. Sample imbalance is an undesirable property of a training set. Since machine learning algorithms tend to be more accurate for classes with many samples. The instances belonging to the minority group are misclassified more often than those belonging to the majority group. Thus, reducing sample imbalance can have a positive effect on classification accuracy[@Johnson2019]. 

The function `sits_reduce_imbalance()` deals with class imbalance; it oversamples minority classes and undersamples majority ones. Oversampling requires generation of synthetic samples. The package uses the SMOTE method that estimates new samples by considering the cluster formed by the nearest neighbors of each minority class. SMOTE takes two samples from this cluster and produces a new one by a random interpolation between them [@Chawla2002].

To perform undersampling, `sits_reduce_imbalance()` builds a SOM map for each majority class, based on the required number of samples to be selected. Each dimension of the SOM is set to `ceiling(sqrt(new_number_samples/4))` to allow a reasonable number of neurons to group similar samples. After calculating the SOM map, the algorithm extracts four samples per neuron to generate a reduced set of samples that approximates the variation of the original one. 

The `sits_reduce_imbalance()` algorithm has two parameters: `n_samples_over` and `n_samples_under`. The first parameter ensures that all classes with samples less than its value are oversampled. The second parameter  controls undersampling; all classes with more samples than its value are undersampled. The following example shows the use of `sits_reduce_imbalance()` in the Cerrado data set used in this chapter. We generate a balanced data set where all classes have between 1000 and 1500 samples. We use `sits_som_evaluate_cluster()` to estimate the confusion between classes of the balanced data set.

```{r, tidy = "styler"}
# reducing imbalances in the Cerrado data set
balanced_samples <- sits_reduce_imbalance(
    samples = samples_cerrado_mod13q1_2bands,
    n_samples_over = 1000,
    n_samples_under = 1500,
    multicores = 4
)
```

```{r, tidy = "styler"}
# print the balanced samples
# some classes have more than 1500 samples due to the SOM map
# each class has betwen 10% and 6% of the full set
sits_labels_summary(balanced_samples)
```

```{r, tidy = "styler", message = FALSE, warning = FALSE}
# clustering time series using SOM
som_cluster_bal <- sits_som_map(
    data = balanced_samples,
    grid_xdim = 10,
    grid_ydim = 10,
    alpha = 1.0,
    distance = "euclidean",
    rlen = 20
)
```


```{r}
# produce a tibble with a summary of the mixed labels
som_eval <- sits_som_evaluate_cluster(som_cluster_bal)
```

```{r, fig.align="center", out.width="90%", fig.cap="Confusion by cluster for the balanced data set"}
# show the result
plot(som_eval) 
```

As shown in the Figure, the balanced data set shows less confusion per class than the unbalanced one. In this case, many of the classes which were confused with other in the original confusion map are now better represented. Reducing class imbalance should be tried as an alternative to reducing the number of samples of the classes using SOM. In general, users should try to balance their training data for better performance. 

## Conclusion{-}

The quality of training data is critical to improve the accuracy of maps resulting from machine learning classification methods. To address this challenge, the `sits` package provides three methods for improving training samples. For large datasets, we recommend using both imbalance reducing and SOM-based algorithms. The SOM-based method identifies potential mislabeled samples and outliers that require further investigation. The results demonstrate a positive impact on the overall classification accuracy.

Users need to take care when defining their classification schema. The complexity and diversity of our planet defies simple class names with hard boundaries. Due to representational and data handling issues, all classification systems have a limited number of categories, which inevitably fail to properly describe the nuances of the planet's landscapes. All representation systems are thus limited and application-dependent. As stated by Janowicz [@Janowicz2012]: "geographical concepts are situated and context-dependent and can be described from different, equally valid, points of view; thus, ontological commitments are arbitrary to a large extent". 

The availability of big data and satellite image time series is a further challenge. In principle, image time series can capture more subtle changes for land classification. In practice, experts need to conceive classification systems and training data collection by understanding how time series information relate to actual land change. Methods for quality analysis such as those presented in this chapter cannot replace user understanding and informed choices. 


<!--chapter:end:07-clustering.Rmd-->

# Machine Learning for Data Cubes{-}

```{r, echo = FALSE}
source("common.R")
if (!file.exists("./tempdir/chp8"))
    dir.create("./tempdir/chp8")
```


## Machine learning classification{-}

<a href="https://www.kaggle.com/brazildatacube/sits-classification-r" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

Machine learning classification is a kind of supervised learning in which an algorithm is trained to predict which category or class an input data point belongs to. It involves teaching a computer program to recognize patterns in data and use those patterns to predict the class label of new data. The goal of classification is to build a model that can accurately assign a class label to new data based on patterns it has learned from previously labeled data. In `sits`, machine learning is used to classify individual time series, using the `time-first` approach. 

The goal of a machine learning models is to approximate a function $y = f(x)$ that maps an input $x$ to a category $y$. A model defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation [@Goodfellow2016]. The difference between the different algorithms is the approach they take to build the mapping that classifies the input data.  

The `sits` package includes two kinds of methods for time series classification: 

1. Machine learning algorithms that do not explicitly consider the temporal stucture of the time series. They treat time series as a vector in a high-dimensional feature space. Each instance of the time series is taken by the algorithm as independent from the others. They include random forest (`sits_rfor()`), support vector machines (`sits_svm()`), extreme gradient boosting (`sits_xgboost()`), and multilayer perceptrons (`sits_mlp()`). 

2. Deep learning methods designed to work with time series. Temporal relations between observed values in a time series are taken into account. From this kind of models, `sits` supports 1D convolution neural networks  (`sits_tempcnn()`), residual 1D networks (`sits_resnet()`), and temporal attention-based encoders (`sits_tae()` and `sits_lighttae()`). In these algorithms, the order of the samples in the time series is relevant for the classifier. 

Based on experience with `sits`, random forests, extreme gradient boosting, and temporal deep learning models outperform SVM and multilayer perceptron models. This is because the temporal behavior of land use and land cover classes varies, with certain dates providing more information than others. For instance, when monitoring deforestation, dates that correspond to forest removal actions are more informative than earlier or later dates. Similarly, in crop mapping, a few dates may capture a large portion of the variation. Classification methods that consider the temporal order of samples are therefore more likely to capture the seasonal behavior of image time series. Random forest and extreme gradient boosting methods that use individual measures as nodes in decision trees can also capture specific events such as deforestation.

The following examples show how to train machine learning methods and apply it to classify a single time series. We use the set `samples_matogrosso_mod13q1`, containing time series samples from Brazilian Mato Grosso state, obtained from the MODIS MOD13Q1 product. It has 1,892 samples and 9 classes (Cerrado, Fallow_Cotton, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet, Soy_Sunflower). Each time series covers 12 months (23 data points) with 6 bands ("NDVI", "EVI", "BLUE", "RED", "NIR", "MIR"). The samples are arranged along an agricultural year, starting in September and ending in August. The data set was used in the paper "Big Earth observation time series analysis for monitoring Brazilian agriculture" [@Picoli2018], and is available in the R package `sitsdata`. Please see the "Setup" section for instructions on how to obtain this package.

The results should not be taken as indication of which method performs better. The most important factor for achieving a good result is the quality of the training data [@Maxwell2018]. Experience shows that classification quality depends on the training samples and on how well the model matches these samples. For examples of ML for classifying large areas, please see the papers by the authors [@Picoli2018; @Picoli2020a; @Simoes2020; @Ferreira2020a].

## Visualizing Sample Patterns{-}

One useful way of describing and understanding the samples is by plotting them. A direct way of doing so is using the `plot` function, as discussed in the "Working with Time Series" chapter. A useful alternative is to estimate a statistical approximation to an idealized pattern based on a generalized additive models (GAM). A GAM is a linear model in which the linear predictor depends linearly on a smooth function of the predictor variables 

$$
y = \beta_{i} + f(x) + \epsilon, \epsilon \sim N(0, \sigma^2).
$$ 

The function `sits_patterns()` uses a GAM to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. This function is based on the R package `dtwSat`[@Maus2019], which implements the TWDTW time series matching method described in @Maus2016. The resulting patterns can be viewed using `plot`.

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Patterns for the samples for Mato Grosso."}
# Estimate the patterns for each class and plot them
samples_matogrosso_mod13q1 %>% 
    sits_patterns() %>% 
    plot()
```

The resulting patterns provide some insights over the time series behavior of each class. The response of the Forest class is quite distinctive.  They also shows that it should be possible to separate between the single cropping classes and the double cropping ones. There are similarities between the double-cropping classes (Soy_Corn, Soy_Millet, Soy_Sunflower and Soy_Sunflower) and  between the Cerrado and Pasture classes. The subtle differences between class signatures provide hints to possible ways by which machine leaning algorithms might distinguish between classes. One example is the difference between the middle-infrared response during the dry season (May to September) to distinguish between the Cerrado and Pasture classes. 

## Common interface to machine learning and deep learning models{-}

The `sits_train()` function provides a common interface to all machine learning models. This function takes two mandatory parameters: the training data (`samples`) and the ML algorithm (`ml_method`). After the model is estimated, it can be used to classify individual time series or data cubes with `sits_classify()`. In what follows, we show how to apply each method for the classification of a single time series. Then, in the "Classification of Images in Data Cubes" we discuss how to classify data cubes.

Since `sits` is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on  testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this chapter. 

When a set of time series organised as tibble is taken as input to the classifier, the result is the same tibble with one additional column ("predicted"), which contains the information on what labels are have been assigned for each interval. The results can be shown in text format using the function `sits_show_prediction()` or graphically using `plot()`.

## Random forest{-}

Random forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, where each decision tree is trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data, and the final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be useful in feature selection and data visualization. Pelletier et al[@Pelletier2016] discuss the robustness of random forest method for satellite image time series classification. 

```{r, echo = FALSE, out.width = "90%", fig.align="center", fig.cap="Random forests algorithm (source: Venkata Jagannath in Wikipedia - licenced as CC-BY-SA 4.0.)"}

knitr::include_graphics("images/random_forest.png") 
```

SITS provides `sits_rfor()`, which uses the R `randomForest` package [@Wright2017]; its main parameter is `num_trees`, which is the number of trees to grow with a default value of 100. The model can be visualized using `plot()`.

```{r, echo = FALSE}
set.seed(290356)
```

```{r, tidy="styler",out.width = "90%", fig.align="center", fig.cap="Most important variables in random forests model."}
# Train the Mato Grosso samples with Random Forests model.
rfor_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_rfor(num_trees = 100)
)
# plot the most important variables of the model
plot(rfor_model)
```

The most important explanatory variables are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band in date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes.  Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural classes and natural ones. This corresponds to summer time when the ground is drier and crops have been harvested.


```{r, tidy="styler",out.width = "90%", fig.align="center", fig.cap="Classification of time series using random forests."}
# retrieve a point to be classified
point_mt_4bands <- sits_select(
    data = point_mt_6bands, 
    bands = c("NDVI", "EVI", "NIR", "MIR")
)
# Classify using Random Forest model and plot the result
point_class <- sits_classify(
    data = point_mt_4bands, 
    ml_model  = rfor_model
)
plot(point_class, bands = c("NDVI", "EVI"))
```

The result shows that the area started out as a forest in 2000, it was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. They are consistent with expert evaluation of the process of land use change in this region of Amazonia.

Random forests are robust to outliers and able to deal with irrelevant inputs [@Hastie2009]. The method tends to overemphasize some variables because its performance tends to stabilize after a part of the trees are grown [@Hastie2009]. In cases where abrupt change takes place, such deforestation mapping, random forests (if properly trained) will emphasize the temporal instances and bands that capture such quick change. 

## Support Vector Machines{-}

The support vector machine (SVM) classifier is a generalization of a linear classifier which finds an optimal separation hyperplane that minimizes misclassification [@Cortes1995]. Since a set of samples with $n$ features defines an n-dimensional feature space, hyperplanes are linear ${(n-1)}$-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations[@James2013]. The maximal margin is computed as the the smallest distance from the observations to the hyperplane. The solution for the hyperplane coefficients depends only on the samples that define the maximum margin criteria, the so-called support vectors.

```{r, echo = FALSE, out.width = "50%", fig.align="center", fig.cap="Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (source: Larhmam in Wikipedia - licensed as CC-BY-SA-4.0 )."}

knitr::include_graphics("images/svm_margin.png") 
```

For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. The new classification model, despite having a linear boundary on the enlarged feature space, generally translates its hyperplane to a nonlinear boundary in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been applied to classify remote sensing data [@Mountrakis2011].

In `sits`, SVM is implemented as a wrapper of `e1071` R package that uses the `LIBSVM` implementation [@Chang2011], the `sits` package adopts the *one-against-one* method for multiclass classification. For a $q$ class problem, this method creates ${q(q-1)/2}$ SVM binary models, one for each class pair combination and tests any unknown input vectors throughout all those models. The overall result is computed by a voting scheme.

The example below shows how to apply the SVM method for classification of time series using default values. The main parameters for the SVM are `kernel` which controls whether to use a non-linear transformation (default is `radial`), `cost` which measures the punishment for wrongly-classified samples (default is 10), and `cross` which sets the value of the k-fold cross validation (default is 10).

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using SVM."}
# Train a SVM model
svm_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_svm())
# Classify using SVM model and plot the result
point_class <- sits_classify(
    data = point_mt_4bands, 
    ml_model = svm_model
)
# plot the result
plot(point_class, bands = c("NDVI", "EVI"))

```
The SVM classifier is less stable and less robust to outliers than the random forests method. In this example, it tends to misclassify some of the data. In 2008, it is likely that the land cover was still "Pasture" rather than "Soy_Millet" as produced by the algorithm, while the "Soy_Cotton" class in 2012 is also inconsistent with the previous and latter classification of "Soy_Corn".


## Extreme Gradient Boosting{-}

The boosting method is based on the idea of starting from a weak predictor and then improving performance sequentially by fitting a better model at each iteration. It starts by fitting a simple classifier to the training data, and using the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although both random forests and boosting use trees for classification, there are important differences. The performance of random forests generally increases with the number of trees until it becomes stable. Boosting trees improve on previous result by applying finer divisions that improve the performance [@Hastie2009]. However, the number of trees grown by boosting techniques has to be limited at the risk of overfitting.

Gradient boosting is a variant of boosting methods where the cost function is minimized by gradient descent. Extreme gradient boosting [@Chen2016], called XGBoost, is an efficient approximation to the gradient loss function. Some recent papers show that it outperforms random forests for remote sensing image classification[@Jafarzadeh2021]. However, this result is not generalizable, since actual performance is controlled by the quality of the training data set.

In SITS, the XGBoost method is implemented by the `sits_xbgoost()` function, which is based on `XGBoost` R package and has five hyperparameters that require tuning. The `sits_xbgoost()` function takes the user choices as input to a cross validation to determine suitable values for the predictor.

The learning rate `eta` varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value `gamma` specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The `max_depth` value controls the maximum depth of the trees. Increasing this value will make the model more complex and more likely to overfit (default is 6). The `subsample` parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The `nrounds` parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. In order to follow the convergence of the algorithm, users can turn the `verbose` parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the random forests method.

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using XGBoost."}
# Train using  XGBoost
xgb_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_xgboost(verbose = 0)
)
# Classify using SVM model and plot the result
point_class <- sits_classify(
    data = point_mt_4bands, 
    ml_model = xgb_model
)
plot(point_class, bands = c("NDVI", "EVI"))
```


## Deep learning using multilayer perceptrons{-}

To support deep learning methods, `sits` uses the `torch` R package, which takes the Facebook `torch` C++ library as a back-end. Machine learning algorithms that use the R `torch` package are similar from those developed using `PyTorch`. Converting image time series algorithms developed in `PyTorch` to be using in `sits` is straightforward. Please see the chapter on "Extensibility" for guidance on how to include new deep learning algorithms in `sits`. 

The simplest deep learning method is multilayer perceptrons (MLPs), which are feedforward artificial neural networks. A MLP consists of three kinds of nodes: an input layer, a set of hidden layers and an output layer. The input layer has the same dimension as the number of the features in the data set. The hidden layers attempt to approximate the best classification function. The output layer makes a decision about which class should be assigned to the input.

In `sits`, users build MLP models using `sits_mlp()`. Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the `layers` parameters, which is a list of integer values. The size of the list is the number of layers and each element of the list indicates the number of nodes per layer.

The choice of the number of layers depends on the inherent separability of the data set to be classified. For data sets where the classes have different signatures, a shallow model (with 3 layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. The user should be aware that some models with many hidden layers may take a long time to train and may not be able to converge. The suggestion is to start with 3 layers and test different options of number of neurons per layer, before increasing the number of layers.

MLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [@Goodfellow2016], we use the `relu` activation function.

Users can also define the optimization method (`optimizer`), which defines the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [@Ruder2016]. Based on experience with image time series, we recommend that users start by using the default method provided by `sits`, which is the `optimizer_adamw` method from package `torchopt`. Please refer to the `torchopt` package for additional information.

Another relevant parameter is the list of dropout rates (`dropout`). Dropout is a technique for randomly dropping units from the neural network during training [@Srivastava2014]. By randomly discarding some neurons, dropout reduces overfitting. Since the purpose of a cascade of neural nets is to improve learning as more data is acquired, discarding some neurons may seem a waste of resources. In practice, dropout prevents an early convergence to a local minimum [@Goodfellow2016]. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.

The following example shows how to use `sits_mlp()`. The default parameters for have been chosen based on a modified verion of @Wang2017, which proposes the use of multilayer perceptrons as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter `layers`; (b) Using the "relu" activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the "optimizer_adamw" as optimizer (default value); (e) a number of training steps (`epochs`) of 100; (f) a `batch_size` of 64, which indicates how many time series are used for input at a given steps; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set side for validation.

In our experience, if the training data is of good quality, using 3 to 5 layers is a reasonable compromise. Further increase on the number of layers will not improve the model. To simplify the output generation, the `verbose` option has been turned off. The default value is "on". After the model has been generated, we plot its training history. 

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Evolution of training accuracy of MLP model."}
# train using an MLP model
# this is an example of how to set parameters
# first-time users should test default options first
mlp_model <- sits_train(
    samples = samples_matogrosso_mod13q1, 
    ml_method = sits_mlp(
        layers           = c(512, 512, 512),
        dropout_rates    = c(0.40, 0.30, 0.20),
        epochs           = 100,
        batch_size       = 64,
        verbose          = FALSE,
        validation_split = 0.2
    ) 
)
# show training evolution
plot(mlp_model)
```
\newpage
Then, we classify a 16-year time series using the multilayer perceptron model.

```{r, tidy="styler", out.width = "90%", fig.align="center", fig.cap="Classification of time series using MLP."}
# Classify using DL model and plot the result
point_mt_6bands %>% 
    sits_select(bands = c("NDVI", "EVI", "NIR", "MIR")) %>% 
    sits_classify(mlp_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

In theory, multilayer perceptron model is able to capture more subtle changes than the random forests and XGBoost models. In this specific case, the result is similar to the them. Although the model mixes the "Soy_Corn" and "Soy_Millet" classes, the distinction between their temporal signatures is quite subtle. Also in this case, this suggests the need to improve the number of samples. In this examples, the MLP model shows an increase in the sensitivity compared to previous models. We recommend that the users compare different configurations, since the MLP model is sensitive to changes in its parameters.

## Temporal Convolutional Neural Network (TempCNN){-}

Convolutional neural networks (CNN) are a class of deep learning methods that apply convolution filters (sliding window) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture that is specifically designed for processing sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series and produces another time series as the result of the convolution. 

TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN is capable of capturing complex temporal patterns in the data and producing accurate predictions.

The TempCNN architecture for satellite image time series classification is proposed by Pelletier et al [@Pelletier2019].  It has three 1D convolutional layers, and a final softmax layer for classification (see figure). The authors use a combination of different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalisation. In the TempCNN reference paper [@Pelletier2019], the authors compare favorably their model with the Recurrent Neural Network proposed by Russwurm and Körner [@Russwurm2018] for land use classification. The figure below shows the architecture of the TempCNN model.

```{r, echo = FALSE, fig.align="center", out.width = "100%", fig.cap="Structure of tempCNN architecture (source: Pelletier et al.(2019))"}

knitr::include_graphics("images/tempcnn.png")
```

The function `sits_tempcnn()` implements the model. The parameter `cnn_layers` controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter `cnn_kernels` indicates the size of the convolution kernels; the default are kernels of size 7. Activation for all 1D-CNN layers uses the "relu" function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter `cnn_dropout_rates`. The `validation_split` controls the size of the test set, relative to the full data set. We recommend to set aside at least 20% of the samples for validation.

```{r, echo = FALSE}
set.seed(290356)
```


```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of TempCNN model."}
library(torchopt)
# train using tempCNN
tempcnn_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_tempcnn(
                          optimizer            = torchopt::optim_adamw,
                          cnn_layers           = c(128, 128, 128),
                          cnn_kernels          = c(7, 7, 7),
                          cnn_dropout_rates    = c(0.2, 0.2, 0.2),
                          epochs               = 100,
                          batch_size           = 64,
                          validation_split     = 0.2,
                          verbose              = FALSE) )

# show training evolution
plot(tempcnn_model)
```

Then, we classify a 16-year time series using the TempCNN model.

```{r, out.width = "90%", tidy="styler", fig.align="center", fig.cap="Classification of time series using TempCNN."}
# Classify using TempCNN model and plot the result
class <- point_mt_6bands %>% 
    sits_select(bands = c("NDVI", "EVI", "NIR", "MIR")) %>% 
    sits_classify(tempcnn_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

The result has important differences from the previous ones. The TempCNN model indicates the "Soy_Cotton" class as the most likely one in 2004. While this result is likely to be wrong, it shows that the time series for year 2004 is different from those of "Forest" and "Pasture" classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, users could consider improving the training data by including samples that represent forest degradation. In our experience, TempCNN models are a reliable way for classifying image time series [@Simoes2021]. Recent work which compares different models also provides evidence of that TempCNN models have satisfactory behavior, especially in the case of crop classes [@Russwurm2020].

## Residual 1D CNN Networks (ResNet){-}

A residual 1D CNN network, also known as ResNet, is an extension of the standard 1D CNN architecture, with the addition of residual connections between the layers. Residual connections allow the network to learn residual mappings, which are the difference between the input and output of a layer. By adding these residual connections, the network can learn to bypass certain layers and still capture important features in the data.

The Residual Network (ResNet) for time series classification was proposed by Wang et al. [@Wang2017], based on the idea of deep residual networks for 2D image recognition [@He2016]. The ResNet architecture is composed of 11 layers, with three blocks of three 1D CNN layers each (see figure below). Each block corresponds to a 1D CNN architecture. The output of each block is combined with a shortcut that links its output to its input, called a "skip connection". The purpose of combining the input layer of each block with its output layer (after the convolutions) is to avoid the so-called "vanishing gradient problem". This issue occurs in deep networks as he neural network's weights are updated based on the partial derivative of the error function. If the gradient is too small, the weights will not be updated, stopping the training[@Hochreiter1998]. Skip connections aim to avoid vanishing gradients from occurring, allowing deep networks to be trained.

```{r, echo = FALSE, out.width = "100%", fig.align="center", fig.cap="Structure of ResNet architecture (source: Wang et al.(2017))."}
knitr::include_graphics("images/resnet.png")
```

In `sits`, the Residual Network is implemented using the `sits_resnet()` function. The default parameters are those proposed by Wang et al. [@Wang2017], as implemented by Fawaz et al.[@Fawaz2019]. The first parameter is `blocks`, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128 filters. Users can control the number of blocks and filter size by changing this parameter. The parameter `kernels` controls the size the of kernels of the three layers inside each block. We have found out that it is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is "relu", which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is `optim_adamw`, available in package `torchopt`.

```{r, tidy="styler", out.width = "100%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of ResNet model."}
# train using ResNet
resnet_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_resnet(
                          blocks               = c(64, 128, 128),
                          kernels              = c(7, 5, 3),
                          epochs               = 100,
                          batch_size           = 64,
                          validation_split     = 0.2,
                          verbose              = FALSE) )
# show training evolution
plot(resnet_model)
```

Then, we classify a 16-year time series using the ResNet model. The behaviour of the ResNet model is similar to that of TempCNN, with more variability.

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using ResNet."}
# Classify using DL model and plot the result
class <- point_mt_6bands %>% 
    sits_select(bands = c("NDVI", "EVI", "NIR", "MIR")) %>% 
    sits_classify(tempcnn_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```
 
## Attention-based models{-}

Attention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of an input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.

The basic idea behind attention-based models is to allow the model to selectively focus on different parts of the input at different times. This can be done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model can then use them to compute a weighted sum of the input. The results capture the model's attention on specific parts of the input.

Attention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that in applications such as language translation not all inputs are alike. Consider the English sentence "Look at all the lonely people". A good translation system needs to relate the words "look" and "people" as the key parts of this sentence and to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences [@Vaswani2017]. 

The basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms the input textual values into numerical vectors, and a decoder that processes these vectors and provides suitable answers. The difference is on how the values are handled internally. In multilayer perceptrons (MLP), all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most relevant combination of inputs that produces the best output. In convolutional nets (CNN), input values that are close in time (1D) or space (2D) are combined to produce higher-level information that helps to distinguish the different components of the input data. For text recognition, the initial choice of deep learning studies was to use recurrent neural networks (RNN) that handle input sequences sequentially. However, neither MLPs, nor CNNs or RNNs have been able to capture the structure of complex inputs such as natural language. The success of transformer-based solutions accounts for substantial improvements in natural language processing.

The two main differences between transformer models and other algorithms are the use of positional encoding and self-attention. Positional encoding assigns an index to each input value which ensures that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention works by comparing every word in the sentence to every other word in the sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT [@Devlin2019] and GPT-3 [@Brown2020].

The application of attention-based models for satellite image time series analysis is proposed by Garnot et [@Garnot2020a] and Russwurm and Körner [@Russwurm2020]. A self-attention network can learn to focus on specific time steps and image features that are most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use the observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In case of deforestation, the algorithm tries to identify the dates where the forest is being cut. Attention-based models are a means to indentify events that characterize each land use and land cover class.

The first model proposed by Garnot and co-authors is a full transformer-based model [@Garnot2020a]. Considering that image time series classification is a easier task that natural language processing, Garnot et al [@Garnot2020b] also propose a simplified version of the full transformer model. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of result. 

In `sits`, the full transformer-based model proposed by Garnot and co-authors [@Garnot2020a]   is implemented using the `sits_tae()` function. The default parameters are those proposed by the authors. The default optimizer is the same is `optim_adamw`, available in package `torchopt`.

```{r, tidy="styler", out.width = "100%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of Temporal Self-Attention model."}
# train a machine learning model using TAE
tae_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_tae(
                          epochs               = 150,
                          batch_size           = 64,
                          optimizer            = torchopt::optim_adamw,
                          validation_split     = 0.2,
                          verbose              = FALSE) )
# show training evolution
plot(tae_model)
```

Then, we classify a 16-year time series using the TAE model. 

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using TAE."}
# Classify using DL model and plot the result
class <- point_mt_6bands %>% 
    sits_select(bands = c("NDVI", "EVI", "NIR", "MIR")) %>% 
    sits_classify(tae_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```

Garnot and co-authors [@Garnot2020a] also proposed the Lightweight Temporal Self-Attention Encoder.  the lightweight self-attention encoder model is that it can achieve high classification accuracy with fewer parameters compared to other neural network models. This makes it a good choice for applications where computational resources are limited. The `sits_lighttae()` function implements this algorithm. The default optimizer is `optim_adamw`, available in package `torchopt`. The most important parameter to be set is the learning rate `lr`. Values ranging from 0.001 to 0.005 should produce reasonable results. See also the section below on model tuning. 

```{r, tidy="styler", out.width = "80%", warning = FALSE, message = FALSE, fig.align="center", fig.cap="Training evolution of Lightweight Temporal Self-Attention model."}
# train a machine learning model using TAE
ltae_model <- sits_train(samples_matogrosso_mod13q1, 
                       sits_lighttae(
                          epochs               = 150,
                          batch_size           = 64,
                          optimizer            = torchopt::optim_adamw,
                          opt_hparams = list(lr = 0.001),
                          validation_split     = 0.2) )
# show training evolution
plot(ltae_model)
```

Then, we classify a 16-year time series using the LightTAE model. 

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of time series using LightTAE."}
# Classify using DL model and plot the result
class <- point_mt_6bands %>% 
    sits_select(bands = c("NDVI", "EVI", "NIR", "MIR")) %>% 
    sits_classify(ltae_model) %>% 
    plot(bands = c("NDVI", "EVI"))
```


The behaviour of both `sits_tae()` and `sits_lighttae()` is similar to that of `sits_tempcnn()`. It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated to the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty, since the classifiers do not agree on the resulting classes. In general, the deep learning temporal-aware models are more sensitive to class variability than random forests and extreme gradient boosters. 

## Model tuning{-}

Deep learning model tuning is the process of selecting the best set of hyperparameters for a specific application. Model tuning enables a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training, but instead are set prior to training and affect the behavior of the model during training. Examples of include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.

Deep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters and selecting the set that gives the best performance.

Deep learning algorithms try to find the optimal point that represents the best value of the prediction function that, given an input $X$ of data points, predicts the result $Y$. In our case, $X$ is a multidimensional time series and $Y$ is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search [@Bengio2012]. All gradient descent methods use an optimization algorithm that is adjusted with hyperparameters such as the learning rate and regularization rate [@Schmidt2021]. The learning rate controls the numerical step of the gradient descent function and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires the use of model tuning methods. 

To reduce the learning curve, `sits` provides default values for all machine learning and deep learning methods, which ensure a reasonable baseline performance. More experienced users may want to refine model hyperparameters, especially for more complex models such as `sits_lighttae()` or `sits_tempcnn()`. To that end, the package provides the `sits_tuning()` function. 

The simplest approach to model tuning is would be to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinational explosion and thus is not recommended. Instead, Bergstra and Bengio [@Bergstra2012] propose to use randomly chosen trials. In their paper, the authors show that random trials are more efficient than grid search trials, allowing the selection of adequate hyperparameters at a fraction of the computational cost. The `sits_tuning()` function follows Bergstra and Bengio [@Bergstra2012] and uses a random search on the chosen hyperparameters.

Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [@Bottou2018]. A large number of optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [@Schmidt2021]. For general deep learning applications, the Adam optimizer [@Kingma2017] provides a good baseline and reliable performance. For this reason, Adam  is the default optimizer in the R `torch` package. Experiments with image time series show that other optimizers may have better performance for the specific problem of land use classification. For this reason, the authors developed the  `torchopt` R package which includes a number of recently proposed optimizers, including Adamw  [@Loshchilov2019], Madgrad [@Defazio2021] and Yogi [@Zaheer2018]. Based on our experiments, we have selected Adamw as the default optimizer for deep learning methods. Using the `sits_tuning()` function allows testing these and other optimizers available in `torch` and `torch_opt` packages.

The `sits_tuning()` function takes the following parameters:

1. `samples` - Training data set to be used by the model.
2. `samples_validation` (optional) - If available, this data set contains time series to be used for validation. If missing, the next parameter will be used.
3. `validation_split` - If `samples_validation` is not used, this parameter defines the proportion of time series in the training data set to be used for validation (default is 20%).
4. `ml_method()`  - Deep learning method (either `sits_mlp()`, `sits_tempcnn()`, `sits_resnet()`, `sits_tae()` or `sits_lighttae()`)
5. `params` - defines the optimizer and its hyperparameters by calling the `sits_tuning_hparams()` function, as shown in the example below. 
6. `trials` - number of trials to run the random search.
7. `multicores` - number of cores to be used for the procedure.
8. `progress` - show progress bar?

The `sits_tuning_hparams()` function inside `sits_tuning()` allows users to define optimizers and their hyperparameters including `lr` (learning rate), `eps` (controls numerical stability) and `weight_decay` (controls overfitting). The default values for `eps` and `weight_decay` in all `sits` deep learning functions are 1.0e-08  and 1.0e-06, respectively. The default `lr` for `sits_lighttae()` and `sits_tempcnn()` is  0.005, and for `sits_tae()` and `sits_resnet()` is 0.001. Users have different ways to randomize the hyperparameters, including: `choice()` (a list of options), `uniform` (a uniform distribution), `randint` (random integers from a uniform distribution), `normal(mean, sd)` (normal distribution), `beta(shape1, shape2)`(beta distribution). These options allow extensive combination of hyperparameters.

In the example, the `sits_tuning()` function finds good hyperparameters to train the `sits_lighttae()` method for the Mato Grosso data set. It tests 100 combinations of learning rate and weight decay for the Adamw optimizer. To randomize the learning rate, it uses a beta distribution with parameters 0.35 and 10, which allows for variation between about 0.2 and 1.0e-00; for the weight decay, the beta distribution with parameters 0.1 and 2 generates values roughly between 1 and 1.0e-24. 

```{r, tidy="styler", eval = FALSE, echo = TRUE}
tuned <- sits_tuning(
     samples = samples_matogrosso_mod13q1,
     ml_method = sits_lighttae(),
     params = sits_tuning_hparams(
         optimizer = torchopt::optim_adamw,
         opt_hparams = list(
             lr = beta(0.35, 10),
             weight_decay = beta(0.1, 2)
         )
     ),
     trials = 100,
     multicores = 6,
     progress = FALSE
)
```

```{r, eval = TRUE, echo = FALSE}
tuned <- readRDS("./etc/tuned.rds")

```

The result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The 10 best results obtain accuracy values between 0.976 and 0.958, as shown below. The best result is obtained by a learning rate of 0.0011 and an weight decay of 2.14e-05, 

```{r} 
# obtain accuracy, kappa, lr and weight decay for the 10 best results
# hyperparameters are organized as a list
hparams_10 <- tuned[1:10,]$opt_hparams
# extract learning rate and weight decay from the list
lr_10 <- purrr::map_dbl(hparams_10, function(h) h$lr)
wd_10 <- purrr::map_dbl(hparams_10, function(h) h$weight_decay)

# create a tibble to display the results
best_10 <- tibble::tibble(
    accuracy = tuned[1:10,]$accuracy,
    kappa = tuned[1:10,]$kappa,
    lr    = lr_10,
    weight_decay = wd_10
)
# print the best combination of hyperparameters
best_10
```

For large data sets, the tuning process is time consuming. Despite this cost, it is recommended for achieving the best performance. In general, tuning hyperparameters for models such as `sits_tempcnn()` and `sits_lighttae()` will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer's and user's accuracies are possible. In cases where one wants to detect change in less frequent classes, tuning can make a difference in the results. 


## Considerations on model choice{-}

The development of machine learning methods for classification of satellite image time series is an ongoing task. There is a lot of recent work using methods such as convolutional neural networks [@Pelletier2019, @Wang2017, @Fawaz2020] and temporal self-attention [@Garnot2020a]. Given the rapid evolution of the field with new methods still being developed, there are few references that offer a comparison between different machine learning methods. Most works on the literature [@Wang2017, @Fawaz2019] compare methods for generic time series classification. Their insights are not directly applicable for satellite image time series data, which have different properties than the time series using in applications such as economics and health.

In the specific case of satellite image time series, Russwurm et al. [@Russwurm2020] present a comparative study between seven deep neural networks for classification of agricultural crops, using random forests (RF) as a baseline. The data is composed of Sentinel-2 images over Britanny, France. Their results indicate a slight difference between the best model (attention-based transformer model) over TempCNN, ResNet and RF. Attention-based models obtain accuracy ranging from 80-81%, TempCNN get 78-80%, and RF gets 78%. Based on this result and also on the authors' experience, we make the following recommendations:

1.  Random forests provide a good baseline for image time series classification and should be included in users' assessments. 

2.  XGBoost is an worthy alternative to Random forests. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.

3.  TempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.

4.  Attention-based models (TAE and LightTAE) can achieve the best overall performance, in case of well designed and balanced training sets and with hyperparameter tuning. 

5. The best means of improving classification performance is to provide an accurate and reliable training data set. Each class should have enough samples to account for spatial and temporal variability. 

<!--chapter:end:08-machinelearning.Rmd-->

```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/chp9")
```

# Image Classification in Data Cubes{-}


<a href="https://www.kaggle.com/esensing/raster-classification-in-sits" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a>

In this chapter, we discuss how to classify data cubes by providing a step-by-step example. Our study area is the state of Rondonia, Brazil, that underwent substantial deforestation in the last decades. The objective of the case study is to detect deforested areas. 

## Training the classification model{-}

The case study uses the training data set `samples_prodes_4bands`, available in package `sitsdata`. This data set consists of 480 samples collected from Sentinel-2 images covering the state of Rondonia. The samples are intended to detect deforestation events, and include four classes: "Forest", "Burned_Area",   "Cleared_Area", and "Highly_Degraded". The time series cover a set of 29 dates with a period of 16 days, ranging from "2020-06-04" to "2021-08-26". The data has 12 attributes, including original bands ("B02", "B03", "B04", "B05", "B08", "B8A", "B11", and "B12") and indices ("NDVI", "EVI" and "NBR").

```{r, tidy = "styler"}
library(sitsdata)
# obtain the samples 
data("samples_prodes_4classes")
# show the contents of the samples
sits_labels_summary(samples_prodes_4classes)
```

To better understand the training set, its is useful to plot the basic patterns associated to the samples. The function `sits_patterns()` uses a generalized additive model (GAM) to predict a smooth, idealized approximation to the time series associated to the each label, for all bands. Since the data cube used in the classification has only three bands ("B02", "B8A", and "B11"), we filter the samples for these bands before showing the patterns. 

```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Patterns associated to the training samples"}
samples_3bands <- sits_select(
    data = samples_prodes_4classes,
    bands = c("B02", "B8A", "B11")
)
plot(sits_patterns(samples_3bands))
```

The patterns show different temporal responses for the selected classes. They match the typical behavior of deforestation in the Amazon. First, the forest is cut at the start of the dry season (June/July). At the end of the dry season, some clear-cut areas are burned to clean the remains; this action is reflected in the steep fall of the response of "B8A" values of burned area samples after July. In areas that are cleared but not burned, response in the middle infra-red band "B11" increases significantly at the end of the dry season, while "B8A" values remain high. This is a sign of mixed pixels which combine forest remains with bare soil. Forest areas show a constant spectral response during the year. Degraded areas show an increase in values of middle infra-red band "B11" compared to native forests, showing a mixed response of vegetation and soil.


## Building a data cube{-}

We now build a data cube from the Sentinel-2 images available in the package `sitsdata`. These images were downloaded from the `SENTINEL-2-L2A` collection in Microsoft Planetary Computer (`MPC`). We have chosen bands "BO2", "B8A" and "B11" images in a small area of 1000 x 1000 pixels the state of Rondonia. As explained in the "Earth observation data cubes" chapter, we need to inform `sits` how to parse these file names to obtain tile, date and band information. Image files are named according to the convention "cube_tile_band_date" (e.g., `cube_20LKP_BO2_2020_06_04.tif`).


```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Color composite image of the cube for date 2021-07-25"}
# files are available in a local directory 
data_dir <- system.file("extdata/Rondonia-20LKP/", package = "sitsdata")
# read data cube
ro_cube_20LKP <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c('X1', "tile", "band", "date")
)

# plot the cube
plot(ro_cube_20LKP, dates = "2021-07-25", red = "B11", green = "B8A", blue = "B02")
```

## Training a deep learning model{-}

The next step is to train a Lightweigth Temporal Attentiona Enconder model model, using the `adamw` optimizer and a learning rate of 0.001. Since the data cube to be classified has bands `BO2`, `B8A` and `B11`,  we select these bands from the training data.

```{r, tidy = "styler", out.width = "80%", fig.align="center", fig.cap="Training evolution of LightTAE model."}
# use only the bands available in the cube
samples_3bands <- sits_select(
    data = samples_prodes_4classes, 
    bands = sits_bands(ro_cube_20LKP)
)

# train model using LightTAE algorithm
ltae_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_lighttae(
        opt_hparams = list(lr = 0.001)
        )
)
# plot the evolution of the model
plot(ltae_model)
```


## Classification using parallel processing{-}

To classify both data cubes and sets of time series, use the function `sits_classify()`, which uses parallel processing for speed up performance, as described in the end of this Chapter. Its most relevant parameters are: (a) `data`, either a data cube or a set of time series; (b) `ml_model`, a trained model using one of the machine learning methods provided; (c) `multicores`, number of CPU cores that will be used for processing; (d) `memsize`, memory available for classification; (e) `output_dir`, directory where results will be stored; (f) `version`, for version control. If users want to follow the processing steps, they should turn on the parameters `verbose` to print information and `progress` to get a progress bar. The result of the classification is a data cube with a set of probability layers, one for each output class. Each probability layer contains the model's assessment of how likely is each pixel to belong to the related class. The probability cube can be visualized with `plot()`. 

```{r, tidy = "styler", out.width = "80%", fig.align="center", fig.cap="Probability maps produced by LightTAE model."}

# classify data cube
ro_cube_20LKP_probs <- sits_classify(
    data     = ro_cube_20LKP,
    ml_model = ltae_model,
    output_dir = "./tempdir/chp9",
    version = "ltae",
    multicores = 4,
    memsize = 12
)
plot(ro_cube_20LKP_probs, palette = "YlGn")
```

The probability cube is a useful tool for data analysis. It is used for post-processing smoothing, as described in this Chapter, but also in uncertainty estimates and active learning, as described in the "Uncertainty and Active Learning" Chapter.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Final classification map."}
# generate thematic map
defor_map <- sits_label_classification(
    cube = ro_cube_20LKP_probs,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp9",
    version = "no_smooth"
)
plot(defor_map)
```

The labelled map generated from the pixel-based time series classification method exhibits a number of misclassified pixels, which are depicted as small patches that appear surrounded by a different class. This occurrence of outliers is a common issue that arises due to the inherent nature of this classification approach. Mixed pixels are prevalent in images, regardless of their resolution, and each class exhibits a considerable degree of data variability. As a result, these factors can lead to outliers that have a higher probability of being misclassified. To overcome this limitation, `sits` employs post-processing smoothing techniques that leverage the spatial context of the probability cubes to refine the results. These techniques will be discussed in the next chapter.

## Map Reclassification{-}

Reclassification of a remote sensing map refers to the process of changing the classes assigned to different pixels in the image. The purpose of reclassification is to modify the information contained in the image to better suit a specific use case. In `sits`, reclassification involves assigning new class labels to pixels based on additional information provided a reference map. Based on the labels of the classification and the reference map, the users defines rules based on the desired outcome. These rules are then applied to the classified map. The result is a new map with updated labels.

To illustrate the reclassification operation in `sits`, we take a classified data cube, stored in the `sitsdata` package. As discussed in the "Earth observation data cubes" chapter, `sits` can create a data cube from a classified image file. Users need to provide the original data source and collection, the directory where  data is stored (`data_dir`), the information on how to retrieve data cube parameters from file names (`parse_info`), and the labels associated to the classified image. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="original classification map."}
# Open classification map
data_dir <- system.file("extdata/Rondonia-Class", package = "sitsdata")
ro_class <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c("X1", "X2", "tile", "start_date", "end_date",
                   "band", "version"),
    bands = "class",
    labels = c("Water", "ClearCut_Burn", "ClearCut_Soil",
               "ClearCut_Veg", "Forest", "Bare_Soil", "Wetland")
)
plot(ro_class)
```

The above map shows the total extent of deforestation by clear cuts estimated by the `sits` random forest algorithm in an area in Rondonia, Brasil, based on an time series of Sentinel-2 images for the period `2020-06-04` to `2021-08-26`. Suppose we want to estimate the deforestation that ocurred period from June 2020 to August 2021. We need a reference map that contains information on forest cuts prior to 2020. 

In this example, we the PRODES deforestation map of Amazonia created by Brazil's National Institute for Space Research (INPE) as our refence. This map is produced by visual interpretation. PRODES measures deforestation on an yearly basis, starting from August of one year to July of the following year. The contains classes that represent the natural world ("Forest", "Water", "NonForest", and  "NonForest2") and classes that capture the yearly deforestation increments. These classes are labelled "dYYYY" and "rYYYY"; the first label refers to deforestation on a given year (e.g., "d2008" for deforestation for August 2007 to July 2008); the second to places where the satellite data is not sufficient to determine the land cover (e.g, "r2010" for 2010). This map is available on package "sitsdata", as shown below.


```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Deforestation map produced by sits."}
data_dir <- system.file("extdata/PRODES", package = "sitsdata")
prodes2021 <- sits_cube(
    source = "USGS",
    collection = "LANDSAT-C2L2-SR",
    data_dir = data_dir,
    parse_info = c("X1", "X2", "tile", "start_date", "end_date",
                   "band", "version"),
    bands = "class",
    version = "v20220606",
    labels = c("Forest", "Water", "NonForest",
               "NonForest2", "NoClass", "d2007", "d2008",
               "d2009", "d2010", "d2011", "d2012",
               "d2013", "d2014", "d2015", "d2016",
               "d2017", "d2018", "r2010", "r2011",
               "r2012", "r2013", "r2014", "r2015",
               "r2016", "r2017", "r2018", "d2019",
               "r2019", "d2020", "NoClass", "r2020",
               "Clouds2021", "d2021", "r2021")
)

```
Since the labels of the deforestation map are specialized and are not part of the default `sits` color table, we define a legend for better visualisation of the different deforestation classes. Using this new legend, we can plot the PRODES deforestation map.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Deforestation map produced by PRODES."}

# use the RColorBrewer pallete "YlOrBr" for the deforestation years
colors <- grDevices::hcl.colors(n = 15, palette = "YlOrBr")
# define the legend for the deforestation map
def_legend <- c(
    "Forest" = "forestgreen", "Water" = "dodgerblue3", 
    "NonForest" = "bisque2", "NonForest2" = "bisque2",
    "d2007" = colors[1],  "d2008" = colors[2],
    "d2009" = colors[3],  "d2010" = colors[4], 
    "d2011" = colors[5],  "d2012" = colors[6],
    "d2013" = colors[7],  "d2014" = colors[8],
    "d2015" = colors[9],  "d2016" = colors[10],
    "d2017" = colors[11], "d2018" = colors[12],
    "d2019" = colors[13], "d2020" = colors[14], 
    "d2021" = colors[15], "r2010" = "azure2",
    "r2011" = "azure2",   "r2012" = "azure2",
    "r2013" = "azure2",   "r2014" = "azure2",
    "r2015" = "azure2",   "r2016" = "azure2",
    "r2017" = "azure2",   "r2018" = "azure2",
    "r2019" = "azure2",   "r2020" = "azure2",
    "r2021" = "azure2",   "NoClass" = "grey90",
    "Clouds2021" = "grey90"
    )
plot(prodes2021, legend = def_legend)
```

Taking the PRODES map as our refence, we can include new labels in the classified map produced by `sits` using `sits_reclassify()`. The new label "Defor_2020" will be applied to all pixels that PRODES considers that have been deforested prior to July 2020. We also include a new label "Non_Forest" to include all pixels that PRODES takes as not covered by native vegetation, such as wetlands and rocky areas. The PRODES classes will be used as a mask over the `sits` deforestation map.

The `sits_reclassify()` operation requires the parameters: (a) `cube`, the classified data cube whose pixels will be reclassified; (b) `mask`, the reference data cube used as a mask; (c) `rules`, a named list. The names of the `rules` list will be the new labels of the classified cube.  Each new label is associated to a `mask` vector that includes the labels of the reference map that will be joined. `sits_reclassify()` then compares the original and reference map pixel by pixel. For each pixel of the reference map whose labels in one of the `rules`, the algorithm relabels the original map. The result will be a reclassified map that has the original labels plus the new labels that have been masked using the reference map.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Deforestation map by sits masked by PRODES map."}
# Reclassify cube
ro_def_202021 <- sits_reclassify(
    cube = ro_class,
    mask = prodes2021,
    rules = list(
        "Deforestation_Mask" = mask %in% c(
            "d2007", "d2008", "d2009",
            "d2010", "d2011", "d2012",
            "d2013", "d2014", "d2015",
            "d2016", "d2017", "d2018",
            "d2019", "d2020",
            "r2010", "r2011", "r2012",
            "r2013", "r2014", "r2015",
            "r2016", "r2017", "r2018",
            "r2019", "r2020", "r2021"
        ),
        "Water" = mask == "Water",
        "Non_Forest" = mask %in% c("NonForest", "NonForest2")
    ),
    memsize = 8,
    multicores = 2,
    output_dir = "./tempdir/chp9",
    version = "defor-reclass"
)
# plot the reclassified map
plot(ro_def_2021)
```
The reclassified map has been split into deforestation prior to mid-2020 (using the PRODES map) and the areas classified by `sits` that are taken as being deforested on the period mid-2020 to mid-2021. This allows the experts to measure how much deforestation occurred in this period according to `sits` and compare the result with that of the PRODES visual interpretation map. 

The `sits_reclassify()` operation is not restricted to the comparison between deforestation maps. It can be used in any case that requires masking of a result based on a reference map. 

## How parallel processing works{-}

This section provides an overview of how the functions `sits_classify()`, `sits_smooth()` and `sits_label_classification()` process images in parallel. To achieve efficiency, `sits` implements a fault tolerant multitasking procedure for big EO data classification. Users are not burdened with the need to learn how to do multiprocessing. Thus, their learning curve is shortened. Image classification in `sits` is done by a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum. 

The classification procedure benefits from the fact that most images available in cloud collections are stored as COGs (cloud-optimized Geotiff). COGs are a regular GeoTIFF files organized in regular square blocks to improve visualization and access for large data sets. Thus, data requests can be optimized to access only portions of the images. All cloud services supported by `sits` use COG files. The classification algorithm in `sits` uses COGs to ensure optimal data access, reducing I/O demand as much as possible.

The approach for parallel processing in `sits`, depicted in the figure below, has the following steps:

1. Based on the block size of individual COG files, calculate the size of each chunk that has to be loaded in memory, considering the number of bands and the length of the timeline. Chunk access is optimized for efficient transfer of data blocks.
2. Divide the total memory available by the chunk size to find out how many processes can be run in parallel. 
3. Each core processes a chunk and produces a subset of the result.
4. Repeat the process until all chunks in the cube have been processed.
5. Check that subimages have been produced correctly. If there is a problem with one or more subimages, run a failure recovery procedure to ensure all data is processed.
6. After all subimages are generated, join them to obtain the result.

```{r, out.width = "90%", out.height = "90%", echo = FALSE, fig.align="center", fig.cap="Parallel processing in sits (source: Simoes et al.,2021)."}

knitr::include_graphics("images/sits_parallel.png")
```

This approach has many advantages. It works in any virtual machine that supports R and has no dependencies on proprietary software. Processing is done in a concurrent and independent way, with no communication between workers. Failure of one worker does not cause failure of the big data processing. The software is prepared to resume classification processing from the last processed chunk, preventing against failures such as memory exhaustion, power supply interruption, or network breakdown. 

To reduce processing time, it is necessary to adjust `sits_classify()`, `sits_smooth()`, and `sits_label_classification()`  according to the capabilities of the host environment. The `memsize` parameter controls the size of the main memory (in GBytes) to be used for classification. A practical approach is to set `memsize` to the maximum memory available in the virtual machine for classification and to chose `multicores` as the largest number of cores available. Based on the memory available and the size of blocks in COG files, `sits` will access the images in an optimized way. In this way, `sits` tries to ensure best possible use of the available resources. 





<!--chapter:end:09-rasterclassification.Rmd-->

```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/ch10")
```

# Bayesian smoothing{-}


## Motivation{-}

The `sits` package uses a *time-first, space-later* approach. Since machine learning classifiers in `sits` are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land-cover classification by incorporating spatial and contextual information into the classification process. Smoothing methods use the neighborhood data to remove outliers and enhance consistency in the resulting product. The smoothing method available in `sits` uses Bayesian inference. 

Bayesian inference is a way of updating our uncertainty in the light of new evidence. It allows the inclusion of expert knowledge. Bayesian smoothing works by combining two elements: (a) our prior belief on class probabilities; (b) the estimated probabilities for a given pixel. To estimate the prior distribution to the class probabilities for each pixel, we use the values for a local neighborhood. The assumption is that, at local level, class probabilities should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels including spatial dependence.  

## Post-processing estimation{-}

To calculate the Bayesian estimate, the probability values $p_{i,k}$  for all pixels $i$ of all classes $k$ are converted to log-odds values using the logit function; this function converts probability values from 0 to 1 to values from negative infinity to infinity. 


$$
    x_{i,k} = \ln \left(\frac{p_{i,k}}{1 - p_{i,k}}\right)
$$
This conversion is used to approximate a Gaussian distribution for the log-odds values. Given a logit value $x_{i,k}$ for pixel $i$ and class $k$, the Bayesian estimate ${E}[\mu_{i,k} | x_{i,k}]$ of the expected logit value $\mu_{i,k}$ conditioned by $x_{i,k}$ is given by: 
    
$$
    {E}[\mu_{i,k} | x_{i,k}] =
    \Biggl [ \frac{s^2_{i,k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times
x_{i,k} +
    \Biggl [ \frac{\sigma^2_{k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times m_{i,k} 
$$
where

1. $x_{i,k}$ is the logit value for pixel $i$ and class $k$.
2. $m_{i,k}$ is the average of logit values for pixels of class $k$ in the neighborhood of pixel $i$.
3. $s^2_{i,k}$ is the variance of logit values for pixels of class $k$ in the neighborhood of pixel $i$.
4. $sigma^2_{k}$ is the prior variance of the logit values for class $k$.

The values of $x_{i,k}$ are obtained directly from the probability cubes. Given a neighborhood defined by the user, the values of $m_{i,k}$ and $s^2_{i,k}$ are also calculated from the probability cubes. The value $sigma^2_{k}$ is set by the user, based on her knowledge of the data variability. Values of the prior variance $sigma^2_{k}$ which are small relative to the local variance $s^2_{i,k}$ increase our confidence in the original probabilities. Conversely, values of the prior variance $sigma^2_{k}$ which are big relative to the local variance $s^2_{i,k}$ increase our confidence in the average probability of the neighborhood. 

Consider the following two-class example. Take a pixel with probability 0.4 (logit $x_{i,1}$ = -0.4054) for class A, and probability 0.6 (logit $x_{i,2}$ = 0.4054) for class B. Thus, without post-processing, the pixel will be labelled as class B.

Consider that the local average is 0.6 (logit $m_{i,1}$ = 0.4054) for class A and 0.4 (logit $m_{i,2}$ = -0.4054) for class B. This is a case of an outlier classified originally as class B in the midst of a set of pixels of class A. Take the local variance of logits to be $s^2_{i,1}$ = 5 for class A and $s^2_{i,2}$ = 10 and for class B. This difference is to be expected if the local variability of class A is smaller than that of class B. 

To complete the estimate, we need to set the parameter $sigma^2_{k}$, which represents our prior belief in the variability of the probability values for each class. If we take both $sigma^2_{A}$ for class A and $sigma^2_{B}$ for class B to be both 10, the Bayesian estimated probability for class A is 0.52  and for class B is 0.48. In this case, the pixel will be relabeled as being of class A. However, if our belief in the original values is higher, we will get a different result. If we set a value of $sigma^2$ to be 5 for both classes A and B, the Bayesian probability estimate will be 0.48 for class A and will be 0.52 for class B. In this case, the original label will be kept. 

## Setting the smoothness parameter {-}

To compute the Bayesian estimate, we need to include our a priori belief in the variability of classes, expressed in the parameter $sigma^2_{k}$. This parameter expresses our confidence on the inherent variability of the distribution of values of a class $k$. The smaller the parameter $sigma^2_{k}$, the more we trust the estimated probability values produced by the classifier for class $k$. Conversely, higher values of $sigma^2_{k}$ indicate lower confidence on the outputs of the classifier and improved confidence on the values of the local average.

We make the following recommendations for setting the $sigma^2_{k}$ parameter:
    
1. Set the $sigma^2_{k}$ parameter with high values (20 or above) to increase the neighborhood influence compared with the probability values for each pixel. Classes whose probabilities have strong spatial autocorrelation will tend to replace outliers of different classes.

2. Set the $sigma^2_{k}$ parameter with low values (5 or below) to reduce the neighborhood influence compared with the probabilities for each pixel of class $k$. In this way, classes which have low spatial autocorrelation are more likely not to be relabeled.

Consider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests, without many outliers inside them, she would set the $sigma^2$ parameter for the class "Forest" to be high. For comparison, one usually wants to avoid that small watersheds which have few similar neighbors be relabeled. In this case, it is advisable to avoid a strong influence of the neighbors; the $sigma^2$ should be set as low as possible. 

## Defining the neighborhood{-}

The intuition for Bayesian smoothing is that homogeneous neighborhoods should have the same class. In homogeneous neighborhoods, the dominant class has both higher average probabilities and lower variance than the other classes. In these neighborhoods, a pixel of a different class is likely to be associated to lower average probabilities and higher local variance. 

One expected consequence of Bayesian smoothing is to improve the borders between the objects created by the classification. In pixel-based classification, mixed pixels at the limits between areas with different classes pose a problem for classification. These pixels contain signatures of two classes. To account for these cases, Bayesian smoothing in `sits` uses a special definition of a neighborhood. Each pixel in a probability map of a class is associated to a neighborhood with a proportion of the local window. The pixels in this neighborhood which are used to compute the Bayesian statistics are those having the highest probability of belonging to the class. 

## Measuring the local variance{-}

To illustrate the impact of neighborhood definition and choices of the $sigma^2_{k}$ parameter, we present a detailed example. The first step is to take a probability cube for a deforestation detection application in an area of the Brazilian Amazon. This cube has been produced by a random forest model with 6 classes. We first build the data cube.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Probability map produced for class Forest."}
# define the classes of the probability cube
labels <- c("Water", "ClearCut_Burn", "ClearCut_Soil",
            "ClearCut_Veg", "Forest", "Wetland")
# directory where the data is stored 
data_dir <- system.file("extdata/Rondonia-20LLQ/", package = "sitsdata")
# create a probability data cube from a file 
probs_cube <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    bands = "probs",
    labels = labels,
    parse_info = c("X1", "X2", "tile", "start_date", "end_date", "band", "version")
)
plot(probs_cube, labels = "Forest")

```
In this example, there are both compact forest patches as well other linear stretches mostly associated with riparian areas. In order to preserve the linear patches 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Probability map for class  for class ClearCut_Burn."}
plot(probs_cube, labels = "ClearCut_Burn")
```


```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Variance map for class Forest."}
var_cube <- sits_variance(
    cube = probs_cube,
    window_size = 9,
    neigh_fraction = 0.5,
    multicores = 4,
    memsize = 24,
    output_dir = data_dir,
    version = "w9-n05"
)
plot(var_cube, labels = "Forest")
```
```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Histogram of variances"}
plot(var_cube, type = "hist")
```


```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Variance map for class Forest."}
var_cube_2 <- sits_variance(
    cube = probs_cube,
    window_size = 5,
    neigh_fraction = 1.0,
    multicores = 4,
    memsize = 24,
    output_dir = data_dir,
    version = "w7-n1"
)
plot(var_cube_2, labels = "Forest")
```

smoothness <- c(5, 40, 40, 40, 5, 15)

bayes_cube <- sits_smooth(
    cube = probs_cube,
    window_size = 9,
    smoothness = 20,
    neigh_fraction = 0.5,
    multicores = 1,
    memsize = 24,
    output_dir = data_dir,
    version = "v-9-05-s20"
)
class_cube <- sits_label_classification(
    bayes_cube,
    multicores = 4,
    memsize = 24,
    output_dir = data_dir,
    version = "v-9-05-false-s20"
)






## Implementation in sits{-}

To run Bayesian smoothing, the parameter of `sits_smooth()` are: (a) `cube`, a probability cube produced by `sits_classify()`; (b) `type` should be `bayes` (the default); (c) `window_size`, the local window to compute the neighborhood probabilities; (d) `neigh_fraction`, fraction of local neighbors used to calculate local statistics; (e) `smoothness`, an estimate of the local variance (see Technical Annex for details); (f) `multicores`, number of CPU cores that will be used for processing; (g) `memsize`, memory available for classification; (h) `output_dir`, directory where results will be stored; (i) `version`, for version control. The resulting cube can be visualized with `plot()`. The bigger one sets the `window_size` and `smoothness` parameters, the stronger the adjustments will be.  In what follows, we compare two situations of smoothing effects, by varying the `window_size` and `smoothness` parameters. 

As explained above, the `window_size` parameter controls the size of the neighborhood. However, not all pixels inside the window will be included in the Bayesian estimator.  To be reliable, local class statistics should only include pixels that are likely to belong to such class. Windows centered on border pixels contain only some pixels belonging to same class as the central pixel; the others belongs to a different class. Consider a window of size 9 x 9 around a pixel in the probability map of class "Forest". It will contain the central pixel and 80 neighbors. Instead of using all those neighbors to compute the local statistics, `sits` uses only some of them. The number of neighbors used to calculate the local statistics is set by the taking those with the highest probability of belonging to class "Forest". The percentage of pixel per window used to calculate local class statistics is set by the `neigh_fraction` parameter. 

Together, the parameters `window_size` and `neigh_fraction` control how many pixels in a neighborhood are used to calculate the local statistics used by the Bayesian estimator. Since the estimator is based on Gaussian distributions, it needs at least 30 samples to produce statistical significant values. For example, setting `window size` to 9 and `neigh_fraction` to 0.5 (the defaults) ensures that at least 40 samples are used to estimate the local statistics. 

```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Probability maps after bayesian smoothing."}
# compute Bayesian smoothing
cube_smooth_w9_s20 <- sits_smooth(
    cube = ro_cube_20LKP_probs,
    window_size = 9,
    neigh_fraction = 0.50,
    smoothness = 20,
    multicores = 4,
    memsize = 12,
    version = "bayes_w9_s20",
    output_dir = "./tempdir/chp10"
)
# plot the result
plot(cube_smooth_w9_s20, palette = "YlGn")
```

Bayesian smoothing has removed some of local variability associated to misclassified pixels which are different from their neighbors. The impact of smoothing is best appreciated comparing the labelled map produced without smoothing to the one that follows the procedure, as shown below.

```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Final classification map after Bayesian smoothing with 5 x 5 window and smoothness = 30."}
# generate thematic map
defor_map_smooth_w9_20 <- sits_label_classification(
    cube = cube_smooth_w9_s20,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp8",
    version = "bayes_w9_s20"
)
plot(defor_map_smooth_w9_20)
```

To produce an even stronger smoothing effect, the example below uses bigger values for `window_size` and `smoothness`. 

```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Probability maps after bayesian smoothing with 9 x 9 window smoothness = 80."}
# compute Bayesian smoothing
cube_smooth_w13_s80 <- sits_smooth(
    cube = ro_cube_20LKP_probs,
    type = "bayes",
    window_size = 13,
    smoothness = 80,
    multicores = 4,
    memsize = 12,
    version = "bayes_w13_s80",
    output_dir = "./tempdir/chp8"
)
# plot the result
plot(cube_smooth_w13_s80, palette = "YlGn")
```


```{r, tidy = "styler", out.width = "90%", fig.align="center", fig.cap="Final classification map after Bayesian smoothing with 9 x 9 size."}
# generate thematic map
defor_map_smooth_w13_s80 <- sits_label_classification(
    cube = cube_smooth_w13_s80,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp8",
    version = "bayes_w13_s80"
)
plot(defor_map_smooth_w13_s80, palette = "YlGn")
```

Comparing the two maps, it is apparent that the smoothing procedure has reduced a lot of the noise in the original classification and produced a more homogeneous result. Although more pleasing to the eye, this map may not be be more accurate than the previous one, since much spatial details has been lost. In general, Bayesian smoothing improves the quality of the final labelled maps and thus should be applied in most situations.

<!--chapter:end:10-bayesiansmoothing.Rmd-->

# Validation and accuracy measurements{-}



```{r, include = FALSE, echo = FALSE}
source("common.R")
dir.create("./tempdir/chp10")
```

## Case study{-}

To show how to do validation and accuracy assessment in`sits`, we show an example of land classification in the Cerrado biome, the second largest biome in Brazil with 1.9 million km$^2$.  The Brazilian Cerrado is a tropical savanna ecoregion with a rich ecosystem ranging from grasslands to woodlands. It is home to more than 7000 species of plants with high levels of endemism [@Klink2005]. It includes three major types of natural vegetation: Open Cerrado, typically composed of grasses and small shrubs with a sporadic presence of small tree vegetation; Cerrado, a typical savanna formation, with the presence of low, irregularly branched, thin-trunked trees; and Cerradão, areas of medium-sized trees (up to 10--12 m)[@Del-Claro2019]. Its natural areas are being converted to agriculture at a fast pace, as it is one of the world's fast moving agricultural frontiers [@Walter2006]. The main agricultural land uses include cattle ranching, crop farms, and planted forests. The classification follows the work by Simoes et al.[@Simoes2021].

The data is composed of 67 Landsat-8 tiles from the Brazil Data Cube, with 23 time steps covering the the period 2017-08-29 to 2018-08-29. Since the data is avaliable in the Brazil Data Cube, users should first obtain access to the BDC, by obtaining an access key. After obtaining the access key, they should  to include their credentials using an environment variables, as shown below. Obtaining a BDC access key is free. Users need to register at the [BDC site](https://brazildatacube.dpi.inpe.br/portal/explore) to obtain the key.
```{r,eval = FALSE}
Sys.setenv(
    "BDC_ACCESS_KEY" = <your_bdc_access_key>
)
```

After obtaining the BDC access key, we now can create a data cube for the Cerrado biome. 

```{r, tidy = "styler", out.width = "80%", fig.align="center", fig.cap="Color composite image of first date of the cube"}

# files are available in the Brazil Data Cube
# 
# obtain the region of interest covering the Cerrado biome
roi_cerrado_shp <- system.file(
    "extdata/shapefiles/cerrado_border/cerrado_border.shp",
    package = "sitsdata")
# read the shapefile as an object of the "sf" package
roi_cerrado <- sf::st_read(roi_cerrado_shp, quiet = TRUE)

# create a data cube for the entire cerrado biome
cerrado_cube <- sits_cube(
        source = "BDC",
        collection = "LC8_30_16D_STK-1",
        roi = roi_cerrado,
        start_date = "2017-08-29",
        end_date = "2018-08-29",
        multicores = 3
)
# plot the first date with NDVI and EVI bands
plot(cerrado_cube, 
     tile = "044049", 
     red = "B7", 
     green = "B5", 
     blue = "B4"
)
```

To classify the Cerrado, we use a training data set produced by by Simoes et al.[@Simoes2021]. The authors carried out a systematic sampling using a grid of 5 x 5 km throughout the Cerrado biome, collecting 85,026 samples. The training data labels were extracted from three sources: the 2018 pastureland map from Parente et al. [@Parente2019], MapBiomas Collection 5 for 2018 [@Souza2020], and~Brazil's National Mapping Agency IBGE land maps for 2016--2018. Out of the 85,026 samples, the authors selected those where there was no disagreement between the labels assigned by the three sources.  The final training set consists of 48,850 points from which the authors extracted the time series using the Landsat-8 data cube available in the BDC. The classes for this training set are: `Annual Crop`, `Cerradao`, `Cerrado`, `Open Cerrado`, `Nat_NonVeg` (Dunes), `Pasture`, `Perennial_Crop`, `Silviculture` (Planted Forests), `Sugarcane`, and `Water`. This data set is available in the package `sitsdata` as `samples_cerrado_lc8`. 


```{r, tidy = "styler", out.width = "70%"}
library(sitsdata)
data("samples_cerrado_lc8")
# show the class distribution in the new training set
sits_labels_summary(samples_cerrado_lc8)
```

## Cross validation of training set{-}

Cross-validation is a technique to estimate the inherent prediction error of a model [@Hastie2009]. Since cross-validation uses only the training samples, its results are not accuracy measures, unless the samples have been carefully collected to represent the diversity of possible occurrences of classes in the study area [@Wadoux2021]. In practice, when working in large areas, it is hard to obtain random stratified samples which cover the different variations in land cover associated to the ecosystems of the study area. Thus, cross-validation should be taken a as measure of model performance on the training data and not as an estimate of overall map accuracy. 

Cross-validation uses part of the available samples to fit the classification model, and a different part to test it. The k-fold validation method splits the data into $k$ partitions with approximately the same size and proceed by fitting the model and testing it $k$ times. At each step, we take one distinct partition for test and the remaining ${k-1}$ for training the model, and calculate its prediction error for classifying the test partition. A simple average gives us an estimation of the expected prediction error. The recommended choices of $k$ are $5$ or $10$ [@Hastie2009].

`sits_kfold_validate()` supports k-fold validation in `sits`. The result is the confusion matrix and the accuracy statistics (overall and by class). In the examples below, we use multiprocessing to speed up the results. 

```{r, echo = FALSE}
set.seed(290356)
```

Since the data set is big and highly imbalanced, we use the function `sits_reduce_imbalance()` to reduce the size and produce a smaller and more balanced sample data set for the validation examples.

```{r, tidy = "styler", echo = TRUE, eval=FALSE}
# reduce imbalance in the data set
# maximum number of samples per class will be 1000 
# minimum number of samples per class will be 500
samples_cerrado_bal <- sits_reduce_imbalance(
    samples = samples_cerrado_lc8,
    n_samples_over = 500,
    n_samples_under = 1000,
    multicores = 4
)
# show new sample distribution
sits_labels_summary(samples_cerrado_bal)
```

```{r, tidy = "styler", echo = FALSE, eval=TRUE}
samples_cerrado_bal <- readRDS("./etc/samples_cerrado_bal.rds")
sits_labels_summary(samples_cerrado_bal)
```

The following code does a five-fold validation using random forests. 

```{r, tidy = "styler"}
# perform a five fold validation for the cerrado data set
# random forests machine learning method using default parameters
val_rfor <- sits_kfold_validate(
    samples = samples_cerrado_bal, 
    folds = 5, 
    ml_method = sits_rfor(),
    multicores = 5
)
# print the validation statistics
sits_accuracy_summary(val_rfor)
```

One useful function in SITS is the capacity to compare different validation methods and store them in an XLS file for further analysis. The following example shows how to do this, using the Cerrado data set. We take the models: random forests(`sits_rfor()`), extreme gradient boosting (`sits_xgboost()`), temporal CNN (`sits_tempcnn()`), and lightweight temporal attention encoder (`sits_lighttae())`.  After computing the confusion matrix and the statistics for each model, we also store the result in a list. When the calculation is finished, the function `sits_to_xlsx()` writes all of the results in an Excel-compatible spreadsheet. 


```{r, tidy = "styler", eval = FALSE}
# Compare different models for the Cerrado data set
# create a list to store the results
results <- list()
# Give a name to the results of the random forest model (see above)
val_rfor$name <- "rfor"
# store the rfor results in a list
results[[length(results) + 1]] <- val_rfor

## Extreme Gradient Boosting
val_xgb <- sits_kfold_validate(
    samples = samples_cerrado_bal,
    ml_method = sits_xgboost(),
    folds = 5,
    multicores = 5
)

# Give a name to the SVM model
val_xgb$name <- "xgboost"
# store the results in a list
results[[length(results) + 1]] <- val_xgb

# Temporal CNN
val_tcnn <- sits_kfold_validate(
    samples = samples_cerrado_bal,
    ml_method = sits_tempcnn(
        optimizer = torchopt::optim_adamw,
        opt_hparams = list(lr = 0.001)
    ),
    folds = 5,
    multicores = 5
)

# Give a name to the result
val_tcnn$name <- "TempCNN"
# store the results in a list
results[[length(results) + 1]] <- val_tcnn

# Light TAE
val_ltae <- sits_kfold_validate(
    samples = samples_cerrado_bal,
    ml_method = sits_lighttae(
        optimizer = torchopt::optim_adamw,
        opt_hparams = list(lr = 0.001)
    ),
    folds = 5,
    multicores = 5
)

# Give a name to the result
val_ltae$name <- "LightTAE"
# store the results in a list
results[[length(results) + 1]] <- val_ltae

# Save to an XLS file
xlsx_file <- "./model_comparison.xlsx"

sits_to_xlsx(results, file = xlsx_file)
```
The resulting Excel file can be opened with R or using spreadsheet programs. The figure below shows a printout of what is read by Excel. As shown below, each sheet corresponds to the output of one model. For simplicity, we show only the result of TempCNN, that has an overall accuracy of 90%. 

```{r, echo = FALSE, fig.align="center", out.width = "90%", out.height = "90%", fig.cap= "Result of 5-fold cross validation of Mato Grosso data using LightTAE"}

knitr::include_graphics("images/k_fold_validation_xlsx.png")

```

The scores for overall accuracy are similar between the models. However, there are significant differences between the models, as shown by comparing their F1 scores, as shown below.    

```{r, eval = FALSE, echo = TRUE}
model_acc <- tibble::tibble(
    "Random Forest" = val_rfor$overall[["Accuracy"]],
    "XGBoost"       = val_xgb$overall[["Accuracy"]],
    "TempCNN"       = val_tcnn$overall[["Accuracy"]],
    "LightTAE"      = val_ltae$overall[["Accuracy"]]
)
options(digits = 3)
model_acc
```

```{r, eval = TRUE, echo = FALSE}
model_acc <- readRDS(file = "./etc/model_acc.rds")
options(digits = 3)
model_acc
```
The table below shows the F1-scores of all classes for each model, as produced by the k-fold validation. The F1-scores are obtained as the harmonic mean between user's accuracy and precision accuracy for each class. The results show that although deep learning models such TempCNN and LightTAE have similar overall accuracies than Random Forests or XGBoost, their F1-scores per class are in most cases better.  

```{r, eval=FALSE, echo = TRUE, tidy = "styler"}
f1_score_rfor <- unname(val_rfor$byClass[,"F1"])
f1_score_xgb <-  unname(val_xgb$byClass[,"F1"])
f1_score_tcnn <-  unname(val_tcnn$byClass[,"F1"])
f1_score_ltae <-  unname(val_ltae$byClass[,"F1"])

f1_scores <- tibble::tibble(
    "Classes"  = sits_labels(samples_cerrado_bal),
    "RandFor"  = f1_score_rfor,
    "XGBoost"  = f1_score_xgb,
    "TempCNN"  = f1_score_tcnn,
    "LightTAE" = f1_score_ltae
)
f1_scores
```
```{r, eval = TRUE, echo = FALSE}
f1_scores <- readRDS(file = "./etc/f1_scores.rds")
options(digits = 3)
f1_scores
```

The cross validation results have to be interpreted carefully. Cross validation is a measure how well the model fits the training data. Using these results as a measure of classification accuracy is only valid if the training data is a good sample of the entire data set. In practice, training data is subject to various sources of bias. In most cases of land classification, some classes are much more frequent than others and as such the training data set will be imbalanced. For large areas, regional differences in soil and climate condition will lead that the same classes will have different spectral responses. When collecting samples for large areas, field analysts may be restricted to areas where they have access (e.g, along roads). An additional problem is that of mixed pixels. Expert interpreters tend to select samples which standout in field work or reference images. Border pixels are unlikely to be chosen as part of training data. For all these reasons, cross validation results should not be considered as indicative of  accuracy measurement over the entire data set. 


## Accuracy assessment of classified images{-}

To measure the accuracy of classified images, the `sits_accuracy()` function uses an area-weighted technique, following the best practices proposed by Olofsson et al. [@Olofsson2013]. The need for area-weighted estimates arises from the fact the land use and land cover classes are not evenly distributed in space. In some applications (e.g., deforestation) where the interest lies in assessing how much of the image has changed, the area mapped as deforested is likely to be a small fraction of the total area. If users disregard the relative importance of small areas where change is taking place, the overall accuracy estimate will be inflated and unrealistic. For this reason, Olofsson et al [@Olofsson2013] argue that "mapped areas should be adjusted to eliminate bias attributable to map classification error and these error-adjusted area estimates should be accompanied by confidence intervals to quantify the sampling variability of the estimated area".

With this motivation, when measuring accuracy of classified images, the function `sits_accuracy()` follows the procedure set by Olofsson et al. [@Olofsson2013]. Given a classified image and a validation file, the first step is to calculate the confusion matrix in the traditional way, i.e., by identifying the commission and omission errors. Then we calculate the unbiased estimator of the proportion of area in cell $i,j$ of the error matrix

$$
\hat{p_{i,j}} = W_i\frac{n_{i,j}}{n_i}
$$
where the total area of the map is $A_{tot}$, the mapping area of class $i$ is $A_{m,i}$ and the proportion of area mapped as class $i$ is $W_i = {A_{m,i}}/{A_{tot}}$. Adjusting for area size allows producing an unbiased estimation of the total area of class $j$, defined as a stratified estimator
$$
\hat{A_j} = A_{tot}\sum_{i=1}^KW_i\frac{n_{i,j}}{n_i}
$$
This unbiased area estimator includes the effect of false negatives (omission error) while not considering the effect of false positives (commission error). The area estimates also allow producing an unbiased estimate of the user's and producer's accuracy for each class. Following @Olofsson2013, we provide the 95% confidence interval for $\hat{A_j}$. 

To use the `sits_accuracy()` function to produce the adjusted area estimates, users have to provide the classified image together with a csv file containing a set of well selected labeled points.  The csv file should have the same format as the one used to obtain samples, as discussed earlier. The labelled points should be based on a random stratified sample. All areas associated to each class should contribute to the test data used for accuracy assessment. 

Because of the biases inherent in cross validation of training data, users should provide an independent validation data set to measure classification accuracy. In this case study, Simoes et al.[@Simoes2021] did a systematic sampling of the Cerrado biome using a 20 x 20 km grid with a total of 5402 points. These samples are independent of the training set used in the classification. They were interpreted by five specialists using high resolution images from the same period of the classification. This resulted in 5286 evaluation samples thus distributed: "Annual Crop" (553), "Cerrado" (3155), "Natural Non Vegetated" (44), "Pasture" (1246), "Perennial Crop" (38), "Silviculture" (94), "Sugarcane" (77), and "Water" (79). This data set is available in package `sitsdata`, as described below. In this validation file, all samples belonging to classes "Cerrado", "Open Cerrado" and "Cerradao" (Woody Savanna) have been grouped together in a single class. 

The first step is to obtain the classification map. The code for the full classification of the Cerrado biome, using the TempCNN algorithm, is shown below. Because of the large data size, the code will not be executed.  For the accuracy assessment, we will use the labelled classification map available in a Dropbox folder. 

```{r, tidy="styler", echo = TRUE, eval = FALSE}
# This code shows the classification of the Cerrado biome
# It is include for information purposes
# It takes a long time to run
tcnn_model <- sits_train(
    samples = samples_cerrado_lc8,
    ml_method = sits_tempcnn()
)
# using the tempCNN model to classify the Cerrado
# this example should be run on a large virtual machine
cerrado_probs_cube <- sits_classify(
    cube = cerrado_cube,
    ml_model = tcnn_model,
    memsize = 128,
    multicores = 64,
    output_dir = "./cerrado_probs"
)
cerrado_bayes_cube <- sits_smooth(
    cube = cerrado_probs_cube,
    type = "bayes",
    memsize = 128,
    multicores = 64,
    output_dir = "./cerrado_bayes"
)
cerrado_classif <- sits_label_classification(
    cube = cerrado_bayes_cube,
    memsize = 128,
    multicores = 64,
    output_dir = "./cerrado_label"
)
```

Since the above code is included for information only, we use the labelled cube stored in a Dropbox folder to perform the accuracy assessment. First, we retrieve the metadata for the cube.

```{r, tidy="styler", out.width = "100%", fig.align="center", fig.cap="Classification of tile 044048 from the Landsat data cube for the Brazilian Cerrado in 2017/2018." }
# retrieve the metadata for the classified cube
# the files are stored as Dropbox links
cerrado_classif_rds <- system.file("extdata/Cerrado/cerrado_classif_dropbox.rds",
                                   package = "sitsdata")
# read the cube metadata
cerrado_classif <- readRDS(cerrado_classif_rds)
# plot one tile of the classification
plot(cerrado_classif, tiles = "044048")
```
The next step is to provide a CSV file with the validation points, as described above.

```{r, eval = FALSE, echo = TRUE}
# get ground truth points
valid_csv <- system.file("extdata/csv/cerrado_lc8_validation.csv",
                            package = "sitsdata")
# calculate accuracy according to Olofsson's method
area_acc <- sits_accuracy(cerrado_classif, 
                          validation_csv = valid_csv)
# print the area estimated accuracy 
area_acc
```

```{r, eval = TRUE, echo = FALSE}
# calculate accuracy according to Olofsson's method
area_acc <- readRDS("./etc/area_acc.rds")
# print the area estimated accuracy 
options(digits = 3)
area_acc
```
This example shows that it is important to correct area estimates in land classification, to reduce the bias effect of misclassification and to take into account the different producer's accuracies associated to each class. It also shows that actual overall accuracy is in general lower than the result of cross-validation.  



<!--chapter:end:11-validation.Rmd-->

# Uncertainty and active learning{-}

```{r, echo = FALSE}
source("common.R")
```

Land use and land cover classification tasks have unique characteristics that differ from other machine learning domains such as image recognition and natural language processing. The main challenge for land classification is to be able to describe the diversity of the planet's landscapes in a handful of labels. However, the diversity of the world's ecosystem makes all classification systems to be biased approximations of reality. As stated by Murphy : “The gradation of properties in the world means that our smallish number of categories will never map perfectly onto all objects” [@Murphy2002]. For this reason, `sits` provides tools for users to improve their classifications by iterative means, using a process called "active learning". 

Active learning for remote sensing data classification is an iterative process of sample selection, labeling, and model retraining. The following steps provide a general overview of how to use active learning for remote sensing data classification:

1. Collect initial training samples: Start by collecting a small set of representative training samples that cover the range of land cover classes of interest.
2. Train a machine learning model: Use the initial training samples to train a machine learning model to classify remote sensing data.
3. Classify the data cube using the model.
4. Identify areas of uncertainty. 
5. Select samples for re-labeling: select a set of unlabeled samples that the model is most uncertain about, i.e., those that the model is least confident in classifying.
6. Label the selected samples: The user labels the selected samples, adding them to the training set.
7. Retrain the model: The model is retrained using the newly labeled samples, and the process repeats itself, starting at step 2.
8. Stop when the classification accuracy is satisfactory: The iterative process continues until the classification accuracy reaches a satisfactory level.

In traditional classification methods, experts provide a set of training samples and use a machine learning algorithm to produce map. By contrast, the active learning approach puts the human in the loop[@Monarch2021]. At each iteration, an unlabeled set of samples is presented to the user, which assigns classes to them and includes them in the training set [@Crawford2013]. The process is repeated until the expert is satisfied with the result, as shown in the Figure below. 

```{r, echo = FALSE, out.width = "100%", fig.align="center", fig.cap="Active learning approach (source: Crawford et al., 2013)"}
knitr::include_graphics("images/active_learning.png")
```

Active learning aims to reduce bias and errors in sample selection and as such improve the accuracy of the result. At each interaction, experts are asked to review  pixels where the ML classifier indicates a high value of uncertainty. Source of classification uncertainty include missing classes and or mislabeled samples. In `sits`, active learning is supported by the combination of three functions: `sits_uncertainty()`, `sits_uncertainty_sampling()` and `sits_confidence_sampling()`. 

## Measuring uncertainty{-} 

Uncertainty in land use and land cover classification refers to the degree of doubt or ambiguity in the accuracy of the classification results. The process of classifying land use and land cover involves interpreting remotely sensed images or other geospatial data to assign specific land use or land cover categories to various regions on the ground. Several sources of uncertainty can arise during this process, including:

1.	Classification errors: These can occur when the classification algorithm misinterprets the spectral or spatial characteristics of the input data, leading to misclassification of land use or land cover categories.
2.	Ambiguity in the definition of land use/land cover categories: The definition of land use or land cover categories can be ambiguous or subjective, leading to inconsistencies in the classification results.
3.	Variability in the landscape: Natural and human-induced variations in the landscape can make it difficult to accurately classify land use or land cover in some regions.
4.	Limitations of the data: The quality and quantity of input data can influence the accuracy of the classification results.

Quantifying uncertainty in land use and land cover classification is important for ensuring that the classification results are reliable and useful for decision-making. Various methods, such as confusion matrices and error matrices, can be used to estimate and visualize the level of uncertainty in classification results. Additionally, incorporating uncertainty estimates into decision-making processes can help to identify regions where further investigation or data collection is needed. 

The function `sits_uncertainty()` calculates the uncertainty cube based on the probabilities produced by the classifier. It takes a probability cube as input. The uncertainty measure is relevant in the context of active leaning, and helps to increase the quantity and quality of training samples by providing information about the confidence of the model. The supported types of uncertainty are 'entropy', 'least', 'margin' and 'ratio'. 

Least confidence sampling is computed as the difference between no uncertainty (100% confidence) and the probability of the most likely class, normalized by the number of classes. Let $P_1(i)$ be the higher class probability for pixel $i$. Then least confidence sampling is expressed as

$$
\theta_{LC} = (1 - P_1(i)) * \frac{n}{n-1}
$$

Margin of confidence sampling is the difference between the two most confident predictions, expressed in range from 0% (no uncertainty) to 100% (maximum uncertainty). Let $P_1(i)$ and be $P_1(i)$ the two higher class probability for pixel $i$. Then, margin of confidence is expressed as 

$$
\theta_{MC} = (1 - P_1(i) - P_2(i))
$$
Ratio of confidence is the measure of the ratio between the two most confident predictions, expressed in range from 0% (no uncertainty) to 100% (maximum uncertainty). Let $P_1(i)$ and be $P_1(i)$ the two higher class probability for pixel $i$. Then, ratio of confidence is expressed as 
$$
\theta_{RC} = \frac{P_2(i)}{P_1(i)}
$$

Entropy is a measure of uncertainty used by Claude Shannon on his classic work "A Mathematical Theory of Communication". It is related to amount of variability in the probabilities associated to a pixel. The lower the variability, the lower the entropy. Let $P_k(i)$ be the probability associated to class $k$ for pixel $i$. The entropy is calculated as 
$$
\theta_{E} = \frac{-\sum_{k=1}^K P_k(i) * log_2(P_k(i))}{log_2{n}}
$$

The parameters for `sits_uncertainty()` are: `cube`, a probability data cube; `type`, the uncertainty measure (default is `least`). In the case of entropy, it also requires the parameters  `window_size`, size of neighborhood to calculate entropy (default is 5) and `window_fn`, function to be applied in entropy calculation (default is `median`). As with other processing functions, users can specify `multicores` as the number of cores to run the function and `memsize`, maximum overall memory (in GB) to run the function. Optional parameters include `output_dir` (output directory for image files) and `version` (version of result).

## Using uncertainty measures for active learning{-}

The following case study shows how uncertainty measures can be used in the context of active learning. The study area is subset of one Sentinel-2 tile in the state of Rondonia, Brazil. The aim of the work is to detect deforestation in the Brazilian Amazonia. 

The study area is located close to the Samuel Hydroelectric Dam, located on the Madeira River in the Brazilian state of Rondônia. Building the dam led to a loss of 56,000 hectares of native forest. The construction of the dam caused the displacement of several indigenous communities and traditional populations, leading to social and cultural disruption. Additionally, the flooding of large areas of forest resulted in the loss of habitats and biodiversity, including several endangered species. The dam has altered the natural flow of the Madeira River, leading to changes in water quality and temperature, and affecting the aquatic life that depends on the river. The changes in river flow have also impacted the navigation and transportation activities of the local communities.

The first step is to produce a regular data cube for the chosen area rom the period 2020-06-01 to 2021-09-01. To reduce processing time and storage, we use only three bands ("B02", "B8A", "B11") plus the cloud band, and take a small area inside the tile. After obtaining a regular cube, we plot the study area in the dates during the temporal interval of the data cube. The first image is taken at the beginning of the dry season in "2020-07-04", when the inundation area of the dam is covered by shallow water. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Area in Rondonia near Samuel dam (source: authors)"}
# create a directory to store files
if (!file.exists("./tempdir/chp11"))
    dir.create("./tempdir/chp11")
# select a S2 tile
s2_cube_ro <- sits_cube(
      source = "MPC",
      collection = "sentinel-2-l2a",
      tiles = "20LMR",
      bands = c("B02", "B8A", "B11", "SCL"),
      start_date = as.Date("2020-06-01"),
      end_date = as.Date("2021-09-01")
)
# select a small area inside the tile

roi = c(lon_max = -63.25790, lon_min = -63.6078, 
        lat_max = -8.72290, lat_min = -8.95630)

# regularize the small area cube

s2_reg_cube_ro <- sits_regularize(
  cube = s2_cube_ro,
  output_dir = "./tempdir/chp11/",
  res = 20,
  roi = roi,
  period = "P16D",
  multicores = 4
)
plot(s2_reg_cube_ro, 
     red = "B11", 
     green = "B8A", 
     blue = "B02",
     date = "2020-07-04")
```

The second image is from "2020-11-09" and shows that most of the inundation area dries during the dry season. In early November 2020, after the end of the dry season, the inundation area is dry and has a response similar to bare soil and to burned areas. The Madeira river can be seen running through the dried inundation area. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Area in Rondonia near Samuel dam in November 2021 (source: authors)"}
plot(s2_reg_cube_ro, 
     red = "B11", 
     green = "B8A", 
     blue = "B02", 
     date = "2020-11-09")
```

The third image is from "2021-08-08".  In early August 2021, after the wet season, the inundation area is again covered by a shallow water layer. A number of burned and clear cut areas can also be seen in the August 2021 image compared with the July 2020 one. Given the contrast between the wet and dry seasons, correct land cover classification of this area is hard. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Area in Rondonia near Samuel dam in August 2021 (source: authors)"}
plot(s2_reg_cube_ro, red = "B11", green = "B8A", blue = "B02", date = "2021-08-08")
```

The next step is to classify this study area using a training set with 480 times series collected over the state of Rondonia (Brasil) for detecting deforestation. The training set uses 4 classes ("Burned_Area", "Forest", "Highly_Degraded" and "Cleared_Area"). The cube is classified using a LightTAE model, post-processed by a Bayesian smoothing, and then labelled.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Classified map for area in Rondonia near Samuel dam (source: authors)"}
library(sitsdata)
# load the training set
data("samples_prodes_4classes")
# select the same three bands used in the data cube
samples_4classes_3bands <- sits_select(
    data = samples_prodes_4classes, 
    bands = c("B02", "B8A", "B11")
)
# Train a lightTAE model 
ltae_model <- sits_train(
    samples = samples_4classes_3bands, 
    ml_method = sits_lighttae()
)
# classify the small area cube
s2_cube_probs <- sits_classify(
    data = s2_reg_cube_ro,
    ml_model = ltae_model,
    output_dir = "./tempdir/chp11/",
    memsize = 15,
    multicores = 5
)
# post-process the probability cube
s2_cube_bayes <- sits_smooth(
    cube = s2_cube_probs,
    output_dir = "./tempdir/chp11/",
    memsize = 16,
    multicores = 4
)
# label the post-processed  probability cube
s2_cube_label <- sits_label_classification(
    cube = s2_cube_bayes,
    output_dir = "./tempdir/chp11/",
    memsize = 16,
    multicores = 4  
)
plot(s2_cube_label)
```
The resulting map correctly identifies the forest area and the deforestation. However, it wrongly classifies the area covered by the Samuel hydroelectric dam. The reason is the lack of samples for classes related to surface water and wetlands. To improve the classification, we need to improve our samples. To do that, the first step is calculate the uncertainty of the classification.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Uncertainty map for classification in Rondonia near Samuel dam (source: authors)"}
# calculate the uncertainty cube
s2_cube_uncert <- sits_uncertainty(
    cube = s2_cube_bayes,
    type = "entropy",
    output_dir = "./tempdir/chp11/",
    memsize = 16,
    multicores = 4  
)
plot(s2_cube_uncert)
```
As expected, the places of highest uncertainty are those covered by surface water or associated to wetlands. These places are likely to be misclassified. For this reason, `sits` provides the function `sits_uncertainty_sampling()` which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 that have high uncertainty. The function has three parameters: `n`, number of uncertain points to be included; `min_uncert`, minimum value of uncertainty for pixels to be included in the list; and `sampling_window`, to improve the spatial distribution of the new samples by avoiding points in the same neighborhood to be included. After running the function, we can use `sits_view()` to visualize the location of the samples.

```{r, tidy = "styler", echo = TRUE, eval = FALSE}
# calculate the uncertainty cube
new_samples <- sits_uncertainty_sampling(
    uncert_cube = s2_cube_uncert,
    n = 20,
    min_uncert = 0.5,
    sampling_window = 10
)
# view the location of the samples
sits_view(new_samples)
```



```{r, tidy = "styler", echo = FALSE, eval = TRUE, out.width = "100%", fig.align="center", fig.cap= "Location of uncertain pixel for classification in Rondonia near Samuel dam (source: authors)"}
# calculate the uncertainty cube
new_samples <- sits_uncertainty_sampling(
    uncert_cube = s2_cube_uncert,
    n = 20,
    min_uncert = 0.5,
    sampling_window = 10
)
knitr::include_graphics("images/uncertain_pixels.png")
```

The visualization shows that the samples are located in the areas covered by the Samuel data. As a first approximation, one can use the label "Wetlands" to designate these samples. A more detailed evaluation, which is recommended in practices, requires analysing these samples with an exploration software such as QGIS and individually labelling each sample. In our case, we will take a direct approach for illustration purposes.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "New land classification in Rondonia near Samuel dam (source: authors)"}
# label the new samples
new_samples$label <- "Wetland"
# obtain the time series from the regularized cube 
new_samples_ts <- sits_get_data(
    cube = s2_reg_cube_ro,
    samples = new_samples
)
# join the new samples with the original ones with 4 classes
samples_4classes_3bands_round_2 <- dplyr::bind_rows(
    samples_4classes_3bands,
    new_samples_ts
)
# train a lightTAE model with the new sample set
ltae_model_v2 <- sits_train(
    samples = samples_4classes_3bands_round_2, 
    ml_method = sits_lighttae()
)
# classify the small area cube
s2_cube_probs_v2 <- sits_classify(
    data = s2_reg_cube_ro,
    ml_model = ltae_model_v2,
    output_dir = "./tempdir/chp11/",
    version = "v2",
    memsize = 16,
    multicores = 4
)
# post-process the probability cube
s2_cube_bayes_v2 <- sits_smooth(
    cube = s2_cube_probs_v2,
    output_dir = "./tempdir/chp11/",
    version = "v2",
    memsize = 16,
    multicores = 4
)
# label the post-processed  probability cube
s2_cube_label_v2 <- sits_label_classification(
    cube = s2_cube_bayes_v2,
    output_dir = "./tempdir/chp11/",
    version = "v2",
    memsize = 16,
    multicores = 4  
)
# plot the second version of the classified cube
plot(s2_cube_label_v2)
```

The results shows a significant quality gain over the earlier classification. There are still some confusion areas in the exposed soils inside the inundation area, some of which have been classified as burned areas. It is useful also to show the uncertainty map associated with the second model. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Uncertainty map for classification in Rondonia near Samuel dam - improved model (source: authors)"}
# calculate the uncertainty cube
s2_cube_uncert_v2 <- sits_uncertainty(
    cube = s2_cube_bayes_v2,
    type = "entropy",
    output_dir = "./tempdir/chp11/",
    version = "v2",
    memsize = 16,
    multicores = 4  
)
plot(s2_cube_uncert_v2)
```
 
As the new uncertainty map shows, there is a significant improvement in the quality of the classification. The remaining areas of high uncertainty are those places affected by the contrast between wet and dry season close to the inundation area. These areas are low-laying places that sometimes are covered by water and sometimes are bare soil areas throughout the year, depending on the intensity of the rainy season. To further improve the quality of the classification, we could obtain new samples of those uncertain areas, label them and add them to samples. In general, as this section shows, combining uncertainty measurements with active learning is a recommended practice for improving classification results. 

<!--chapter:end:12-uncertainty.Rmd-->

# Ensemble Prediction from Multiple Models{-}

Ensemble prediction is a powerful technique for combining predictions from multiple models to produce more accurate and robust predictions. In general, ensemble predictions produce better predictions than using a single model. This is because the errors of individual models can cancel out or be reduced when combined with the predictions of other models. As a result, ensemble predictions can lead to better overall accuracy and reduce the risk of overfitting. 

Ensemble predictions are  more robust to changes in the data or the model. If a single model is sensitive to certain types of data or errors, ensemble predictions can help reduce the impact of these issues by combining the predictions of multiple models. Ensemble predictions can help you choose the best model or models for a given task. By comparing the predictions of different models, you can identify which models perform best on different types of data or tasks. This can be especially useful when working with complex or uncertain data. By combining the predictions of multiple models, users can identify which features or factors are most important for making accurate predictions. When using ensemble methods, it's important to choose models that are diverse and have different sources of error. This can help ensure that the ensemble predictions are more accurate and robust.

Overall, ensemble predictions are a powerful tool for improving the accuracy and robustness of machine learning models. By combining the predictions of multiple models, users can reduce errors and uncertainty, and gain new insights into the underlying patterns in the data.

The `sits` package provides the function `sits_combine_predictions()` to estimate ensemble predictions using probability cubes produced by `sits_classify()` and optionally post-processed with `sits_smooth()`.  There are two ways to do ensemble predictions from multiple models in `sits`:

* Averaging: In this approach, the predictions of each model are averaged to produce the final prediction. This method works well when the models have similar accuracy and errors. 

* Uncertainty: Predictions from different models are compared in terms of their uncertainties on a pixel-by-pixel basis; predictions with lower uncertainty are chosen as the more likely ones to be valid. 

In what follows, we will use the same data used as in the "Image Classification in Data Cubes" chapter  section to illustrate how to produce an ensemble prediction. For simplicity, we repeat the steps taken for classify an image in that chapter: create a data cube, train a model using the lightweight temporal attention encoder algorithm (`sits_lighttae()`), then classify, post-process and label the data cube. As a starting point, we plot two instances of the data cube, at the start and at the end of the time series. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2020-07-06."}
# files are available in a local directory 
data_dir <- system.file("extdata/Rondonia-20LKP/", package = "sitsdata")
# read data cube
ro_cube_20LKP <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    parse_info = c('X1', "tile", "band", "date")
)
plot(ro_cube_20LKP, 
    date = "2020-07-06", 
    red = "B11", 
    green = "B8A", 
    blue = "B02"
)
```
The image from 2020-07-06 shows many areas under deforestation, especially a large one located in top center of the image. It is useful to compare to an image one year later, which shows a number of burned areas resulting from forest removal followed by fire. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2021-08-10."}
plot(ro_cube_20LKP, 
    date = "2021-08-10", 
    red = "B11", 
    green = "B8A", 
    blue = "B02"
)
```
The samples used in the classification are the same as those used in the "Image Classification in Data Cubes" chapter. Please refer to that chapter for a more detailed description of the temporal response of the samples. We first reproduce the result obtained in that chapter using the `sits_lighttae()` algorithm. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Image Classification using LightTAE model."}
# get the samples from library "sitsdata"
library(sitsdata)
data(samples_prodes_4classes)
# use only the bands available in the cube
samples_3bands <- sits_select(
    data = samples_prodes_4classes, 
    bands = sits_bands(ro_cube_20LKP)
)
# train model using LightTAE algorithm
ltae_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_lighttae(
        opt_hparams = list(lr = 0.001)
        )
)
# classify data cube
ro_cube_probs_ltae <- sits_classify(
    data     = ro_cube_20LKP,
    ml_model = ltae_model,
    output_dir = "./tempdir/chp12",
    version = "ltae",
    multicores = 4,
    memsize = 12
)
# smooth data cube
ro_cube_bayes_ltae <- sits_smooth(
    cube    = ro_cube_probs_ltae,
    output_dir = "./tempdir/chp12",
    version = "ltae",
    multicores = 4,
    memsize = 12
)
# generate thematic map
defor_map <- sits_label_classification(
    cube = ro_cube_bayes_ltae,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp12",
    version = "ltae"
)
plot(defor_map)
```
The deforestation map produced by the `sits_lighttae()` algorithm has spatial consistency; arguably, it underestimates the burned areas in the right-hand corner of the image. The method tries to modelling temporal behavior of the reflectances. For this reason, it sometimes fails to detect changes that occur in the last dates of the time series, as it occurs when areas are burned in August. 

To build a two-member ensemble, we now classify the same image using a random forests algorithm.  

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia using a random forests algorithm."}

# train model using random forests algorithm
rfor_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_rfor()
)

# classify the data cube using the tempCNN model
ro_cube_probs_rfor <- sits_classify(
    data = ro_cube_20LKP,
    ml_model = rfor_model,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4
)
# post-process the probability cube
ro_cube_bayes_rfor <- sits_smooth(
    cube = ro_cube_probs_rfor,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4
)
# label the post-processed  probability cube
ro_cube_label_rfor <- sits_label_classification(
    cube = ro_cube_bayes_rfor,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4  
)
# plot the random forests version of the classified cube
plot(ro_cube_label_rfor)
```

Comparing the two results, while most of the land areas have been classified equally, there are places of disagreement concerning the places classified as "Burned_Area" and "Highly Degraded". Since the random forests model is sensitive to the response of images in the end of the period, it tends to be better to distinguish burned areas. However, it tends to reduce the forest areas, classifying some of them as highly degraded. Such misclassification happens because the random forests algorithm disregards the temporal correlation of the input data. Values from a single date are used to distinguish between natural and degraded forest areas. 

Given the differences and complementarities between the two predicted outcomes, it is useful to combine them using `sits_combine_predictions()`. The first option for ensemble prediction is to take the average of the probability maps to reduce noise.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia near Samuel dam using the average of the probabilities produced by lightTAE and tempCNN algorithms."}
# combine the two predictions by taking the average of the probabilities for each class
s2_cube_average_probs <- sits_combine_predictions(
  cubes = list(ro_cube_bayes_ltae, ro_cube_bayes_rfor),
  type = "average",
  output_dir = "./tempdir/chp11/",
  version = "average",
  memsize = 16,
  multicores = 1
)
# label the average probability cube
s2_cube_average_class <- sits_label_classification(
    cube = s2_cube_average_probs,
    output_dir = "./tempdir/chp11/",
    version = "average",
    memsize = 16,
    multicores = 4  
)
# plot the second version of the classified cube
plot(s2_cube_average_class)
```
Compared with the initial map, the result has increased the number of pixels classified as burned areas and highly degraded. Not all areas classified as degraded forest by the random forests method have been included in the final map. Only those places where the random forests algorithm has high confidence have been included. In general, the average map results on a better classification than the individual results.

<!--chapter:end:13-ensembleprediction.Rmd-->

# Visualising and Exporting data{-}

```{r, include = FALSE}
source("common.R")
```

This section is intended for programmers and experts that would like to extend the capabilities of `sits`, either by including new data sources, ML algorithms, exporting data to be used in Python or QGIS, or including new display colors. 


## How colors work in sits{-}

In examples provided in the book, the color legend is taken for the predefined color table provided by `sits`.  This default color table is displayed using the command `sits_colors_show`. This color definition file assigns colors to 99 class names, including the IPCC and IGBP land use classes. 

```{r, tidy = "styler", out.width = "100%", out.height = "100%", echo = FALSE, fig.align="center", fig.cap="Default colors used in the sits package"}
# Display default sits colors
sits_colors_show()
```

The color table can be extended or adjusted by accessing and modifying the default color table, using the commands `sits_colors()` to retrieve the table and `sits_colors_set()` to update the table according to user choices, as shown in the example below.
```{r}
# retrieve the color table
color_tb <- sits_colors()
# show the color table
color_tb
```

As an example of a user-defined color table, consider a definition that covers level 1 of the Anderson Classification System used in the US National Land Cover Data, obtained by defining a new color table, as shown below. The colors can be defined by HEX values or by names accepted as R color codes.
```{r, tidy = "styler", out.width = "80%", out.height = "80%", fig.align="center", fig.cap="Example of Anderson Land Classification Scheme use in sits"}
# define a color table based on the Anderson Land Classification System
us_nlcd <- tibble::tibble(name = character(), color = character())
us_nlcd <- us_nlcd %>% 
  tibble::add_row(name = "Urban Built Up", color =  "#85929E") %>% 
  tibble::add_row(name = "Agricultural Land", color = "#F0B27A") %>% 
  tibble::add_row(name = "Rangeland", color = "#F1C40F") %>% 
  tibble::add_row(name = "Forest Land", color = "#27AE60") %>% 
  tibble::add_row(name = "Water", color = "#2980B9") %>% 
  tibble::add_row(name = "Wetland", color = "#D4E6F1") %>% 
  tibble::add_row(name = "Barren Land", color = "#FDEBD0") %>% 
  tibble::add_row(name = "Tundra", color = "#EBDEF0") %>% 
  tibble::add_row(name = "Snow and Ice", color = "#F7F9F9")
# load the color table into `sits`
sits_colors_set(us_nlcd)
# show the new color table used by sits
sits_colors_show()
```

```{r, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE} 
# reset the color table
sits_colors_reset()
```

As an alternative, users may define their own legends and pass them as parameters to to the `plot` function. Please see the example provided in Section "Map Reclassification" on Chapter "Image Classification in Data Cubes". 

## Exporting data to JSON{-}

Both the data cube and the time series tibble can be exported to exchange formats such as JSON.

```{r, tidy = "styler", eval = FALSE}
library(jsonlite)
# export the data cube to JSON
jsonlite::write_json(
  x = s2_20LKP_cube_MPC,
  path = "./data_cube.json",
  pretty = TRUE
)
# export the time series to JSON
jsonlite::write_json(
  x = samples_prodes_4classes,
  path = "./time_series.json",
  pretty = TRUE
)
```


<!--chapter:end:14-visualization.Rmd-->

# Technical Annex {-}

This chapter contains technical details on the algorithms available in `sits`. It is intended to support those that want to understand how the package works and also want to contribute to its development.

##  Bayesian smoothing {-}

Doing post-processing using Bayesian smoothing in SITS is straightforward. The result of the `sits_classify` function applied to a data cube is set of probability images, one per class. The next step is to apply the `sits_smooth` function. By default, this function selects the most likely class for each pixel considering only the probabilities of each class for each pixel. For continuous probability distributions, Bayesian inference is expressed by the rule:

$$
\pi(\theta|x) \propto \pi(x|\theta)\pi(\theta)
$$

Bayesian inference involves the estimation of an unknown parameter $\theta$, which is the random variable that describe what we are trying to measure. In the case of smoothing of image classification, $\theta$ is the class probability for a given pixel, conditioned by the probability values of that pixel. We model our initial belief about this value by a probability distribution,  $\pi(\theta)$, called the \emph{prior} distribution. It represents what we know about $\theta$ \emph{before} observing the data. The distribution $\pi(x|\theta)$, called the \emph{likelihood}, is estimated based on the observed data. It represents the added information provided by our observations. The \emph{posterior} distribution $\pi(\theta|x)$ is our improved belief of $\theta$ \emph{after} seeing the data. Bayes's rule states that the \emph{posterior} probability is proportional to the product of the \emph{likelihood} and the \emph{prior} probability.

### Derivation of bayesian parameters for spatiotemporal smoothing{-}

In our post-classification smoothing model, we consider the output of a machine learning algorithm that provides the probabilities of each pixel in the image to belong to target classes. More formally, consider a set of $K$ classes that are candidates for labelling each pixel. Let $p_{i,t,k}$ be the probability of pixel $i$ belonging to class $k$, $k = 1, \dots, K$. We have 
$$
\sum_{k=1}^K p_{i,k} = 1, p_{i,k} > 0
$$
We label a pixel $p_i$ as being of class $k$ if
$$
	p_{i, k} > p_{i,m}, \forall m = 1, \dots, K, m \neq k
$$


For each pixel $i$, we take the odds of the classification for class $k$, expressed as
$$
	O_{i,k} = p_{i,k} / (1-p_{i,k})
$$
where $p_{i, k}$ is the probability of class $k$. We have more confidence in pixels with higher odds since their class assignment is stronger. There are situations, such as border pixels or mixed ones, where the odds of different classes are similar in magnitude. We take them as cases of low confidence in the classification result. To assess and correct these cases,  Bayesian smoothing methods borrow strength from the neighbors and reduces the variance of the estimated class for each pixel.

We further make the transformation 
$$
	x_{i,k} = \log [O_{i,k}]
$$
which measures the *logit* (log of the odds) associated to classifying the pixel $i$ as being of class $k$. The support of $x_{i, k}$ is $\mathbb{R}$. We can express the pixel data as a $K$-dimensional multivariate logit vector 

$$
\mathbf{x}_{i}=(x_{i,k_{0}},x_{i,k_{1}},\dots{},x_{i,k_{K}})
$$ 


For each pixel, the random variable that describes the class probability $k$ is denoted by $\theta_{i,k}$. This formulation allows uses to use the class covariance matrix in our formulations. We can express Bayes' rule for all combinations of pixel and classes for a time interval as

$$
\pi(\boldsymbol\theta_{i}|\mathbf{x}_{i}) \propto \pi(\mathbf{x}_{i}|\boldsymbol\theta_{i})\pi(\boldsymbol\theta_{i}).	
$$

We assume the conditional distribution $\mathbf{x}_{i}|\boldsymbol\theta_{i}$ follows a multivariate normal distribution

$$
    [\mathbf{x}_{i}|\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\boldsymbol\theta_{i},\boldsymbol\Sigma_{i}),
$$

where $\boldsymbol\theta_{i}$ is the mean parameter vector for the pixel $i$, and $\boldsymbol\Sigma_{i}$ is a known $k\times{}k$ covariance matrix that we will use as a parameter to control the level of smoothness effect. We will discuss later on how to estimate $\boldsymbol\Sigma_{i}$. To model our uncertainty about the parameter $\boldsymbol\theta_{i}$, we will assume it also follows a multivariate normal distribution with hyper-parameters $\mathbf{m}_{i}$ for the mean vector, and $\mathbf{S}_{i}$ for the covariance matrix. 

$$
    [\boldsymbol\theta_{i}]\sim\mathcal{N}_{K}(\mathbf{m}_{i}, \mathbf{S}_{i}).
$$

The above equation defines our prior distribution. The hyper-parameters $\mathbf{m}_{i}$ and $\mathbf{S}_{i}$ are obtained using the neighboring pixels of pixel $i$. The neighborhood can be defined as any graph scheme (e.g. a given Chebyshev distance on the time-space lattice) and can include the referencing pixel $i$ as a neighbor. More formally, let 

$$
    \mathbf{V}_{i}=\{\mathbf{x}_{i_{j}}\}_{j=1}^{N}, 
$$
denote the $N$ logit vectors of a spatiotemporal neighborhood $N$ of pixel $i$. Then the prior mean is calculated by

$$
	\mathbf{m}_{i}=\operatorname{E}[\mathbf{V}_{i}],
$$

and the prior covariance matrix by

$$
    \mathbf{S}_{i}=\operatorname{E}\left[
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)
      \left(\mathbf{V}_{i}-\mathbf{m}_{i}\right)^\intercal
    \right].
$$
The $\boldsymbol\theta_{i}$ parameter model is our initial belief about a pixel vector using the neighborhood information in the prior distribution. It represents what we know about the probable value of $\mathbf{x}_{i}$ (and hence, about the class probabilities as the logit function is a monotonically increasing function) \emph{before} observing it. The \emph{likelihood} function $P[\mathbf{x}_{i,t}|\boldsymbol\theta_{i,t}]$ represents the added information provided by our observation of $\mathbf{x}_{i,t}$. The \emph{posterior} probability density function $P[\boldsymbol\theta_{i,t}|\mathbf{x}_{i,t}]$ is our improved belief of the pixel vector \emph{after} seeing $\mathbf{x}_{i,t}$.

Since the likelihood and prior are multivariate normal distributions, the posterior will also be a multivariate normal distribution, whose updated parameters can be derived by applying the density functions associated to the above equations. The posterior distribution is given by

$$
    [\boldsymbol\theta_{i}|\mathbf{x}_{i}]\sim\mathcal{N}_{K}\left(
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}( \mathbf{S}_{i}^{-1}\mathbf{m}_{i} + \boldsymbol\Sigma^{-1} \mathbf{x}_{i}),
    (\mathbf{S}_{i}^{-1} + \boldsymbol\Sigma^{-1})^{-1}
    \right).
$$
At this point, we are able to infer the estimator $\hat{\boldsymbol\theta}_{i}$ for the $\boldsymbol\theta_{i}|\mathbf{x}_{i}$ parameter. For the multivariate normal distribution, the posterior mean minimizes the quadratic loss but the absolute and zero-one loss functions. It can be taken from the updated mean parameter of the posterior distribution which, after some algebra, can be expressed as

$$
    \hat{\boldsymbol{\theta}}_{i}=\operatorname{E}[\boldsymbol\theta_{i}|\mathbf{x}_{i}]=\boldsymbol\Sigma_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{m}_{i} +
    \mathbf{S}_{i}\left(\boldsymbol\Sigma_{i}+\mathbf{S}_{i}\right)^{-1}\mathbf{x}_{i}.
$$

The estimator value for the logit vector $\hat{\boldsymbol\theta}_{i}$ is a weighted average of the original logit vector $\mathbf{x}_{i}$ and the neighborhood mean vector $\mathbf{m}_{i}$. The weights are given by the covariance matrix $\mathbf{S}_{i}$ of the prior distribution and the covariance matrix of the conditional distribution. The matrix $\mathbf{S}_{i}$ is calculated considering the  neighbors and the matrix $\boldsymbol\Sigma_{i}$ is the smoothing factor provided as prior belief by the user. 

When the values of local class covariance $\mathbf{S}_{i}$ are higher than those the conditional covariance $\boldsymbol\Sigma_{i}$, our confidence on the influence of the neighbors is low, and the smoothing algorithm gives more weight to the original pixel value $x_{i,k}$. When the local class covariance $\mathbf{S}_{i}$ decreases relative to the smoothness factor $\boldsymbol\Sigma_{i}$, our confidence on the influence of the neighborhood increases. The smoothing procedure will be most relevant in situations where the original classification odds ratio is low, showing a low level of separability between classes. In these cases, the updated values of the classes will be influenced by the local class variances. 

In practice, $\boldsymbol\Sigma_{i}$ is a user-controlled covariance matrix parameter that will be set by users based on their knowledge of the region to be classified. In the simplest case, users can associate the  conditional covariance $\boldsymbol\Sigma_{i}$ to a diagonal matrix, using only one hyperparameter $\sigma^2_k$ to set the level of smoothness. Higher values of $\sigma^2_k$ will cause the assignment of the local mean to the pixel updated probability. In our case, after some classification tests, we decided to use $\sigma^2_k=20$ by default for all $k$. 

The version implemented in `sits` includes the following parameters

- `smoothness`: user-controlled parameter that controls the influence of the neighborhood values on the probabibility value of each class of a pixel. Can be defined as a unique value for all classes, or a matrix whose size of size $num_classes * num_classes$. The default value is 20, but users are encouraged define `smoothness` as a diagonal matrix whose values reflect the relative importance of each class in the output product. 
- `window_size`: size of $n*n$ window used to define the neighborhood.
- `neigh_fraction`: percentage of pixels per window used to calculate local class statistics. The aim is to use only those pixels in the window that are likely to be part of the same class as the central pixel. For example, for a `window_size` of size 9 and `neigh_fraction` of 0.5, half of the pixels of the 9 x 9 window (those with higher class probabibility) will be used to estimate the local class statistics $\mathbf{m}_{i,t}$ and $\mathbf{S}_{i,t}$ which represent the mean and standard deviation of the classes associated to prox

<!--chapter:end:15-annex.Rmd-->

# References{-}

<!--chapter:end:16-references.Rmd-->

