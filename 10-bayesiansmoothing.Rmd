```{r, include = FALSE}
source("common.R")
dir.create("./tempdir/ch10")
sits:::.conf_set_options("tmap_legend_text_size" = 0.7)
sits:::.conf_set_options("tmap_legend_title_size" = 0.7)
sits:::.conf_set_options("tmap_max_cells" = 1e+09) 
```

# Bayesian smoothing for post-processing{-}

## Introduction{-}

Image classification post-processing has been defined as "a refinement of the labeling in a classified image to enhance its classification accuracy" [@Huang2014]. In remote sensing image analysis, these procedures combine pixel-based classification methods with a spatial post-processing method to remove outliers and misclassified pixels. For pixel-based classifiers, post-processing methods allow including spatial information in the final results.

The `sits` package uses a *time-first, space-later* approach. Since machine learning classifiers in `sits` are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land-cover classification by incorporating spatial and contextual information into the classification process.
<!---
PEDRO: 30) Por todo texto, sao usados termos como "land use and land cover classes", "land use classification", "land-cover classification" (acima), "land use/land cover". Acho que seria bom definirmos um padrao.
-->
Smoothing methods use neighborhood data to remove outliers and enhance consistency in the resulting product. 

Most statistical classifiers use training samples derived from "pure" pixels that users have selected to represent the desired output classes. However, images contain many mixed pixels irrespective of the resolution. Also, there is a considerable degree of data variability in each class. These effects lead to outliers whose chance of misclassification is significant. To offset these problems, most post-processing methods use the "smoothness assumption" [@Schindler2012]: nearby pixels tend to have the same label. To put this assumption in practice, smoothing methods in `sits` use the neighborhood information to remove outliers and enhance consistency in the resulting product.
<!---
PEDRO: 31) Existem duas ocorrencias de "product" neste capitulo, que so tem 
-->

## Motivation{-}

The smoothing method available in `sits` uses Bayesian inference for including expert knowledge on the derivation of probabilities. As stated by [@Spiegelhalter2009]: "In the Bayesian paradigm, degrees of belief in states of nature are specified. Bayesian statistical methods start with existing 'prior' beliefs and update these using data to give 'posterior' beliefs, which may be used as the basis for inferential decisions". Bayesian inference has been established as a major method for assessing probability. 

Bayesian inference allows the inclusion of expert knowledge.
<!---
PEDRO: 32) Esta frase acima ja esta no paragrafo anterior.
-->
The assumption is that class probabilities at the local level should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels, considering spatial dependence.  

## Bayesian estimation{-}

The Bayesian estimate is based on two random variables: (a) The observed class probabilities for each pixel denoted by a random variable $p_{i,k}$, where $i$ is the index of the pixel and $k$ indicates the class; (b) The underlying class probabilities for each pixel, denoted by a random variable $\phi_{i,k}$. The probabilities $p_{i,k}$ are the classifier's output, being subject to noise, outliers, and classification errors. Our estimation aims to remove these effects and obtain $\phi_{i,k}$ to approximate the actual class probability better. 

To more our inference tractable, 
<!---
PEDRO: 33) To make our inference tractable?
-->
we first convert the class probability values $p_{i,k}$  to log-odds values using the logit function, as shown below. The logit function converts probability values ranging from $0$ to $1$ to values from negative infinity to infinity. The conversion from probabilities logit values is helpful to support our assumption of normal distribution functions for our data. 
<!---
PEDRO: 34) "assumption of normal distribution [sem o functions] for our data?"
-->


$$
    x_{i,k} = \ln \left(\frac{p_{i,k}}{1 - p_{i,k}}\right)
$$
In what follows, we consider two random variables for each pixel $i$: (a) $x_{i,k}$, the observed class logits; (b) $\mu_{i,k}$, the inferred logit values. In other words, we measure $x_{i,k}$, but want to obtain $\mu_{i,k} | x_{i,k}$. The Bayesian inference procedure can be expressed as

$$
    \pi(\mu|x) \propto{} \pi(x|\mu)\pi(\mu).
$$
To estimate the conditional posterior distribution $\pi(\theta{}|x)$, we combine two distributions: (a) the distribution $\pi(x|\mu)$, known as the likelihood function, which expresses the dependency of the measured values $x_{i,k}$ in the underlying values $\mu_{i,k}$; and (b) $\pi(\mu)$, which is our guess on the actual data distribution, known as the prior. For simplicity, we also assume independence between the different classes $k$, instead of considering a multivariate distribution. Therefore, each class $k$ is updated separately. 

We assume that the likelihood $x_{i,k} | \mu_{i,k}$ follows a normal distribution, $N(\mu_{i,k}, \sigma^2_{k})$, with parameters $\mu_{i,k}$ and $\sigma^2_{k}$. 
<!---
PEDRO: 35) "with parameters $\mu_{i,k}$ and $\sigma^2_{k}$" poderia ser removido, visto que ja esta explicito na formula.
-->

The variance $\sigma^2_{k}$ will be estimated by users based on their expertise and used as a hyperparameter to control the smoothness of the resulting estimate. Therefore
<!---
PEDRO: 36) Poderia ser algo do tipo "The variance $\sigma^2_{k}$ is a hyperparameter that controls the smoothness of the resulting estimate."?
-->

$$
x_{i,k} | \mu_{i,k} \sim N(\mu_{i,k}, \sigma^2_{k})
$$
is the likelihood function. We will assume a normal local prior for the parameter $\mu_{i,k}$ with parameters $m_{i,k}$ and $s^2_{i,k}$:

$$
\mu_{i,k} \sim N(m_{i,k}, s^2_{i,k}).
$$
We estimate the local means and variances for the prior distribution by considering a spatial neighboring. Let $\#(V_{i})$ be the number of elements in the neighborhood $V_{i}$. We then can calculate the mean value by

$$
m_{i,t,k} = \frac{\sum_{(j) \in V_{i}} x_{j,k}}{\#(V_{i})}
$$
and the variance by
$$
s^2_{i,k} = \frac{\sum_{(j) \in V_{i}} [x_{j,k} - m_{i,k}]^2}{\#(V_{i})-1}.    
$$
Given these assumptions, the Bayesian update for the expected conditional mean ${E}[\mu_{i,k} | x_{i,k}]$ is given by:
$$
\begin{equation}
{E}[\mu_{i,k} | x_{i,k}] =
\frac{m_{i,t} \times \sigma^2_{k} + 
x_{i,k} \times s^2_{i,k}}{ \sigma^2_{k} +s^2_{i,k}}
\end{equation}
$$

which can be expressed as a weighted mean

$$ 
{E}[\mu_{i,k} | x_{i,k}] =
\Biggl [ \frac{s^2_{i,k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times
x_{i,k} +
\Biggl [ \frac{\sigma^2_{k}}{\sigma^2_{k} +s^2_{i,k}} \Biggr ] \times m_{i,k} 
$$

where

1. $x_{i,k}$ is the logit value for pixel $i$ and class $k$.
2. $m_{i,k}$ is the average of logit values for pixels of class $k$ in the neighborhood of pixel $i$.
3. $s^2_{i,k}$ is the variance of logit values for pixels of class $k$ in the neighborhood of pixel $i$.
4. $\sigma^2_k$ is the prior variance of the logit values for class $k$.

The above equation is a weighted average between the value $x_{i,k}$ for the pixel and the mean $m_{i,k}$ for the neighboring pixels. When the variance $s^2_{i,k}$ for the neighbors is too high, the smoothing algorithm gives more weight to the pixel value $x_{i,k}$. On the other hand, when the noise $\sigma^2_k$ increases, the method gives more weight to the neighborhood mean $m_{i,k}$.

The parameter $\sigma^2_k$ controls the level of smoothness. If $\sigma^2_k$ is zero, the smoothed value ${E}[\mu_{i,k} | x_{i,k}]$ will be equal to the pixel value $x_{i,k}$. Making $\sigma^2_k$ high leads to much smoothness. Values of the prior variance $\sigma^2_k$, which are small relative to the local variance $s^2_{i,k}$, increase our confidence in the original probabilities. Conversely, values of the prior variance $\sigma^2_k$, which are big relative to the local variance $s^2_{i,k}$, increase our confidence in the average probability of the neighborhood. 

Thus, the parameter $\sigma^2_k$ expresses our confidence in the inherent variability of the distribution of values of a class $k$. The smaller the parameter $\sigma^2_k$, the more we trust the estimated probability values produced by the classifier for class $k$. Conversely, higher values of $\sigma^2_k$ indicate lower confidence in the classifier outputs and improved confidence in the local averages.

Consider the following two-class example. Take a pixel with probability $0.4$ (logit $x_{i,1} = -0.4054$) for class A and probability $0.6$ (logit $x_{i,2} = 0.4054$) for class B. Without post-processing, the pixel will be labeled as class B. Consider that the local average is $0.6$ (logit $m_{i,1} = 0.4054$) for class A and $0.4$ (logit $m_{i,2} = -0.4054$) for class B. This is a case of an outlier classified originally as class B in the midst of a set of class A pixels. Take the local variance of logits to be $s^2_{i,1} = 5$ for class A and $s^2_{i,2} = 10$ and for class B. This difference is expected if the local variability of class A is smaller than that of class B. 

To complete the estimate, we need to set the parameter $\sigma^2_{k}$, representing our prior belief in the variability of the probability values for each class. If we take both $\sigma^2_{A}$ for class A and $\sigma^2_{B}$ for class B to be both $10$, the Bayesian estimated probability for class A is $0.52$  and for class B is $0.48$. In this case, the pixel will be relabeled as being class A. However, if our belief in the original values is higher, we will get a different result. If we set $\sigma^2$ to be $5$ for both classes A and B, the Bayesian probability estimate will be $0.48$ for class A and $0.52$ for class B. In this case, the original label will be kept. 

We make the following recommendations for setting the $\sigma^2_{k}$ parameter:
    
1. Set the $\sigma^2_{k}$ parameter with high values ($20$ or above) to increase the neighborhood influence compared with the probability values for each pixel. Classes whose probabilities have strong spatial autocorrelation will tend to replace outliers of different classes.

2. Set the $\sigma^2_{k}$ parameter with low values ($5$ or below) to reduce the neighborhood influence compared with the probabilities for each pixel of class $k$. In this way, classes with low spatial autocorrelation are more likely not to be relabeled.

Consider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests without many outliers inside them, she will set the $\sigma^2$ parameter for the class "Forest" to be high. For comparison, to avoid that small watersheds with few similar neighbors being relabeled, it is advisable to avoid a strong influence of the neighbors, setting $\sigma^2$ to be as low as possible. 

## Defining the neighborhood{-}

The intuition for Bayesian smoothing is that homogeneous neighborhoods should have the same class. In homogeneous neighborhoods, the dominant class has both higher average probabilities and lower variance than the other classes. In these neighborhoods, a pixel of a different class is likely to be associated to lower average probabilities and higher local variance. Mixed pixels at the limits between areas with different classes pose a problem for classification. These pixels contain signatures of two classes. To account for these cases, Bayesian smoothing in `sits` uses a special definition of a neighborhood.

To be reliable, local class statistics should only include pixels likely to belong to such class. Windows centered on border pixels contain only some pixels belonging to the same class as the central pixel; the others belong to a different class. Consider a window of size $7 \times 7$ around a pixel in the probability map of class "Forest". It will contain the central pixel and 80 neighbors.
<!---
PEDRO: 37) Nao seria 48 vizinhos? 7*7-1? Ou seria uma vizinhanca 9x9?
-->
Instead of using all those neighbors to compute the local statistics, `sits` uses only those with the highest probability of belonging to the class "Forest". Such a percentage of pixels per window to calculate the local statistics is user-controllable. 

## Measuring the local variance{-}

As discussed above, the effect of the Bayesian estimator depends on the values of the a prior variance $\sigma^2_k$ set by the user and of the local variance $s^2_{i,1}$ measured for each pixel. To illustrate the impact of the choices of the $\sigma^2_k$ parameter, we present a detailed example. The first step is to take a probability cube for a deforestation detection application in an area of the Brazilian Amazon. This cube has been produced by a random forest model with six classes. We first build the data cube and then plot the probabilities for classes "Water" and "Forest". 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Probability map produced for classes Forest and Water."}
# define the classes of the probability cube
labels <- c("Water", "ClearCut_Burn", "ClearCut_Soil",
            "ClearCut_Veg", "Forest", "Wetland")
# directory where the data is stored 
data_dir <- system.file("extdata/Rondonia-20LLQ/", package = "sitsdata")
# create a probability data cube from a file 
probs_cube <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir,
    bands = "probs",
    labels = labels,
    parse_info = c("X1", "X2", "tile", "start_date", "end_date", "band", "version"))

# plot the probabilities for water and forest
plot(probs_cube, labels = c("Water", "Forest"))
```

The probability map for class "Forest" shows high values associated with compact patches and linear stretches in riparian areas. By contrast, the probability map for class "Water" has mostly low values, except in a few places with a high chance of occurrence of this class. To further understand the behavior of the Bayesian estimator, it is helpful to examine the local variance associated with the logits of the probabilities. 

The `sits_variance()` function estimates the local variances for the logits, which correspond to the $s^2_{i,k}$ parameter in the Bayesian estimator. It has the following parameters: (a) `cube`, a probability cube; (b) `window_size`, the dimension of the local neighborhood; (c) `neigh_fraction`, the percentage of pixels in the neighborhood which will be used to calculate the variance; (d) `multicores`, number of CPU cores that will be used for processing; (e) `memsize`, memory available; (f) `output_dir`, directory where results will be stored; (g) `version`, for version control. In the example below, we will use half of the pixels of a $7 \times 7$ window to estimate the variance. The chosen pixels will be those with the highest probability to be more representative of the actual class distribution. The output values are the variances of the logits.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Variance map for class Forest."}
var_cube <- sits_variance(
    cube = probs_cube,
    window_size = 7,
    neigh_fraction = 0.5,
    multicores = 4,
    memsize = 24,
    output_dir = "./tempdir/chp10",
    version = "w7-n05")

plot(var_cube, labels = c("Water", "Forest"))
```

The plot for the "Forest" class shows that the areas of low variance are associated both with dense forest patches as well as areas where trees have been completely removed. Areas of high variance are primarily associated with the borders between forest areas and the other classes. By contrast, the plot for the "Water" class is not informative, with small areas of high variance located near the areas of high water probability. Both plots show that most variance values are low, and high values reach 30. This information is relevant for setting the values of the prior variance $\sigma^2$, as discussed below.

## Running Bayesian smoothing {-}

To run Bayesian smoothing, we use `sits_smooth()` with parameters: (a) `cube`, a probability cube produced by `sits_classify()`; (b) `window_size`, the local window to compute the neighborhood probabilities; (d) `neigh_fraction`, fraction of local neighbors used to calculate local statistics; (e) `smoothness`, a vector with estimates of the prior variance of each class; (f) `multicores`, number of CPU cores that will be used for processing; (g) `memsize`, memory available for classification; (h) `output_dir`, a directory where results will be stored; (i) `version`, for version control. The resulting cube can be visualized with `plot()`. In what follows, we compare the smoothing effect by varying the `window_size` and `smoothness` parameters. 

Together, the parameters `window_size` and `neigh_fraction` control how many pixels in a neighborhood the Bayesian estimator will use to calculate the local statistics. For example, setting `window size` to $7$ and `neigh_fraction` to $0.5$ (the defaults) ensures that $25$ samples are used to estimate the local statistics. 

Our first reference is the classified map without smoothing, which shows the presence of outliers and classification errors. To obtain it, we use `sits_label_classification()`, taking the probability map as an input, as follows.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Classified map without smoothing."}
# Generate the thematic map
class_map <- sits_label_classification(
    cube = probs_cube,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp10",
    version = "no_smooth")

# Plot the result
plot(class_map)
```

To remove the outliers and classification errors, we run a smoothing procedure where all prior variances are set to the same value of $20$, which is relatively high compared with the maximum local class variance shown above. In this case, for most situations, the new value of the 
<!---
PEDRO: 38) Esta faltando terminar a frase
-->

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Probability maps after bayesian smoothing."}
# Compute Bayesian smoothing
cube_smooth_w7_f05_s20 <- sits_smooth(
    cube = probs_cube,
    window_size = 7,
    neigh_fraction = 0.50,
    smoothness = 20, 
    multicores = 4,
    memsize = 12,
    version = "w7-f05-s20",
    output_dir = "./tempdir/chp10")

# Plot the result
plot(cube_smooth_w7_f05_s20, labels = c("Water", "Forest"), palette = "YlGn")
```

Bayesian smoothing has removed some of the local variability associated with misclassified pixels that differ from their neighbors. There is a side effect: the water areas surrounded by forests have not been preserved in the forest probability map. The smoothing impact is best appreciated by comparing the labeled map produced without smoothing to the one that follows the procedure, as shown below.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Final classification map after Bayesian smoothing with 7 x 7 window, neigh_fraction = 0.5 and smoothness = 20."}
# Generate the thematic map
defor_map_w7_f05_20 <- sits_label_classification(
    cube = cube_smooth_w7_f05_s20,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp8",
    version = "w7-f05-s20")

plot(defor_map_w7_f05_20)
```

In the smoothed map, the outliers have been removed by expanding forest areas. Forests have replaced small corridors of water and soil encircled by trees. This effect is due to the high probability of forest detection in the training data. To keep the water areas and reduce the expansion of the forest area, a viable alternative is to reduce the smoothness ($\sigma^2$) for the "Forest" and "Water" classes. In this way, the local influence of the forest in the other classes is reduced. As for the water areas, since they are narrow, their neighborhoods will have many low probability values, which would reduce the expected value of the Bayesian estimator. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap="Probability maps after Bayesian smoothing with 7 x 7 window with low smoothness for classes Water and Forest"}
# Reduce smoothing for classes Water and Forest
# Labels:  "Water", "ClearCut_Burn", "ClearCut_Soil", 
#          "ClearCut_Veg", "Forest", "Wetland"
smooth_water_forest <- c(5, 20, 20, 20, 5, 20)
# Compute Bayesian smoothing
cube_smooth_w7_f05_swf <- sits_smooth(
    cube = probs_cube,
    window_size = 7,
    neigh_fraction = 0.5,
    smoothness = smooth_water_forest,
    multicores = 4,
    memsize = 12,
    version = "w7-f05-swf",
    output_dir = "./tempdir/chp10")

# Computed labeled map
defor_map_w7_f05_swf <- sits_label_classification(
    cube = cube_smooth_w7_f05_swf,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp10",
    version = "w7-f05-swf")

plot(defor_map_w7_f05_swf)
```

Comparing the two maps, the narrow water streams inside the forest area have been better preserved. Small corridors between forest areas have also been maintained. A better comparison between the two maps requires importing them into QGIS. Exporting data from `sits` to QGIS is discussed in the Chapter [Visualising and exporting data](https://e-sensing.github.io/sitsbook/visualising-and-exporting-data.html). 

In conclusion, post-processing is a desirable step in any classification process. Bayesian smoothing improves the borders between the objects created by the classification and removes outliers that result from pixel-based classification. It is a reliable method that should be used in most situations. 
